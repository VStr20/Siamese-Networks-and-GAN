{"cells":[{"cell_type":"markdown","metadata":{},"source":["https://drive.google.com/drive/folders/1l7Fxjyr1NXgoNCby2-ow-dVmlkywixKz?usp=sharing"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-10-22T11:12:33.548364Z","iopub.status.busy":"2023-10-22T11:12:33.548097Z","iopub.status.idle":"2023-10-22T11:12:34.918310Z","shell.execute_reply":"2023-10-22T11:12:34.917286Z","shell.execute_reply.started":"2023-10-22T11:12:33.548338Z"},"id":"rRxFWUaqSGkM","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]}],"source":["import numpy as np\n","import pandas\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","#importing the necessary libaries\n","import os"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-10-22T11:12:34.920715Z","iopub.status.busy":"2023-10-22T11:12:34.920217Z","iopub.status.idle":"2023-10-22T11:12:34.925047Z","shell.execute_reply":"2023-10-22T11:12:34.924033Z","shell.execute_reply.started":"2023-10-22T11:12:34.920679Z"},"id":"Trh7BX4OW_qW","outputId":"f13f7b7f-6c15-43fb-aec5-7c036efd0daa","trusted":true},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-10-22T11:13:47.000425Z","iopub.status.busy":"2023-10-22T11:13:47.000045Z","iopub.status.idle":"2023-10-22T11:13:47.946439Z","shell.execute_reply":"2023-10-22T11:13:47.945227Z","shell.execute_reply.started":"2023-10-22T11:13:47.000392Z"},"id":"bX2qJH3NoORn","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["tar (child): /lfwdataset/lfw (1).tgz: Cannot open: No such file or directory\n","tar (child): Error is not recoverable: exiting now\n","tar: Child returned status 2\n","tar: Error is not recoverable: exiting now\n"]}],"source":["# !tar -xzvf \"/lfwdataset/lfw (1).tgz\" -C \"/lfw\"\n","# Used to unzip the file"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-10-22T11:22:41.414299Z","iopub.status.busy":"2023-10-22T11:22:41.413312Z","iopub.status.idle":"2023-10-22T11:22:55.409226Z","shell.execute_reply":"2023-10-22T11:22:55.408211Z","shell.execute_reply.started":"2023-10-22T11:22:41.414252Z"},"id":"cWVbrK7aSSaq","outputId":"52e73b47-ca1a-42ee-9b59-c85e14352386","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of people with 2 or more images :  1680\n"]}],"source":["MINIMUM_SIZE = 2\n","# path = '/content/drive/MyDrive/Assignment2/lfw/lfw'\n","path = '/kaggle/input/lfwdataset1/lfw/lfw'\n","\n","# file_count = 0\n","count=0\n","for entry in os.scandir(path):\n","    file_count = 0\n","    for fil in os.scandir(entry):\n","        if fil.is_file():\n","            file_count += 1\n","        if file_count == MINIMUM_SIZE:\n","            count+=1\n","\n","# loop through the folders and files to find people with more than 2 images\n","print(\"Number of people with 2 or more images : \", count)"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-10-22T11:23:00.273837Z","iopub.status.busy":"2023-10-22T11:23:00.273411Z","iopub.status.idle":"2023-10-22T11:23:00.284055Z","shell.execute_reply":"2023-10-22T11:23:00.283045Z","shell.execute_reply.started":"2023-10-22T11:23:00.273804Z"},"id":"7AyC-NhMWBXO","outputId":"f886cf73-069a-4be7-f8cc-025b1660c152","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of folders/distinct people :  603645\n"]}],"source":["count=0\n","for entry in os.scandir(path):\n","    count+=1\n","print(\"Number of folders/distinct people : \", count)\n","\n","# The number of distinct people"]},{"cell_type":"markdown","metadata":{"id":"JWAODgtPWPVw"},"source":["The dataset contains 1680 distinct people. Here images for only 1680 people are used. These are divided as follows -\n","\n","Training data - 560\n","Validation data - 140\n","Test data - 100"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-10-22T11:23:14.824439Z","iopub.status.busy":"2023-10-22T11:23:14.823823Z","iopub.status.idle":"2023-10-22T11:23:14.846885Z","shell.execute_reply":"2023-10-22T11:23:14.846088Z","shell.execute_reply.started":"2023-10-22T11:23:14.824404Z"},"id":"thkOVhFFWDNb","outputId":"323de6d9-c920-441a-d5e0-d1b8004a0550","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Image: /kaggle/input/lfwdataset1/lfw/lfw/Tyler_Hamilton/Tyler_Hamilton_0002.jpg\n","Size: 250 x 250\n","Image: /kaggle/input/lfwdataset1/lfw/lfw/Tyler_Hamilton/Tyler_Hamilton_0001.jpg\n","Size: 250 x 250\n","All images are of size 250*250\n"]}],"source":["from PIL import Image\n","import os\n"," \n","# path = '/content/drive/MyDrive/Assignment2/lfw/lfw/'\n","path = '/kaggle/input/lfwdataset1/lfw/lfw'\n","\n","count = 0\n","\n","for entry in os.listdir(path):\n","    entry_path = os.path.join(path, entry)\n","\n","    # Check if the entry is a main folder\n","    if os.path.isdir(entry_path):\n","\n","        # Iterate through the files in the sub-folders\n","        for fil in os.listdir(entry_path):\n","            file_path = os.path.join(entry_path, fil)\n","\n","            if file_path.endswith(('.jpg', '.jpeg', '.png', '.gif')):\n","                print(f\"Image: {file_path}\")\n","\n","                # Open the image and get its size\n","                img = Image.open(file_path)\n","                width, height = img.size\n","                print(f\"Size: {width} x {height}\")\n","                count+=1\n","        if(count==10):\n","            break\n","    break\n","\n","print(\"All images are of size 250*250\")\n","\n","#reference - chatgpt"]},{"cell_type":"markdown","metadata":{"id":"nd9kxHd6M189"},"source":["The code below divides the images into train, validation and test set."]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-10-22T11:23:43.035958Z","iopub.status.busy":"2023-10-22T11:23:43.035045Z","iopub.status.idle":"2023-10-22T11:23:43.041466Z","shell.execute_reply":"2023-10-22T11:23:43.040416Z","shell.execute_reply.started":"2023-10-22T11:23:43.035924Z"},"id":"bvbBcs1PrVgd","trusted":true},"outputs":[],"source":["# from PIL import Image\n","# import os\n","\n","# # Define the output size for the resized images\n","# output_size = (200, 200)  # Change this to the desired size\n","\n","# input_path = '/content/drive/MyDrive/Assignment2/lfw/lfw/'\n","\n","# file_count = 0\n","\n","# # Define the thresholds for the split\n","# validation_threshold = 560  # Number of persons for training\n","# test_threshold = 700  # Number of persons for validation\n","\n","# # Code below same as above cell\n","# for entry in os.listdir(input_path):\n","#     entry_path = os.path.join(input_path, entry)\n","\n","#     if os.path.isdir(entry_path):\n","#         # Create a subdirectory to store the cropped and resized images\n","#         if file_count >= validation_threshold and file_count < test_threshold:\n","#             output_path = '/content/drive/MyDrive/Assignment2/lfw/validation_set/'\n","#         elif file_count >= test_threshold and file_count<=800:\n","#             output_path = '/content/drive/MyDrive/Assignment2/lfw/test_set/'\n","#         elif file_count<validation_threshold and file_count>=0:\n","#           output_path = '/content/drive/MyDrive/Assignment2/lfw/train_set/'\n","#         else:\n","#           break\n","\n","#         output_dir = os.path.join(output_path, entry)\n","#         os.makedirs(output_dir, exist_ok=True)\n","\n","#         for fil in os.listdir(entry_path):\n","#             file_path = os.path.join(entry_path, fil)\n","\n","#             if file_path.endswith(('.jpg', '.jpeg', '.png', '.gif')):\n","#                 # Open the image\n","#                 img = Image.open(file_path)\n","\n","#                 # Resize the image to the specified output size\n","#                 img = img.resize(output_size)\n","\n","#                 # Save the resized image to the output directory\n","#                 output_file_path = os.path.join(output_dir, fil)\n","#                 img.save(output_file_path)\n","\n","#         file_count += 1\n","\n","# print(f\"All images are resized to {output_size[0]}x{output_size[1]} and saved in the directory.\")"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-10-22T11:23:49.218825Z","iopub.status.busy":"2023-10-22T11:23:49.218196Z","iopub.status.idle":"2023-10-22T11:25:50.113294Z","shell.execute_reply":"2023-10-22T11:25:50.112324Z","shell.execute_reply.started":"2023-10-22T11:23:49.218791Z"},"id":"FgrcuzS5i9QI","trusted":true},"outputs":[],"source":["import os\n","import random\n","\n","# path to the dataset\n","# lfw_dir = '/content/drive/MyDrive/Assignment2/lfw/train_set'\n","lfw_dir = '/kaggle/input/lfwdataset1/lfw/train_set'\n","\n","\n","# Get a list of subfolder for each person\n","person_folders = [os.path.join(lfw_dir, folder) for folder in os.listdir(lfw_dir)]\n","\n","# Initialize lists to store positive and negative pairs\n","positive_pairs = []\n","negative_pairs = []\n","\n","# Generate pairs of similar and dissimilar images\n","for folder in person_folders:\n","    images = os.listdir(folder)\n","    if len(images) >= 2:\n","        # Generate positive pairs\n","        positive_pairs.extend([(os.path.join(folder, images[i]), os.path.join(folder, images[j])) for i in range(len(images)) for j in range(i + 1, len(images))])\n","\n","# Generate negative pairs\n","for i in range(len(person_folders)):\n","    for j in range(i + 1, len(person_folders)):\n","        folder1_images = os.listdir(person_folders[i])\n","        folder2_images = os.listdir(person_folders[j])\n","        if folder1_images and folder2_images:\n","            negative_pairs.append((os.path.join(person_folders[i], random.choice(folder1_images)), os.path.join(person_folders[j], random.choice(folder2_images))))\n"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-10-22T11:25:50.127537Z","iopub.status.busy":"2023-10-22T11:25:50.127260Z","iopub.status.idle":"2023-10-22T11:25:50.146619Z","shell.execute_reply":"2023-10-22T11:25:50.145704Z","shell.execute_reply.started":"2023-10-22T11:25:50.127513Z"},"id":"wbOV0YQ_jfa7","outputId":"dc4c8d0a-38ac-4241-d590-52d42b178281","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["('/kaggle/input/lfwdataset1/lfw/train_set/Xanana_Gusmao/Xanana_Gusmao_0001.jpg', '/kaggle/input/lfwdataset1/lfw/train_set/Kai-Uwe_Ricke/Kai-Uwe_Ricke_0001.jpg')\n","20221 156520\n"]}],"source":["# Assign labels to positive and negative pairs\n","positive_labels = [1] * len(positive_pairs)\n","negative_labels = [0] * len(negative_pairs)\n","\n","print(negative_pairs[0])\n","print(len(positive_pairs), len(negative_pairs))\n","# Combine positive and negative pairs and labels\n","paired_images = positive_pairs + negative_pairs\n","labels = positive_labels + negative_labels"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2023-10-22T11:25:57.387326Z","iopub.status.busy":"2023-10-22T11:25:57.386979Z","iopub.status.idle":"2023-10-22T11:25:59.156050Z","shell.execute_reply":"2023-10-22T11:25:59.155217Z","shell.execute_reply.started":"2023-10-22T11:25:57.387299Z"},"id":"w8m38Z6KtW1s","trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision.transforms as transforms\n","\n","#importing necessary libraries for model architecture and training"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2023-10-22T11:26:01.356570Z","iopub.status.busy":"2023-10-22T11:26:01.355731Z","iopub.status.idle":"2023-10-22T11:26:01.365234Z","shell.execute_reply":"2023-10-22T11:26:01.364252Z","shell.execute_reply.started":"2023-10-22T11:26:01.356531Z"},"id":"whzlVQ_OxFgP","trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torchvision.models as models\n","\n","# Define the Siamese Network class\n","class SiameseNetwork(nn.Module):\n","    def __init__(self):\n","        super(SiameseNetwork, self).__init__()\n","\n","        # Load the pre-trained ResNet-50 model\n","        self.resnet = models.resnet50(pretrained=True)\n","\n","        # Remove the final classification layer\n","        self.resnet = nn.Sequential(*list(self.resnet.children())[:-1])\n","\n","        # Define the fully connected layers for the Siamese network namely 2 dense fully connected layers\n","        self.fc = nn.Sequential(\n","            nn.Linear(2048, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 256)\n","        )\n","\n","    def forward_one(self, x):\n","        x = self.resnet(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.fc(x)\n","        return x\n","\n","    def forward(self, input1, input2):\n","        output1 = self.forward_one(input1)\n","        output2 = self.forward_one(input2)\n","        return output1, output2\n"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2023-10-22T11:26:12.150563Z","iopub.status.busy":"2023-10-22T11:26:12.149501Z","iopub.status.idle":"2023-10-22T11:26:12.157997Z","shell.execute_reply":"2023-10-22T11:26:12.157120Z","shell.execute_reply.started":"2023-10-22T11:26:12.150523Z"},"id":"z5JCJeHi6yrn","trusted":true},"outputs":[],"source":["import torch.nn.functional as F\n","\n","class ContrastiveLossWithRegularization(nn.Module):\n","    def __init__(self, margin=2.5, regularization_strength=0.01):\n","        super(ContrastiveLossWithRegularization, self).__init__()\n","        self.margin = margin\n","        self.regularization_strength = regularization_strength\n","\n","    def forward(self, output1, output2, label, model):\n","        euclidean_distance = F.pairwise_distance(output1, output2, keepdim=True)\n","        loss_similarity = torch.mean((1 - label) * torch.pow(euclidean_distance, 2))\n","        loss_dissimilarity = torch.mean(label * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n","\n","        # Calculate L2 regularization term\n","        l2_regularization = 0.0\n","        for param in model.parameters():\n","            l2_regularization += torch.norm(param, p=2)\n","\n","        # Combine the contrastive loss and regularization\n","        total_loss = loss_similarity + loss_dissimilarity + self.regularization_strength * l2_regularization\n","\n","        return total_loss\n"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2023-10-22T11:26:14.639942Z","iopub.status.busy":"2023-10-22T11:26:14.639556Z","iopub.status.idle":"2023-10-22T11:26:14.648007Z","shell.execute_reply":"2023-10-22T11:26:14.647037Z","shell.execute_reply.started":"2023-10-22T11:26:14.639914Z"},"id":"cyAJYuL7PMDt","trusted":true},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset\n","from torchvision import transforms\n","from PIL import Image\n","\n","import random\n","from PIL import Image\n","\n","#Creating a custom dataset for loading the data into the Siamese network\n","\n","class SiameseDataset(Dataset):\n","    def __init__(self, paired_images, labels, transforms):\n","        self.paired_images = paired_images\n","        self.labels = labels\n","        self.transforms = transforms\n","\n","    def __len__(self):\n","        return len(self.paired_images)\n","\n","    def __getitem__(self, idx):\n","        image1_path, image2_path = self.paired_images[idx]\n","        label = self.labels[idx]\n","\n","        # Load the images\n","        image1 = Image.open(image1_path)\n","        image2 = Image.open(image2_path)\n","\n","        # Apply a transformation\n","        if self.transforms:\n","            image1 = self.transforms(image1)\n","            image2 = self.transforms(image2)\n","\n","        return image1, image2, label\n"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2023-10-22T11:26:18.569295Z","iopub.status.busy":"2023-10-22T11:26:18.568914Z","iopub.status.idle":"2023-10-22T11:26:18.573714Z","shell.execute_reply":"2023-10-22T11:26:18.572732Z","shell.execute_reply.started":"2023-10-22T11:26:18.569263Z"},"id":"TU9JSRcWli-r","trusted":true},"outputs":[],"source":["import random\n","import torch.nn.functional as F"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2023-10-22T11:26:21.807968Z","iopub.status.busy":"2023-10-22T11:26:21.807067Z","iopub.status.idle":"2023-10-22T11:26:21.813371Z","shell.execute_reply":"2023-10-22T11:26:21.812317Z","shell.execute_reply.started":"2023-10-22T11:26:21.807928Z"},"id":"utbswV_DW9ZR","trusted":true},"outputs":[],"source":["from torchvision import transforms\n","\n","# Using image augmentation before the images is passed to the Siamese Network\n","transform_list = transforms.Compose([\n","    transforms.RandomHorizontalFlip(),  # Random horizontal flip\n","    transforms.RandomRotation(15),  # Random rotation by 15 degrees\n","    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),  # Random color adjustments\n","    transforms.ToTensor(),  # Convert to a tensor\n","])\n"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2023-10-22T11:26:26.231382Z","iopub.status.busy":"2023-10-22T11:26:26.230657Z","iopub.status.idle":"2023-10-22T11:26:26.264636Z","shell.execute_reply":"2023-10-22T11:26:26.263651Z","shell.execute_reply.started":"2023-10-22T11:26:26.231351Z"},"id":"XMNcIzTbQbkw","trusted":true},"outputs":[],"source":["# train_path = '/content/drive/MyDrive/Assignment2/lfw/train_set'\n","# validation_path = '/content/drive/MyDrive/Assignment2/lfw/validation_set'\n","# test_path = '/content/drive/MyDrive/Assignment2/lfw/test_set'\n","\n","\n","# Check if CUDA is available\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","else:\n","    device = torch.device(\"cpu\")\n","\n","train_dataset = SiameseDataset(paired_images, labels, transforms = transform_list)\n","\n","batch_size = 32\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-10-22T11:26:29.258330Z","iopub.status.busy":"2023-10-22T11:26:29.257961Z","iopub.status.idle":"2023-10-22T11:26:29.274885Z","shell.execute_reply":"2023-10-22T11:26:29.273742Z","shell.execute_reply.started":"2023-10-22T11:26:29.258302Z"},"id":"aPk8y0xYf_8R","outputId":"6e5628fa-0b3c-4353-f6d9-4f4860eb16be","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["|===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n","|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n","|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n","|---------------------------------------------------------------------------|\n","| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |\n","|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n","|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n","|---------------------------------------------------------------------------|\n","| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n","|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n","|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |\n","|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n","|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\n","|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n","|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n","|---------------------------------------------------------------------------|\n","| Allocations           |       0    |       0    |       0    |       0    |\n","|       from large pool |       0    |       0    |       0    |       0    |\n","|       from small pool |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |       0    |       0    |       0    |       0    |\n","|       from large pool |       0    |       0    |       0    |       0    |\n","|       from small pool |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |       0    |       0    |       0    |       0    |\n","|       from large pool |       0    |       0    |       0    |       0    |\n","|       from small pool |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |       0    |       0    |       0    |       0    |\n","|       from large pool |       0    |       0    |       0    |       0    |\n","|       from small pool |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n"]}],"source":["torch.cuda.empty_cache()\n","print(torch.cuda.memory_summary(device=device, abbreviated=False))"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-10-22T11:26:33.657878Z","iopub.status.busy":"2023-10-22T11:26:33.656975Z","iopub.status.idle":"2023-10-22T11:26:37.492238Z","shell.execute_reply":"2023-10-22T11:26:37.491360Z","shell.execute_reply.started":"2023-10-22T11:26:33.657841Z"},"id":"GyDIcGCZnwul","outputId":"496af334-1743-48c6-90bf-0df663e53704","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n","100%|██████████| 97.8M/97.8M [00:00<00:00, 248MB/s]\n"]}],"source":["import torch.optim as optim\n","\n","margin = 2.5  # Adjust margin as needed\n","model = SiameseNetwork().to(device)\n","criterion = ContrastiveLossWithRegularization(margin)\n","\n","# Optimizer 1: Stochastic Gradient Descent (SGD)\n","optimizer_sgd = optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=1e-4)\n","\n","# Optimizer 2: Adam\n","optimizer_adam = optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-4)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-10-22T11:12:34.972535Z","iopub.status.idle":"2023-10-22T11:12:34.972930Z","shell.execute_reply":"2023-10-22T11:12:34.972721Z","shell.execute_reply.started":"2023-10-22T11:12:34.972700Z"},"id":"dHqJt44E63Hf","trusted":true},"outputs":[],"source":["# # Training loop\n","# num_epochs = 1  # Adjust as needed\n","# for epoch in range(num_epochs):\n","#     for i, data in enumerate(train_loader, 0):\n","#         input1, input2, label = data\n","#         input1, input2, label = input1.to(device), input2.to(device), label.to(device)\n","#         optimizer_adam.zero_grad()\n","#         output1, output2 = model(input1, input2)\n","#         loss = criterion(output1, output2, label, model)\n","#         loss.backward()\n","#         optimizer_adam.step()\n","\n","#         if (i + 1) % 10 == 0:\n","#             print(f\"Epoch [{epoch + 1}/{num_epochs}] Batch [{i + 1}/{len(train_loader)}] Loss: {loss.item():.4f}\")\n","\n","# # Adam optimizer with no learning rate scheduler"]},{"cell_type":"markdown","metadata":{"id":"GcxrJ8EpZqZD"},"source":["LR Scheduler: https://d2l.ai/chapter_optimization/lr-scheduler.html"]},{"cell_type":"markdown","metadata":{"id":"YedvzNpQQJu6"},"source":["Using 2 learning schedulers - Step and Cosine Annealing and 2 optimizers - SGD and ADAM."]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-10-22T11:26:41.295323Z","iopub.status.busy":"2023-10-22T11:26:41.294931Z","iopub.status.idle":"2023-10-22T12:15:25.979393Z","shell.execute_reply":"2023-10-22T12:15:25.978566Z","shell.execute_reply.started":"2023-10-22T11:26:41.295294Z"},"id":"s13l1GGNPgIB","outputId":"284c72aa-d94b-4bc7-e6bd-74c8c949fdb9","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [1/1] Batch [10/5524] Loss: 8.9178\n","Epoch [1/1] Batch [20/5524] Loss: 9.5055\n","Epoch [1/1] Batch [30/5524] Loss: 8.6697\n","Epoch [1/1] Batch [40/5524] Loss: 8.7592\n","Epoch [1/1] Batch [50/5524] Loss: 9.3062\n","Epoch [1/1] Batch [60/5524] Loss: 8.9356\n","Epoch [1/1] Batch [70/5524] Loss: 9.4482\n","Epoch [1/1] Batch [80/5524] Loss: 8.6295\n","Epoch [1/1] Batch [90/5524] Loss: 8.9881\n","Epoch [1/1] Batch [100/5524] Loss: 8.6265\n","Epoch [1/1] Batch [110/5524] Loss: 9.0295\n","Epoch [1/1] Batch [120/5524] Loss: 8.2396\n","Epoch [1/1] Batch [130/5524] Loss: 8.1142\n","Epoch [1/1] Batch [140/5524] Loss: 8.1545\n","Epoch [1/1] Batch [150/5524] Loss: 8.4778\n","Epoch [1/1] Batch [160/5524] Loss: 7.9510\n","Epoch [1/1] Batch [170/5524] Loss: 8.7907\n","Epoch [1/1] Batch [180/5524] Loss: 8.0745\n","Epoch [1/1] Batch [190/5524] Loss: 7.8371\n","Epoch [1/1] Batch [200/5524] Loss: 7.7365\n","Epoch [1/1] Batch [210/5524] Loss: 7.3097\n","Epoch [1/1] Batch [220/5524] Loss: 7.2163\n","Epoch [1/1] Batch [230/5524] Loss: 7.4405\n","Epoch [1/1] Batch [240/5524] Loss: 7.2293\n","Epoch [1/1] Batch [250/5524] Loss: 7.2371\n","Epoch [1/1] Batch [260/5524] Loss: 7.1616\n","Epoch [1/1] Batch [270/5524] Loss: 7.3351\n","Epoch [1/1] Batch [280/5524] Loss: 7.1026\n","Epoch [1/1] Batch [290/5524] Loss: 6.8613\n","Epoch [1/1] Batch [300/5524] Loss: 6.2994\n","Epoch [1/1] Batch [310/5524] Loss: 6.8016\n","Epoch [1/1] Batch [320/5524] Loss: 6.5504\n","Epoch [1/1] Batch [330/5524] Loss: 6.7589\n","Epoch [1/1] Batch [340/5524] Loss: 6.0329\n","Epoch [1/1] Batch [350/5524] Loss: 5.9190\n","Epoch [1/1] Batch [360/5524] Loss: 5.9858\n","Epoch [1/1] Batch [370/5524] Loss: 5.4132\n","Epoch [1/1] Batch [380/5524] Loss: 5.4544\n","Epoch [1/1] Batch [390/5524] Loss: 5.5269\n","Epoch [1/1] Batch [400/5524] Loss: 5.6008\n","Epoch [1/1] Batch [410/5524] Loss: 5.3329\n","Epoch [1/1] Batch [420/5524] Loss: 5.7104\n","Epoch [1/1] Batch [430/5524] Loss: 5.6081\n","Epoch [1/1] Batch [440/5524] Loss: 5.3741\n","Epoch [1/1] Batch [450/5524] Loss: 5.2674\n","Epoch [1/1] Batch [460/5524] Loss: 5.0142\n","Epoch [1/1] Batch [470/5524] Loss: 4.7727\n","Epoch [1/1] Batch [480/5524] Loss: 4.4821\n","Epoch [1/1] Batch [490/5524] Loss: 4.9041\n","Epoch [1/1] Batch [500/5524] Loss: 4.8231\n","Epoch [1/1] Batch [510/5524] Loss: 4.7440\n","Epoch [1/1] Batch [520/5524] Loss: 4.3342\n","Epoch [1/1] Batch [530/5524] Loss: 4.8538\n","Epoch [1/1] Batch [540/5524] Loss: 4.9266\n","Epoch [1/1] Batch [550/5524] Loss: 4.0620\n","Epoch [1/1] Batch [560/5524] Loss: 4.1491\n","Epoch [1/1] Batch [570/5524] Loss: 4.6681\n","Epoch [1/1] Batch [580/5524] Loss: 4.1238\n","Epoch [1/1] Batch [590/5524] Loss: 4.2155\n","Epoch [1/1] Batch [600/5524] Loss: 3.9656\n","Epoch [1/1] Batch [610/5524] Loss: 3.8948\n","Epoch [1/1] Batch [620/5524] Loss: 3.9808\n","Epoch [1/1] Batch [630/5524] Loss: 3.2730\n","Epoch [1/1] Batch [640/5524] Loss: 3.3905\n","Epoch [1/1] Batch [650/5524] Loss: 3.5860\n","Epoch [1/1] Batch [660/5524] Loss: 3.2114\n","Epoch [1/1] Batch [670/5524] Loss: 3.2697\n","Epoch [1/1] Batch [680/5524] Loss: 3.0483\n","Epoch [1/1] Batch [690/5524] Loss: 4.0790\n","Epoch [1/1] Batch [700/5524] Loss: 3.3849\n","Epoch [1/1] Batch [710/5524] Loss: 3.3118\n","Epoch [1/1] Batch [720/5524] Loss: 2.7798\n","Epoch [1/1] Batch [730/5524] Loss: 3.0321\n","Epoch [1/1] Batch [740/5524] Loss: 2.9752\n","Epoch [1/1] Batch [750/5524] Loss: 2.9182\n","Epoch [1/1] Batch [760/5524] Loss: 2.6956\n","Epoch [1/1] Batch [770/5524] Loss: 2.6482\n","Epoch [1/1] Batch [780/5524] Loss: 2.1650\n","Epoch [1/1] Batch [790/5524] Loss: 2.5670\n","Epoch [1/1] Batch [800/5524] Loss: 2.3601\n","Epoch [1/1] Batch [810/5524] Loss: 2.6270\n","Epoch [1/1] Batch [820/5524] Loss: 2.7346\n","Epoch [1/1] Batch [830/5524] Loss: 2.6998\n","Epoch [1/1] Batch [840/5524] Loss: 2.5449\n","Epoch [1/1] Batch [850/5524] Loss: 2.1756\n","Epoch [1/1] Batch [860/5524] Loss: 2.1617\n","Epoch [1/1] Batch [870/5524] Loss: 2.6077\n","Epoch [1/1] Batch [880/5524] Loss: 2.8817\n","Epoch [1/1] Batch [890/5524] Loss: 2.1795\n","Epoch [1/1] Batch [900/5524] Loss: 2.3073\n","Epoch [1/1] Batch [910/5524] Loss: 2.4599\n","Epoch [1/1] Batch [920/5524] Loss: 2.2461\n","Epoch [1/1] Batch [930/5524] Loss: 2.0564\n","Epoch [1/1] Batch [940/5524] Loss: 1.8532\n","Epoch [1/1] Batch [950/5524] Loss: 2.3079\n","Epoch [1/1] Batch [960/5524] Loss: 1.9879\n","Epoch [1/1] Batch [970/5524] Loss: 2.1558\n","Epoch [1/1] Batch [980/5524] Loss: 2.1427\n","Epoch [1/1] Batch [990/5524] Loss: 1.7971\n","Epoch [1/1] Batch [1000/5524] Loss: 2.2197\n","Epoch [1/1] Batch [1010/5524] Loss: 1.9010\n","Epoch [1/1] Batch [1020/5524] Loss: 1.9419\n","Epoch [1/1] Batch [1030/5524] Loss: 1.7411\n","Epoch [1/1] Batch [1040/5524] Loss: 2.0333\n","Epoch [1/1] Batch [1050/5524] Loss: 2.1359\n","Epoch [1/1] Batch [1060/5524] Loss: 2.2699\n","Epoch [1/1] Batch [1070/5524] Loss: 1.7474\n","Epoch [1/1] Batch [1080/5524] Loss: 2.2009\n","Epoch [1/1] Batch [1090/5524] Loss: 1.7127\n","Epoch [1/1] Batch [1100/5524] Loss: 1.7183\n","Epoch [1/1] Batch [1110/5524] Loss: 1.9062\n","Epoch [1/1] Batch [1120/5524] Loss: 1.8576\n","Epoch [1/1] Batch [1130/5524] Loss: 2.1700\n","Epoch [1/1] Batch [1140/5524] Loss: 1.1679\n","Epoch [1/1] Batch [1150/5524] Loss: 1.1818\n","Epoch [1/1] Batch [1160/5524] Loss: 1.4717\n","Epoch [1/1] Batch [1170/5524] Loss: 2.1355\n","Epoch [1/1] Batch [1180/5524] Loss: 2.0770\n","Epoch [1/1] Batch [1190/5524] Loss: 2.0872\n","Epoch [1/1] Batch [1200/5524] Loss: 1.7061\n","Epoch [1/1] Batch [1210/5524] Loss: 1.3529\n","Epoch [1/1] Batch [1220/5524] Loss: 1.6638\n","Epoch [1/1] Batch [1230/5524] Loss: 1.6336\n","Epoch [1/1] Batch [1240/5524] Loss: 1.7997\n","Epoch [1/1] Batch [1250/5524] Loss: 1.1513\n","Epoch [1/1] Batch [1260/5524] Loss: 1.1415\n","Epoch [1/1] Batch [1270/5524] Loss: 1.5957\n","Epoch [1/1] Batch [1280/5524] Loss: 1.2825\n","Epoch [1/1] Batch [1290/5524] Loss: 2.6576\n","Epoch [1/1] Batch [1300/5524] Loss: 1.3072\n","Epoch [1/1] Batch [1310/5524] Loss: 1.1908\n","Epoch [1/1] Batch [1320/5524] Loss: 1.8205\n","Epoch [1/1] Batch [1330/5524] Loss: 1.7970\n","Epoch [1/1] Batch [1340/5524] Loss: 1.9321\n","Epoch [1/1] Batch [1350/5524] Loss: 1.5633\n","Epoch [1/1] Batch [1360/5524] Loss: 1.2097\n","Epoch [1/1] Batch [1370/5524] Loss: 1.3449\n","Epoch [1/1] Batch [1380/5524] Loss: 1.3200\n","Epoch [1/1] Batch [1390/5524] Loss: 1.1426\n","Epoch [1/1] Batch [1400/5524] Loss: 1.2891\n","Epoch [1/1] Batch [1410/5524] Loss: 1.4501\n","Epoch [1/1] Batch [1420/5524] Loss: 1.7475\n","Epoch [1/1] Batch [1430/5524] Loss: 1.4130\n","Epoch [1/1] Batch [1440/5524] Loss: 1.2373\n","Epoch [1/1] Batch [1450/5524] Loss: 1.5402\n","Epoch [1/1] Batch [1460/5524] Loss: 2.0766\n","Epoch [1/1] Batch [1470/5524] Loss: 1.2039\n","Epoch [1/1] Batch [1480/5524] Loss: 1.9866\n","Epoch [1/1] Batch [1490/5524] Loss: 1.5174\n","Epoch [1/1] Batch [1500/5524] Loss: 1.0426\n","Epoch [1/1] Batch [1510/5524] Loss: 1.1969\n","Epoch [1/1] Batch [1520/5524] Loss: 1.0094\n","Epoch [1/1] Batch [1530/5524] Loss: 1.4759\n","Epoch [1/1] Batch [1540/5524] Loss: 1.1517\n","Epoch [1/1] Batch [1550/5524] Loss: 0.9668\n","Epoch [1/1] Batch [1560/5524] Loss: 1.3101\n","Epoch [1/1] Batch [1570/5524] Loss: 1.2920\n","Epoch [1/1] Batch [1580/5524] Loss: 1.4500\n","Epoch [1/1] Batch [1590/5524] Loss: 0.9478\n","Epoch [1/1] Batch [1600/5524] Loss: 1.4232\n","Epoch [1/1] Batch [1610/5524] Loss: 1.2578\n","Epoch [1/1] Batch [1620/5524] Loss: 1.9199\n","Epoch [1/1] Batch [1630/5524] Loss: 1.9056\n","Epoch [1/1] Batch [1640/5524] Loss: 0.8999\n","Epoch [1/1] Batch [1650/5524] Loss: 1.7445\n","Epoch [1/1] Batch [1660/5524] Loss: 1.0437\n","Epoch [1/1] Batch [1670/5524] Loss: 0.9161\n","Epoch [1/1] Batch [1680/5524] Loss: 0.8600\n","Epoch [1/1] Batch [1690/5524] Loss: 1.5034\n","Epoch [1/1] Batch [1700/5524] Loss: 1.6454\n","Epoch [1/1] Batch [1710/5524] Loss: 1.4877\n","Epoch [1/1] Batch [1720/5524] Loss: 0.7389\n","Epoch [1/1] Batch [1730/5524] Loss: 0.9951\n","Epoch [1/1] Batch [1740/5524] Loss: 1.3394\n","Epoch [1/1] Batch [1750/5524] Loss: 1.4944\n","Epoch [1/1] Batch [1760/5524] Loss: 1.1646\n","Epoch [1/1] Batch [1770/5524] Loss: 0.9966\n","Epoch [1/1] Batch [1780/5524] Loss: 0.9871\n","Epoch [1/1] Batch [1790/5524] Loss: 0.9790\n","Epoch [1/1] Batch [1800/5524] Loss: 0.9662\n","Epoch [1/1] Batch [1810/5524] Loss: 1.4524\n","Epoch [1/1] Batch [1820/5524] Loss: 0.9616\n","Epoch [1/1] Batch [1830/5524] Loss: 0.9491\n","Epoch [1/1] Batch [1840/5524] Loss: 1.1083\n","Epoch [1/1] Batch [1850/5524] Loss: 1.4366\n","Epoch [1/1] Batch [1860/5524] Loss: 1.4526\n","Epoch [1/1] Batch [1870/5524] Loss: 1.5893\n","Epoch [1/1] Batch [1880/5524] Loss: 1.4249\n","Epoch [1/1] Batch [1890/5524] Loss: 1.0787\n","Epoch [1/1] Batch [1900/5524] Loss: 0.9128\n","Epoch [1/1] Batch [1910/5524] Loss: 0.7625\n","Epoch [1/1] Batch [1920/5524] Loss: 1.0685\n","Epoch [1/1] Batch [1930/5524] Loss: 1.2155\n","Epoch [1/1] Batch [1940/5524] Loss: 0.9097\n","Epoch [1/1] Batch [1950/5524] Loss: 0.9001\n","Epoch [1/1] Batch [1960/5524] Loss: 1.0448\n","Epoch [1/1] Batch [1970/5524] Loss: 1.0659\n","Epoch [1/1] Batch [1980/5524] Loss: 1.0519\n","Epoch [1/1] Batch [1990/5524] Loss: 1.0480\n","Epoch [1/1] Batch [2000/5524] Loss: 0.8808\n","Epoch [1/1] Batch [2010/5524] Loss: 1.1869\n","Epoch [1/1] Batch [2020/5524] Loss: 0.7082\n","Epoch [1/1] Batch [2030/5524] Loss: 0.3696\n","Epoch [1/1] Batch [2040/5524] Loss: 0.5333\n","Epoch [1/1] Batch [2050/5524] Loss: 1.0346\n","Epoch [1/1] Batch [2060/5524] Loss: 1.3434\n","Epoch [1/1] Batch [2070/5524] Loss: 1.1846\n","Epoch [1/1] Batch [2080/5524] Loss: 1.4942\n","Epoch [1/1] Batch [2090/5524] Loss: 0.8561\n","Epoch [1/1] Batch [2100/5524] Loss: 1.1959\n","Epoch [1/1] Batch [2110/5524] Loss: 1.3203\n","Epoch [1/1] Batch [2120/5524] Loss: 1.6607\n","Epoch [1/1] Batch [2130/5524] Loss: 1.0020\n","Epoch [1/1] Batch [2140/5524] Loss: 0.7030\n","Epoch [1/1] Batch [2150/5524] Loss: 1.0047\n","Epoch [1/1] Batch [2160/5524] Loss: 0.7016\n","Epoch [1/1] Batch [2170/5524] Loss: 0.6730\n","Epoch [1/1] Batch [2180/5524] Loss: 0.8315\n","Epoch [1/1] Batch [2190/5524] Loss: 1.5113\n","Epoch [1/1] Batch [2200/5524] Loss: 1.8179\n","Epoch [1/1] Batch [2210/5524] Loss: 0.9930\n","Epoch [1/1] Batch [2220/5524] Loss: 1.1595\n","Epoch [1/1] Batch [2230/5524] Loss: 0.3567\n","Epoch [1/1] Batch [2240/5524] Loss: 0.9888\n","Epoch [1/1] Batch [2250/5524] Loss: 1.1354\n","Epoch [1/1] Batch [2260/5524] Loss: 1.6044\n","Epoch [1/1] Batch [2270/5524] Loss: 1.1688\n","Epoch [1/1] Batch [2280/5524] Loss: 0.8114\n","Epoch [1/1] Batch [2290/5524] Loss: 0.4866\n","Epoch [1/1] Batch [2300/5524] Loss: 1.4639\n","Epoch [1/1] Batch [2310/5524] Loss: 0.6597\n","Epoch [1/1] Batch [2320/5524] Loss: 0.3294\n","Epoch [1/1] Batch [2330/5524] Loss: 1.1464\n","Epoch [1/1] Batch [2340/5524] Loss: 0.8171\n","Epoch [1/1] Batch [2350/5524] Loss: 1.1135\n","Epoch [1/1] Batch [2360/5524] Loss: 1.1180\n","Epoch [1/1] Batch [2370/5524] Loss: 1.2695\n","Epoch [1/1] Batch [2380/5524] Loss: 1.1120\n","Epoch [1/1] Batch [2390/5524] Loss: 0.7938\n","Epoch [1/1] Batch [2400/5524] Loss: 0.4776\n","Epoch [1/1] Batch [2410/5524] Loss: 1.2783\n","Epoch [1/1] Batch [2420/5524] Loss: 0.3052\n","Epoch [1/1] Batch [2430/5524] Loss: 0.7815\n","Epoch [1/1] Batch [2440/5524] Loss: 0.9453\n","Epoch [1/1] Batch [2450/5524] Loss: 1.0976\n","Epoch [1/1] Batch [2460/5524] Loss: 0.7851\n","Epoch [1/1] Batch [2470/5524] Loss: 0.7869\n","Epoch [1/1] Batch [2480/5524] Loss: 1.1085\n","Epoch [1/1] Batch [2490/5524] Loss: 0.7748\n","Epoch [1/1] Batch [2500/5524] Loss: 0.7782\n","Epoch [1/1] Batch [2510/5524] Loss: 1.1081\n","Epoch [1/1] Batch [2520/5524] Loss: 0.4642\n","Epoch [1/1] Batch [2530/5524] Loss: 0.9382\n","Epoch [1/1] Batch [2540/5524] Loss: 1.1006\n","Epoch [1/1] Batch [2550/5524] Loss: 1.1130\n","Epoch [1/1] Batch [2560/5524] Loss: 0.9242\n","Epoch [1/1] Batch [2570/5524] Loss: 0.9305\n","Epoch [1/1] Batch [2580/5524] Loss: 0.7640\n","Epoch [1/1] Batch [2590/5524] Loss: 0.5982\n","Epoch [1/1] Batch [2600/5524] Loss: 1.0764\n","Epoch [1/1] Batch [2610/5524] Loss: 1.0718\n","Epoch [1/1] Batch [2620/5524] Loss: 0.6229\n","Epoch [1/1] Batch [2630/5524] Loss: 0.5982\n","Epoch [1/1] Batch [2640/5524] Loss: 0.9210\n","Epoch [1/1] Batch [2650/5524] Loss: 0.6103\n","Epoch [1/1] Batch [2660/5524] Loss: 0.7593\n","Epoch [1/1] Batch [2670/5524] Loss: 1.2505\n","Epoch [1/1] Batch [2680/5524] Loss: 0.7686\n","Epoch [1/1] Batch [2690/5524] Loss: 0.5931\n","Epoch [1/1] Batch [2700/5524] Loss: 0.9118\n","Epoch [1/1] Batch [2710/5524] Loss: 1.2252\n","Epoch [1/1] Batch [2720/5524] Loss: 0.5877\n","Epoch [1/1] Batch [2730/5524] Loss: 0.9102\n","Epoch [1/1] Batch [2740/5524] Loss: 0.9157\n","Epoch [1/1] Batch [2750/5524] Loss: 0.5735\n","Epoch [1/1] Batch [2760/5524] Loss: 0.7407\n","Epoch [1/1] Batch [2770/5524] Loss: 0.8997\n","Epoch [1/1] Batch [2780/5524] Loss: 1.0590\n","Epoch [1/1] Batch [2790/5524] Loss: 0.7359\n","Epoch [1/1] Batch [2800/5524] Loss: 0.4006\n","Epoch [1/1] Batch [2810/5524] Loss: 1.5433\n","Epoch [1/1] Batch [2820/5524] Loss: 0.7508\n","Epoch [1/1] Batch [2830/5524] Loss: 1.2206\n","Epoch [1/1] Batch [2840/5524] Loss: 0.7462\n","Epoch [1/1] Batch [2850/5524] Loss: 0.8935\n","Epoch [1/1] Batch [2860/5524] Loss: 0.5611\n","Epoch [1/1] Batch [2870/5524] Loss: 1.2348\n","Epoch [1/1] Batch [2880/5524] Loss: 0.4439\n","Epoch [1/1] Batch [2890/5524] Loss: 0.7313\n","Epoch [1/1] Batch [2900/5524] Loss: 0.8927\n","Epoch [1/1] Batch [2910/5524] Loss: 0.8865\n","Epoch [1/1] Batch [2920/5524] Loss: 0.8889\n","Epoch [1/1] Batch [2930/5524] Loss: 1.5013\n","Epoch [1/1] Batch [2940/5524] Loss: 0.5693\n","Epoch [1/1] Batch [2950/5524] Loss: 1.5492\n","Epoch [1/1] Batch [2960/5524] Loss: 1.0423\n","Epoch [1/1] Batch [2970/5524] Loss: 0.8828\n","Epoch [1/1] Batch [2980/5524] Loss: 1.0433\n","Epoch [1/1] Batch [2990/5524] Loss: 1.2128\n","Epoch [1/1] Batch [3000/5524] Loss: 1.0201\n","Epoch [1/1] Batch [3010/5524] Loss: 0.5633\n","Epoch [1/1] Batch [3020/5524] Loss: 1.2193\n","Epoch [1/1] Batch [3030/5524] Loss: 1.0474\n","Epoch [1/1] Batch [3040/5524] Loss: 0.5475\n","Epoch [1/1] Batch [3050/5524] Loss: 0.7116\n","Epoch [1/1] Batch [3060/5524] Loss: 1.2092\n","Epoch [1/1] Batch [3070/5524] Loss: 1.0306\n","Epoch [1/1] Batch [3080/5524] Loss: 0.8762\n","Epoch [1/1] Batch [3090/5524] Loss: 0.5372\n","Epoch [1/1] Batch [3100/5524] Loss: 1.3540\n","Epoch [1/1] Batch [3110/5524] Loss: 0.7057\n","Epoch [1/1] Batch [3120/5524] Loss: 0.8623\n","Epoch [1/1] Batch [3130/5524] Loss: 0.5344\n","Epoch [1/1] Batch [3140/5524] Loss: 0.5325\n","Epoch [1/1] Batch [3150/5524] Loss: 0.3664\n","Epoch [1/1] Batch [3160/5524] Loss: 0.8534\n","Epoch [1/1] Batch [3170/5524] Loss: 0.8584\n","Epoch [1/1] Batch [3180/5524] Loss: 0.5311\n","Epoch [1/1] Batch [3190/5524] Loss: 0.6934\n","Epoch [1/1] Batch [3200/5524] Loss: 0.5330\n","Epoch [1/1] Batch [3210/5524] Loss: 1.0004\n","Epoch [1/1] Batch [3220/5524] Loss: 0.5311\n","Epoch [1/1] Batch [3230/5524] Loss: 0.8483\n","Epoch [1/1] Batch [3240/5524] Loss: 0.8522\n","Epoch [1/1] Batch [3250/5524] Loss: 1.0121\n","Epoch [1/1] Batch [3260/5524] Loss: 1.1477\n","Epoch [1/1] Batch [3270/5524] Loss: 0.5351\n","Epoch [1/1] Batch [3280/5524] Loss: 0.5238\n","Epoch [1/1] Batch [3290/5524] Loss: 1.0058\n","Epoch [1/1] Batch [3300/5524] Loss: 0.5327\n","Epoch [1/1] Batch [3310/5524] Loss: 1.1767\n","Epoch [1/1] Batch [3320/5524] Loss: 0.9968\n","Epoch [1/1] Batch [3330/5524] Loss: 0.9984\n","Epoch [1/1] Batch [3340/5524] Loss: 0.8426\n","Epoch [1/1] Batch [3350/5524] Loss: 0.5110\n","Epoch [1/1] Batch [3360/5524] Loss: 0.3393\n","Epoch [1/1] Batch [3370/5524] Loss: 0.5116\n","Epoch [1/1] Batch [3380/5524] Loss: 0.9971\n","Epoch [1/1] Batch [3390/5524] Loss: 0.3686\n","Epoch [1/1] Batch [3400/5524] Loss: 1.4757\n","Epoch [1/1] Batch [3410/5524] Loss: 0.6814\n","Epoch [1/1] Batch [3420/5524] Loss: 0.5166\n","Epoch [1/1] Batch [3430/5524] Loss: 1.1398\n","Epoch [1/1] Batch [3440/5524] Loss: 0.6767\n","Epoch [1/1] Batch [3450/5524] Loss: 1.1517\n","Epoch [1/1] Batch [3460/5524] Loss: 0.8314\n","Epoch [1/1] Batch [3470/5524] Loss: 0.6682\n","Epoch [1/1] Batch [3480/5524] Loss: 0.3478\n","Epoch [1/1] Batch [3490/5524] Loss: 0.3523\n","Epoch [1/1] Batch [3500/5524] Loss: 1.8316\n","Epoch [1/1] Batch [3510/5524] Loss: 0.1723\n","Epoch [1/1] Batch [3520/5524] Loss: 0.5083\n","Epoch [1/1] Batch [3530/5524] Loss: 0.6650\n","Epoch [1/1] Batch [3540/5524] Loss: 0.4945\n","Epoch [1/1] Batch [3550/5524] Loss: 0.6600\n","Epoch [1/1] Batch [3560/5524] Loss: 0.6622\n","Epoch [1/1] Batch [3570/5524] Loss: 0.4982\n","Epoch [1/1] Batch [3580/5524] Loss: 0.5109\n","Epoch [1/1] Batch [3590/5524] Loss: 0.9760\n","Epoch [1/1] Batch [3600/5524] Loss: 0.8216\n","Epoch [1/1] Batch [3610/5524] Loss: 0.9854\n","Epoch [1/1] Batch [3620/5524] Loss: 0.6589\n","Epoch [1/1] Batch [3630/5524] Loss: 1.3140\n","Epoch [1/1] Batch [3640/5524] Loss: 0.1527\n","Epoch [1/1] Batch [3650/5524] Loss: 0.6553\n","Epoch [1/1] Batch [3660/5524] Loss: 0.8133\n","Epoch [1/1] Batch [3670/5524] Loss: 1.1534\n","Epoch [1/1] Batch [3680/5524] Loss: 0.6578\n","Epoch [1/1] Batch [3690/5524] Loss: 0.8115\n","Epoch [1/1] Batch [3700/5524] Loss: 0.9740\n","Epoch [1/1] Batch [3710/5524] Loss: 0.9654\n","Epoch [1/1] Batch [3720/5524] Loss: 1.1295\n","Epoch [1/1] Batch [3730/5524] Loss: 1.5031\n","Epoch [1/1] Batch [3740/5524] Loss: 0.6530\n","Epoch [1/1] Batch [3750/5524] Loss: 0.8113\n","Epoch [1/1] Batch [3760/5524] Loss: 0.9771\n","Epoch [1/1] Batch [3770/5524] Loss: 0.8147\n","Epoch [1/1] Batch [3780/5524] Loss: 0.4938\n","Epoch [1/1] Batch [3790/5524] Loss: 0.8097\n","Epoch [1/1] Batch [3800/5524] Loss: 0.6476\n","Epoch [1/1] Batch [3810/5524] Loss: 0.6502\n","Epoch [1/1] Batch [3820/5524] Loss: 0.8113\n","Epoch [1/1] Batch [3830/5524] Loss: 0.4912\n","Epoch [1/1] Batch [3840/5524] Loss: 1.1251\n","Epoch [1/1] Batch [3850/5524] Loss: 0.4828\n","Epoch [1/1] Batch [3860/5524] Loss: 0.9758\n","Epoch [1/1] Batch [3870/5524] Loss: 0.6435\n","Epoch [1/1] Batch [3880/5524] Loss: 0.8083\n","Epoch [1/1] Batch [3890/5524] Loss: 0.3099\n","Epoch [1/1] Batch [3900/5524] Loss: 0.8040\n","Epoch [1/1] Batch [3910/5524] Loss: 0.8006\n","Epoch [1/1] Batch [3920/5524] Loss: 1.1033\n","Epoch [1/1] Batch [3930/5524] Loss: 0.6373\n","Epoch [1/1] Batch [3940/5524] Loss: 0.6408\n","Epoch [1/1] Batch [3950/5524] Loss: 0.8028\n","Epoch [1/1] Batch [3960/5524] Loss: 0.3060\n","Epoch [1/1] Batch [3970/5524] Loss: 0.6319\n","Epoch [1/1] Batch [3980/5524] Loss: 0.6399\n","Epoch [1/1] Batch [3990/5524] Loss: 0.8067\n","Epoch [1/1] Batch [4000/5524] Loss: 0.4861\n","Epoch [1/1] Batch [4010/5524] Loss: 1.2610\n","Epoch [1/1] Batch [4020/5524] Loss: 0.7943\n","Epoch [1/1] Batch [4030/5524] Loss: 0.3316\n","Epoch [1/1] Batch [4040/5524] Loss: 0.7946\n","Epoch [1/1] Batch [4050/5524] Loss: 0.9640\n","Epoch [1/1] Batch [4060/5524] Loss: 0.4686\n","Epoch [1/1] Batch [4070/5524] Loss: 0.9576\n","Epoch [1/1] Batch [4080/5524] Loss: 0.7954\n","Epoch [1/1] Batch [4090/5524] Loss: 0.6314\n","Epoch [1/1] Batch [4100/5524] Loss: 0.7961\n","Epoch [1/1] Batch [4110/5524] Loss: 0.7942\n","Epoch [1/1] Batch [4120/5524] Loss: 0.1374\n","Epoch [1/1] Batch [4130/5524] Loss: 1.1349\n","Epoch [1/1] Batch [4140/5524] Loss: 0.6301\n","Epoch [1/1] Batch [4150/5524] Loss: 0.7898\n","Epoch [1/1] Batch [4160/5524] Loss: 0.9575\n","Epoch [1/1] Batch [4170/5524] Loss: 1.7901\n","Epoch [1/1] Batch [4180/5524] Loss: 0.4691\n","Epoch [1/1] Batch [4190/5524] Loss: 0.7931\n","Epoch [1/1] Batch [4200/5524] Loss: 1.1193\n","Epoch [1/1] Batch [4210/5524] Loss: 0.4688\n","Epoch [1/1] Batch [4220/5524] Loss: 0.9768\n","Epoch [1/1] Batch [4230/5524] Loss: 0.7990\n","Epoch [1/1] Batch [4240/5524] Loss: 0.7964\n","Epoch [1/1] Batch [4250/5524] Loss: 1.1094\n","Epoch [1/1] Batch [4260/5524] Loss: 0.3221\n","Epoch [1/1] Batch [4270/5524] Loss: 0.4746\n","Epoch [1/1] Batch [4280/5524] Loss: 0.6233\n","Epoch [1/1] Batch [4290/5524] Loss: 1.1332\n","Epoch [1/1] Batch [4300/5524] Loss: 0.4582\n","Epoch [1/1] Batch [4310/5524] Loss: 0.6256\n","Epoch [1/1] Batch [4320/5524] Loss: 0.4650\n","Epoch [1/1] Batch [4330/5524] Loss: 0.3079\n","Epoch [1/1] Batch [4340/5524] Loss: 0.3079\n","Epoch [1/1] Batch [4350/5524] Loss: 0.7888\n","Epoch [1/1] Batch [4360/5524] Loss: 0.9536\n","Epoch [1/1] Batch [4370/5524] Loss: 0.7840\n","Epoch [1/1] Batch [4380/5524] Loss: 0.4573\n","Epoch [1/1] Batch [4390/5524] Loss: 0.4544\n","Epoch [1/1] Batch [4400/5524] Loss: 0.6213\n","Epoch [1/1] Batch [4410/5524] Loss: 0.4630\n","Epoch [1/1] Batch [4420/5524] Loss: 0.6266\n","Epoch [1/1] Batch [4430/5524] Loss: 0.7830\n","Epoch [1/1] Batch [4440/5524] Loss: 0.9559\n","Epoch [1/1] Batch [4450/5524] Loss: 0.1258\n","Epoch [1/1] Batch [4460/5524] Loss: 0.4572\n","Epoch [1/1] Batch [4470/5524] Loss: 1.2788\n","Epoch [1/1] Batch [4480/5524] Loss: 0.7810\n","Epoch [1/1] Batch [4490/5524] Loss: 0.6207\n","Epoch [1/1] Batch [4500/5524] Loss: 0.4543\n","Epoch [1/1] Batch [4510/5524] Loss: 0.9541\n","Epoch [1/1] Batch [4520/5524] Loss: 1.1288\n","Epoch [1/1] Batch [4530/5524] Loss: 1.2451\n","Epoch [1/1] Batch [4540/5524] Loss: 1.2400\n","Epoch [1/1] Batch [4550/5524] Loss: 0.7782\n","Epoch [1/1] Batch [4560/5524] Loss: 0.3013\n","Epoch [1/1] Batch [4570/5524] Loss: 0.6194\n","Epoch [1/1] Batch [4580/5524] Loss: 0.6169\n","Epoch [1/1] Batch [4590/5524] Loss: 0.4613\n","Epoch [1/1] Batch [4600/5524] Loss: 0.6181\n","Epoch [1/1] Batch [4610/5524] Loss: 0.2992\n","Epoch [1/1] Batch [4620/5524] Loss: 0.9386\n","Epoch [1/1] Batch [4630/5524] Loss: 0.4705\n","Epoch [1/1] Batch [4640/5524] Loss: 0.3006\n","Epoch [1/1] Batch [4650/5524] Loss: 0.7789\n","Epoch [1/1] Batch [4660/5524] Loss: 0.6224\n","Epoch [1/1] Batch [4670/5524] Loss: 0.3001\n","Epoch [1/1] Batch [4680/5524] Loss: 0.7825\n","Epoch [1/1] Batch [4690/5524] Loss: 0.4631\n","Epoch [1/1] Batch [4700/5524] Loss: 0.4702\n","Epoch [1/1] Batch [4710/5524] Loss: 0.6215\n","Epoch [1/1] Batch [4720/5524] Loss: 0.7865\n","Epoch [1/1] Batch [4730/5524] Loss: 0.7873\n","Epoch [1/1] Batch [4740/5524] Loss: 0.7796\n","Epoch [1/1] Batch [4750/5524] Loss: 0.4540\n","Epoch [1/1] Batch [4760/5524] Loss: 1.1080\n","Epoch [1/1] Batch [4770/5524] Loss: 0.7838\n","Epoch [1/1] Batch [4780/5524] Loss: 0.4495\n","Epoch [1/1] Batch [4790/5524] Loss: 1.1268\n","Epoch [1/1] Batch [4800/5524] Loss: 0.6122\n","Epoch [1/1] Batch [4810/5524] Loss: 0.9351\n","Epoch [1/1] Batch [4820/5524] Loss: 0.4494\n","Epoch [1/1] Batch [4830/5524] Loss: 0.7842\n","Epoch [1/1] Batch [4840/5524] Loss: 0.4459\n","Epoch [1/1] Batch [4850/5524] Loss: 0.1041\n","Epoch [1/1] Batch [4860/5524] Loss: 0.7877\n","Epoch [1/1] Batch [4870/5524] Loss: 1.2875\n","Epoch [1/1] Batch [4880/5524] Loss: 0.9451\n","Epoch [1/1] Batch [4890/5524] Loss: 0.4449\n","Epoch [1/1] Batch [4900/5524] Loss: 0.9447\n","Epoch [1/1] Batch [4910/5524] Loss: 0.7769\n","Epoch [1/1] Batch [4920/5524] Loss: 0.7798\n","Epoch [1/1] Batch [4930/5524] Loss: 0.9328\n","Epoch [1/1] Batch [4940/5524] Loss: 0.7713\n","Epoch [1/1] Batch [4950/5524] Loss: 0.7782\n","Epoch [1/1] Batch [4960/5524] Loss: 0.7758\n","Epoch [1/1] Batch [4970/5524] Loss: 0.6159\n","Epoch [1/1] Batch [4980/5524] Loss: 0.6115\n","Epoch [1/1] Batch [4990/5524] Loss: 0.6161\n","Epoch [1/1] Batch [5000/5524] Loss: 1.0983\n","Epoch [1/1] Batch [5010/5524] Loss: 0.7801\n","Epoch [1/1] Batch [5020/5524] Loss: 0.7778\n","Epoch [1/1] Batch [5030/5524] Loss: 0.7737\n","Epoch [1/1] Batch [5040/5524] Loss: 0.6140\n","Epoch [1/1] Batch [5050/5524] Loss: 0.7820\n","Epoch [1/1] Batch [5060/5524] Loss: 0.9605\n","Epoch [1/1] Batch [5070/5524] Loss: 0.7863\n","Epoch [1/1] Batch [5080/5524] Loss: 0.2771\n","Epoch [1/1] Batch [5090/5524] Loss: 0.6129\n","Epoch [1/1] Batch [5100/5524] Loss: 0.7779\n","Epoch [1/1] Batch [5110/5524] Loss: 0.7788\n","Epoch [1/1] Batch [5120/5524] Loss: 0.6166\n","Epoch [1/1] Batch [5130/5524] Loss: 1.4153\n","Epoch [1/1] Batch [5140/5524] Loss: 0.4628\n","Epoch [1/1] Batch [5150/5524] Loss: 0.4663\n","Epoch [1/1] Batch [5160/5524] Loss: 1.4304\n","Epoch [1/1] Batch [5170/5524] Loss: 0.4572\n","Epoch [1/1] Batch [5180/5524] Loss: 0.7731\n","Epoch [1/1] Batch [5190/5524] Loss: 0.9334\n","Epoch [1/1] Batch [5200/5524] Loss: 0.2880\n","Epoch [1/1] Batch [5210/5524] Loss: 0.4506\n","Epoch [1/1] Batch [5220/5524] Loss: 0.4509\n","Epoch [1/1] Batch [5230/5524] Loss: 0.7755\n","Epoch [1/1] Batch [5240/5524] Loss: 0.2842\n","Epoch [1/1] Batch [5250/5524] Loss: 0.6133\n","Epoch [1/1] Batch [5260/5524] Loss: 0.4445\n","Epoch [1/1] Batch [5270/5524] Loss: 0.9364\n","Epoch [1/1] Batch [5280/5524] Loss: 0.7705\n","Epoch [1/1] Batch [5290/5524] Loss: 1.1134\n","Epoch [1/1] Batch [5300/5524] Loss: 0.6119\n","Epoch [1/1] Batch [5310/5524] Loss: 0.4424\n","Epoch [1/1] Batch [5320/5524] Loss: 0.7755\n","Epoch [1/1] Batch [5330/5524] Loss: 1.4394\n","Epoch [1/1] Batch [5340/5524] Loss: 0.7727\n","Epoch [1/1] Batch [5350/5524] Loss: 0.2946\n","Epoch [1/1] Batch [5360/5524] Loss: 0.9325\n","Epoch [1/1] Batch [5370/5524] Loss: 0.7813\n","Epoch [1/1] Batch [5380/5524] Loss: 0.4499\n","Epoch [1/1] Batch [5390/5524] Loss: 0.2823\n","Epoch [1/1] Batch [5400/5524] Loss: 0.7776\n","Epoch [1/1] Batch [5410/5524] Loss: 0.7763\n","Epoch [1/1] Batch [5420/5524] Loss: 1.2633\n","Epoch [1/1] Batch [5430/5524] Loss: 0.7696\n","Epoch [1/1] Batch [5440/5524] Loss: 0.4510\n","Epoch [1/1] Batch [5450/5524] Loss: 0.7803\n","Epoch [1/1] Batch [5460/5524] Loss: 0.6145\n","Epoch [1/1] Batch [5470/5524] Loss: 1.1106\n","Epoch [1/1] Batch [5480/5524] Loss: 1.0944\n","Epoch [1/1] Batch [5490/5524] Loss: 0.7809\n","Epoch [1/1] Batch [5500/5524] Loss: 0.9444\n","Epoch [1/1] Batch [5510/5524] Loss: 0.2953\n","Epoch [1/1] Batch [5520/5524] Loss: 0.7739\n"]}],"source":["from torch.optim.lr_scheduler import CosineAnnealingLR\n","num_epochs = 1\n","\n","# Create a CosineAnnealingLR scheduler\n","scheduler1 = CosineAnnealingLR(optimizer_adam, T_max=num_epochs)\n","\n","for epoch in range(num_epochs):\n","    for i, data in enumerate(train_loader, 0):\n","        input1, input2, label = data\n","        input1, input2, label = input1.to(device), input2.to(device), label.to(device)\n","        optimizer_adam.zero_grad()\n","        output1, output2 = model(input1, input2)\n","        loss = criterion(output1, output2, label, model)\n","        loss.backward()\n","        optimizer_adam.step()\n","\n","        # Update the learning rate after each optimizer step\n","        scheduler1.step()\n","\n","        if (i + 1) % 10 == 0:\n","            print(f\"Epoch [{epoch + 1}/{num_epochs}] Batch [{i + 1}/{len(train_loader)}] Loss: {loss.item():.4f}\")\n"]},{"cell_type":"markdown","metadata":{"id":"tynfOv1vIMNY"},"source":["Code below resizes the custom images to desired size"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2023-10-22T11:12:34.975870Z","iopub.status.idle":"2023-10-22T11:12:34.976307Z","shell.execute_reply":"2023-10-22T11:12:34.976099Z","shell.execute_reply.started":"2023-10-22T11:12:34.976078Z"},"id":"kA76y2WFcLxz","outputId":"cf6364f7-6fa0-4d44-a7c4-0d076f02874c","trusted":true},"outputs":[],"source":["# from PIL import Image\n","# import os\n","\n","# # Define the output size for the resized images\n","# output_size = (200, 200)  # Change this to the desired size\n","\n","# input_path = '/content/drive/MyDrive/Assignment2/lfw/Custom/'\n","# output_path = '/content/drive/MyDrive/Assignment2/lfw/Custom1/'\n","\n","# file_count = 0\n","\n","# for entry in os.listdir(input_path):\n","#     entry_path = os.path.join(input_path, entry)\n","\n","#     if os.path.isdir(entry_path):\n","#         # Create a subdirectory to store the cropped and resized images\n","\n","#         output_dir = os.path.join(output_path, entry)\n","#         os.makedirs(output_dir, exist_ok=True)\n","\n","#         for fil in os.listdir(entry_path):\n","#             file_path = os.path.join(entry_path, fil)\n","\n","#             if file_path.endswith(('.jpg', '.jpeg', '.png', '.gif')):\n","#                 # Open the image\n","#                 img = Image.open(file_path)\n","\n","#                 # Resize the image to the specified output size\n","#                 img = img.resize(output_size)\n","\n","#                 # Save the resized image to the output directory\n","#                 output_file_path = os.path.join(output_dir, fil)\n","#                 img.save(output_file_path)\n","\n","#         file_count += 1\n","\n","# print(f\"All images are resized to {output_size[0]}x{output_size[1]} and saved in the directory.\")\n","\n","# # Resizing the custom images"]},{"cell_type":"markdown","metadata":{"id":"-6b_iBEHITjt"},"source":["Creating a validation and test dataloader"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":247},"execution":{"iopub.execute_input":"2023-10-22T12:15:38.121486Z","iopub.status.busy":"2023-10-22T12:15:38.121038Z","iopub.status.idle":"2023-10-22T12:15:45.829927Z","shell.execute_reply":"2023-10-22T12:15:45.828869Z","shell.execute_reply.started":"2023-10-22T12:15:38.121429Z"},"id":"ZMLfgT52TQkn","outputId":"ecf8a60f-2ae5-4152-ffcd-b239bd25d7c7","trusted":true},"outputs":[],"source":["import os\n","import random\n","\n","# path to the dataset\n","val_dir = '/kaggle/input/lfwdataset1/lfw/validation_set'\n","# val_dir = '/content/drive/MyDrive/Assignment2/lfw/validation_set'\n","\n","# Get a list of subfolder for each person\n","person_folders = [os.path.join(val_dir, folder) for folder in os.listdir(val_dir)]\n","\n","# Initialize lists to store positive and negative pairs\n","positive_pairsv = []\n","negative_pairsv = []\n","\n","# Generate pairs of similar and dissimilar images\n","for folder in person_folders:\n","    images = os.listdir(folder)\n","    if len(images) >= 2:\n","        # Generate positive pairs\n","        positive_pairsv.extend([(os.path.join(folder, images[i]), os.path.join(folder, images[j])) for i in range(len(images)) for j in range(i + 1, len(images))])\n","\n","# Generate negative pairs\n","for i in range(len(person_folders)):\n","    for j in range(i + 1, len(person_folders)):\n","        folder1_images = os.listdir(person_folders[i])\n","        folder2_images = os.listdir(person_folders[j])\n","        if folder1_images and folder2_images:\n","            negative_pairsv.append((os.path.join(person_folders[i], random.choice(folder1_images)), os.path.join(person_folders[j], random.choice(folder2_images))))\n"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2023-10-22T12:15:59.154274Z","iopub.status.busy":"2023-10-22T12:15:59.153919Z","iopub.status.idle":"2023-10-22T12:16:03.381870Z","shell.execute_reply":"2023-10-22T12:16:03.380946Z","shell.execute_reply.started":"2023-10-22T12:15:59.154247Z"},"id":"vXP1QAezUK0l","trusted":true},"outputs":[],"source":["import os\n","import random\n","\n","# path to the dataset\n","# test_dir = '/content/drive/MyDrive/Assignment2/lfw/test_set'\n","test_dir = '/kaggle/input/lfwdataset1/lfw/test_set'\n","\n","\n","# Get a list of subfolder for each person\n","person_folders = [os.path.join(test_dir, folder) for folder in os.listdir(test_dir)]\n","\n","# Initialize lists to store positive and negative pairs\n","positive_pairst = []\n","negative_pairst = []\n","\n","# Generate pairs of similar and dissimilar images\n","for folder in person_folders:\n","    images = os.listdir(folder)\n","    if len(images) >= 2:\n","        # Generate positive pairs\n","        positive_pairst.extend([(os.path.join(folder, images[i]), os.path.join(folder, images[j])) for i in range(len(images)) for j in range(i + 1, len(images))])\n","\n","# Generate negative pairs\n","for i in range(len(person_folders)):\n","    for j in range(i + 1, len(person_folders)):\n","        folder1_images = os.listdir(person_folders[i])\n","        folder2_images = os.listdir(person_folders[j])\n","        if folder1_images and folder2_images:\n","            negative_pairst.append((os.path.join(person_folders[i], random.choice(folder1_images)), os.path.join(person_folders[j], random.choice(folder2_images))))\n"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-10-22T12:16:03.383842Z","iopub.status.busy":"2023-10-22T12:16:03.383541Z","iopub.status.idle":"2023-10-22T12:16:03.390085Z","shell.execute_reply":"2023-10-22T12:16:03.388974Z","shell.execute_reply.started":"2023-10-22T12:16:03.383816Z"},"id":"siZo_lmLTSOT","outputId":"af0ace5a-ffa8-4d74-bae4-3346beb8ba91","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["('/kaggle/input/lfwdataset1/lfw/validation_set/Svend_Robinson/Svend_Robinson_0001.jpg', '/kaggle/input/lfwdataset1/lfw/validation_set/Costas_Simitis/Costas_Simitis_0005.jpg')\n","8599 9730\n"]}],"source":["# Assign labels to positive and negative pairs\n","positive_labelsv = [1] * len(positive_pairsv)\n","negative_labelsv = [0] * len(negative_pairsv)\n","\n","print(negative_pairsv[0])\n","print(len(positive_pairsv), len(negative_pairsv))\n","# Combine positive and negative pairs and labels\n","validation_paired_images = positive_pairsv + negative_pairsv\n","validation_labels = positive_labelsv + negative_labelsv"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-10-22T12:16:07.129310Z","iopub.status.busy":"2023-10-22T12:16:07.128592Z","iopub.status.idle":"2023-10-22T12:16:07.135697Z","shell.execute_reply":"2023-10-22T12:16:07.134630Z","shell.execute_reply.started":"2023-10-22T12:16:07.129272Z"},"id":"tWHiCHpqUztN","outputId":"88a2a915-7014-4148-e675-927682fb5c4d","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["('/kaggle/input/lfwdataset1/lfw/test_set/Coco_dEste/Coco_dEste_0001.jpg', '/kaggle/input/lfwdataset1/lfw/test_set/Barbara_Bodine/Barbara_Bodine_0001.jpg')\n","2467 5050\n"]}],"source":["# Assign labels to positive and negative pairs\n","positive_labelst = [1] * len(positive_pairst)\n","negative_labelst = [0] * len(negative_pairst)\n","\n","print(negative_pairst[0])\n","print(len(positive_pairst), len(negative_pairst))\n","# Combine positive and negative pairs and labels\n","test_paired_images = positive_pairst + negative_pairst\n","test_labels = positive_labelst + negative_labelst"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2023-10-22T12:16:44.338195Z","iopub.status.busy":"2023-10-22T12:16:44.337803Z","iopub.status.idle":"2023-10-22T12:16:44.357181Z","shell.execute_reply":"2023-10-22T12:16:44.356271Z","shell.execute_reply.started":"2023-10-22T12:16:44.338160Z"},"id":"_--7TaJYbcab","trusted":true},"outputs":[],"source":["import os\n","import random\n","\n","# path to the dataset\n","# c_dir = '/content/drive/MyDrive/Assignment2/lfw/Custom1'\n","c_dir = '/kaggle/input/lfwdataset1/lfw/Custom1'\n","\n","# Get a list of subfolder for each person\n","person_folders = [os.path.join(c_dir, folder) for folder in os.listdir(c_dir)]\n","\n","# Initialize lists to store positive and negative pairs\n","positive_pairsc = []\n","negative_pairsc = []\n","\n","# Generate pairs of similar and dissimilar images\n","for folder in person_folders:\n","    images = os.listdir(folder)\n","    if len(images) >= 2:\n","        # Generate positive pairs\n","        positive_pairsc.extend([(os.path.join(folder, images[i]), os.path.join(folder, images[j])) for i in range(len(images)) for j in range(i + 1, len(images))])\n","\n","# Generate negative pairs\n","for i in range(len(person_folders)):\n","    for j in range(i + 1, len(person_folders)):\n","        folder1_images = os.listdir(person_folders[i])\n","        folder2_images = os.listdir(person_folders[j])\n","        if folder1_images and folder2_images:\n","            negative_pairsc.append((os.path.join(person_folders[i], random.choice(folder1_images)), os.path.join(person_folders[j], random.choice(folder2_images))))\n"]},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-10-22T12:16:48.525000Z","iopub.status.busy":"2023-10-22T12:16:48.524113Z","iopub.status.idle":"2023-10-22T12:16:48.531267Z","shell.execute_reply":"2023-10-22T12:16:48.530266Z","shell.execute_reply.started":"2023-10-22T12:16:48.524951Z"},"id":"uOWBsCBmbd3B","outputId":"122c986f-2b1d-4d6b-88c4-451f76f92eb5","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["('/kaggle/input/lfwdataset1/lfw/Custom1/2/Screenshot 2023-10-21 233210.png', '/kaggle/input/lfwdataset1/lfw/Custom1/1/IMG_20230317_223910.jpg')\n","2 1\n"]}],"source":["# Assign labels to positive and negative pairs\n","positive_labelsc = [1] * len(positive_pairsc)\n","negative_labelsc = [0] * len(negative_pairsc)\n","\n","print(negative_pairsc[0])\n","print(len(positive_pairsc), len(negative_pairsc))\n","# Combine positive and negative pairs and labels\n","custom_paired_images = positive_pairsc + negative_pairsc\n","custom_labels = positive_labelsc + negative_labelsc"]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":247},"execution":{"iopub.execute_input":"2023-10-22T12:16:56.411819Z","iopub.status.busy":"2023-10-22T12:16:56.411409Z","iopub.status.idle":"2023-10-22T12:16:56.418205Z","shell.execute_reply":"2023-10-22T12:16:56.417112Z","shell.execute_reply.started":"2023-10-22T12:16:56.411785Z"},"id":"IgmNKCypbm_q","outputId":"d949e8ef-a5b5-4692-9afd-f98273136d1c","trusted":true},"outputs":[],"source":["# Validation dataset\n","validation_dataset = SiameseDataset(validation_paired_images, validation_labels, transforms=transform_list)\n","validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n","\n","# Testing dataset\n","test_dataset = SiameseDataset(test_paired_images, test_labels, transforms=transform_list)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","#Custom dataset\n","custom_dataset = SiameseDataset(custom_paired_images, custom_labels, transforms=transform_list)\n","custom_loader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=False)"]},{"cell_type":"markdown","metadata":{"id":"JTs8wvIvVo-B"},"source":["Testing validation and test accuracies for model with ADAM optimizer and Cosine Annealing Learning rate scheduler"]},{"cell_type":"code","execution_count":46,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-10-22T12:17:23.238251Z","iopub.status.busy":"2023-10-22T12:17:23.237876Z","iopub.status.idle":"2023-10-22T12:23:38.686212Z","shell.execute_reply":"2023-10-22T12:23:38.685235Z","shell.execute_reply.started":"2023-10-22T12:17:23.238220Z"},"id":"7U5qH9CWTIXb","outputId":"adc2c91a-581a-4e2a-8441-584c2321358b","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Validation Accuracy: 0.4691\n","Test Accuracy: 0.3282\n"]}],"source":["def evaluate(model, data_loader):\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for data in data_loader:\n","            input1, input2, label = data\n","            input1, input2, label = input1.to(device), input2.to(device), label.to(device)\n","            output1, output2 = model(input1, input2)\n","            distances = F.pairwise_distance(output1, output2)\n","#             print(distances)\n","            predictions = distances < margin\n","            correct += (predictions == label.byte()).sum().item()\n","            total += label.size(0)\n","    accuracy = correct / total\n","    return accuracy\n","\n","# Evaluate on validation and test datasets\n","validation_accuracy = evaluate(model, validation_loader)\n","test_accuracy = evaluate(model, test_loader)\n","# custom_accuracy = evaluate(model, custom_loader)\n","\n","print(f\"Validation Accuracy: {validation_accuracy:.4f}\")\n","print(f\"Test Accuracy: {test_accuracy:.4f}\")\n","# print(f\"Test Accuracy: {custom_accuracy:.4f}\")\n"]},{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2023-10-22T12:26:22.363131Z","iopub.status.busy":"2023-10-22T12:26:22.362766Z","iopub.status.idle":"2023-10-22T12:26:22.367494Z","shell.execute_reply":"2023-10-22T12:26:22.366539Z","shell.execute_reply.started":"2023-10-22T12:26:22.363104Z"},"trusted":true},"outputs":[],"source":["# custom_accuracy = evaluate(model, custom_loader)\n","# print(f\"Test Accuracy: {custom_accuracy:.4f}\")"]},{"cell_type":"markdown","metadata":{"id":"C5EXROe3HPVy"},"source":["Train the model using ADAM optimizer and Reduce LR on plateau scheduler"]},{"cell_type":"code","execution_count":50,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"execution":{"iopub.execute_input":"2023-10-22T12:32:12.820290Z","iopub.status.busy":"2023-10-22T12:32:12.819380Z","iopub.status.idle":"2023-10-22T13:20:05.825924Z","shell.execute_reply":"2023-10-22T13:20:05.824820Z","shell.execute_reply.started":"2023-10-22T12:32:12.820258Z"},"id":"PAuYXnmvSJZf","outputId":"286d7ca4-3081-4046-fef0-9c9d7721f002","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [1/1] Batch [10/5524] Loss: 0.7982\n","Epoch [1/1] Batch [20/5524] Loss: 0.7945\n","Epoch [1/1] Batch [30/5524] Loss: 0.6223\n","Epoch [1/1] Batch [40/5524] Loss: 0.6278\n","Epoch [1/1] Batch [50/5524] Loss: 0.7970\n","Epoch [1/1] Batch [60/5524] Loss: 0.7927\n","Epoch [1/1] Batch [70/5524] Loss: 0.4607\n","Epoch [1/1] Batch [80/5524] Loss: 1.1060\n","Epoch [1/1] Batch [90/5524] Loss: 0.4566\n","Epoch [1/1] Batch [100/5524] Loss: 0.7916\n","Epoch [1/1] Batch [110/5524] Loss: 1.1237\n","Epoch [1/1] Batch [120/5524] Loss: 0.7831\n","Epoch [1/1] Batch [130/5524] Loss: 0.4623\n","Epoch [1/1] Batch [140/5524] Loss: 0.9549\n","Epoch [1/1] Batch [150/5524] Loss: 1.6140\n","Epoch [1/1] Batch [160/5524] Loss: 0.7895\n","Epoch [1/1] Batch [170/5524] Loss: 0.2957\n","Epoch [1/1] Batch [180/5524] Loss: 0.4666\n","Epoch [1/1] Batch [190/5524] Loss: 0.3024\n","Epoch [1/1] Batch [200/5524] Loss: 0.6287\n","Epoch [1/1] Batch [210/5524] Loss: 0.7945\n","Epoch [1/1] Batch [220/5524] Loss: 0.2873\n","Epoch [1/1] Batch [230/5524] Loss: 0.6260\n","Epoch [1/1] Batch [240/5524] Loss: 1.2997\n","Epoch [1/1] Batch [250/5524] Loss: 0.4643\n","Epoch [1/1] Batch [260/5524] Loss: 0.3232\n","Epoch [1/1] Batch [270/5524] Loss: 0.1379\n","Epoch [1/1] Batch [280/5524] Loss: 0.9594\n","Epoch [1/1] Batch [290/5524] Loss: 0.7912\n","Epoch [1/1] Batch [300/5524] Loss: 0.4612\n","Epoch [1/1] Batch [310/5524] Loss: 0.6283\n","Epoch [1/1] Batch [320/5524] Loss: 0.6211\n","Epoch [1/1] Batch [330/5524] Loss: 0.9708\n","Epoch [1/1] Batch [340/5524] Loss: 0.9592\n","Epoch [1/1] Batch [350/5524] Loss: 0.6380\n","Epoch [1/1] Batch [360/5524] Loss: 0.3237\n","Epoch [1/1] Batch [370/5524] Loss: 0.9656\n","Epoch [1/1] Batch [380/5524] Loss: 0.1420\n","Epoch [1/1] Batch [390/5524] Loss: 0.6284\n","Epoch [1/1] Batch [400/5524] Loss: 0.1490\n","Epoch [1/1] Batch [410/5524] Loss: 0.9688\n","Epoch [1/1] Batch [420/5524] Loss: 0.9525\n","Epoch [1/1] Batch [430/5524] Loss: 0.4620\n","Epoch [1/1] Batch [440/5524] Loss: 0.4655\n","Epoch [1/1] Batch [450/5524] Loss: 1.1275\n","Epoch [1/1] Batch [460/5524] Loss: 0.6246\n","Epoch [1/1] Batch [470/5524] Loss: 0.4552\n","Epoch [1/1] Batch [480/5524] Loss: 0.4714\n","Epoch [1/1] Batch [490/5524] Loss: 0.9576\n","Epoch [1/1] Batch [500/5524] Loss: 0.7967\n","Epoch [1/1] Batch [510/5524] Loss: 0.9373\n","Epoch [1/1] Batch [520/5524] Loss: 1.1306\n","Epoch [1/1] Batch [530/5524] Loss: 0.9578\n","Epoch [1/1] Batch [540/5524] Loss: 0.4635\n","Epoch [1/1] Batch [550/5524] Loss: 0.9638\n","Epoch [1/1] Batch [560/5524] Loss: 0.7959\n","Epoch [1/1] Batch [570/5524] Loss: 0.7956\n","Epoch [1/1] Batch [580/5524] Loss: 0.4760\n","Epoch [1/1] Batch [590/5524] Loss: 0.1259\n","Epoch [1/1] Batch [600/5524] Loss: 0.9643\n","Epoch [1/1] Batch [610/5524] Loss: 0.2891\n","Epoch [1/1] Batch [620/5524] Loss: 0.3031\n","Epoch [1/1] Batch [630/5524] Loss: 0.6290\n","Epoch [1/1] Batch [640/5524] Loss: 0.6294\n","Epoch [1/1] Batch [650/5524] Loss: 0.2952\n","Epoch [1/1] Batch [660/5524] Loss: 0.4688\n","Epoch [1/1] Batch [670/5524] Loss: 0.9462\n","Epoch [1/1] Batch [680/5524] Loss: 0.3113\n","Epoch [1/1] Batch [690/5524] Loss: 0.7913\n","Epoch [1/1] Batch [700/5524] Loss: 0.9601\n","Epoch [1/1] Batch [710/5524] Loss: 0.4646\n","Epoch [1/1] Batch [720/5524] Loss: 0.9580\n","Epoch [1/1] Batch [730/5524] Loss: 0.9537\n","Epoch [1/1] Batch [740/5524] Loss: 0.7942\n","Epoch [1/1] Batch [750/5524] Loss: 0.4678\n","Epoch [1/1] Batch [760/5524] Loss: 0.6317\n","Epoch [1/1] Batch [770/5524] Loss: 0.9520\n","Epoch [1/1] Batch [780/5524] Loss: 0.4673\n","Epoch [1/1] Batch [790/5524] Loss: 0.4708\n","Epoch [1/1] Batch [800/5524] Loss: 0.9622\n","Epoch [1/1] Batch [810/5524] Loss: 0.9514\n","Epoch [1/1] Batch [820/5524] Loss: 0.6391\n","Epoch [1/1] Batch [830/5524] Loss: 0.7889\n","Epoch [1/1] Batch [840/5524] Loss: 0.2847\n","Epoch [1/1] Batch [850/5524] Loss: 1.1124\n","Epoch [1/1] Batch [860/5524] Loss: 0.2948\n","Epoch [1/1] Batch [870/5524] Loss: 0.9601\n","Epoch [1/1] Batch [880/5524] Loss: 0.6316\n","Epoch [1/1] Batch [890/5524] Loss: 1.2878\n","Epoch [1/1] Batch [900/5524] Loss: 0.4533\n","Epoch [1/1] Batch [910/5524] Loss: 0.9719\n","Epoch [1/1] Batch [920/5524] Loss: 0.6251\n","Epoch [1/1] Batch [930/5524] Loss: 0.7891\n","Epoch [1/1] Batch [940/5524] Loss: 1.1071\n","Epoch [1/1] Batch [950/5524] Loss: 0.4659\n","Epoch [1/1] Batch [960/5524] Loss: 0.6443\n","Epoch [1/1] Batch [970/5524] Loss: 0.2948\n","Epoch [1/1] Batch [980/5524] Loss: 0.6281\n","Epoch [1/1] Batch [990/5524] Loss: 0.6204\n","Epoch [1/1] Batch [1000/5524] Loss: 0.8015\n","Epoch [1/1] Batch [1010/5524] Loss: 0.1258\n","Epoch [1/1] Batch [1020/5524] Loss: 0.7802\n","Epoch [1/1] Batch [1030/5524] Loss: 0.9626\n","Epoch [1/1] Batch [1040/5524] Loss: 1.4414\n","Epoch [1/1] Batch [1050/5524] Loss: 0.2931\n","Epoch [1/1] Batch [1060/5524] Loss: 0.9627\n","Epoch [1/1] Batch [1070/5524] Loss: 1.4146\n","Epoch [1/1] Batch [1080/5524] Loss: 0.4724\n","Epoch [1/1] Batch [1090/5524] Loss: 1.3088\n","Epoch [1/1] Batch [1100/5524] Loss: 0.6326\n","Epoch [1/1] Batch [1110/5524] Loss: 0.6287\n","Epoch [1/1] Batch [1120/5524] Loss: 0.4651\n","Epoch [1/1] Batch [1130/5524] Loss: 0.4582\n","Epoch [1/1] Batch [1140/5524] Loss: 0.6216\n","Epoch [1/1] Batch [1150/5524] Loss: 0.2932\n","Epoch [1/1] Batch [1160/5524] Loss: 1.2873\n","Epoch [1/1] Batch [1170/5524] Loss: 0.2857\n","Epoch [1/1] Batch [1180/5524] Loss: 0.2926\n","Epoch [1/1] Batch [1190/5524] Loss: 0.6340\n","Epoch [1/1] Batch [1200/5524] Loss: 0.9669\n","Epoch [1/1] Batch [1210/5524] Loss: 1.2970\n","Epoch [1/1] Batch [1220/5524] Loss: 0.8038\n","Epoch [1/1] Batch [1230/5524] Loss: 0.9549\n","Epoch [1/1] Batch [1240/5524] Loss: 0.3046\n","Epoch [1/1] Batch [1250/5524] Loss: 0.4600\n","Epoch [1/1] Batch [1260/5524] Loss: 0.3104\n","Epoch [1/1] Batch [1270/5524] Loss: 1.2940\n","Epoch [1/1] Batch [1280/5524] Loss: 0.8020\n","Epoch [1/1] Batch [1290/5524] Loss: 0.2969\n","Epoch [1/1] Batch [1300/5524] Loss: 0.9644\n","Epoch [1/1] Batch [1310/5524] Loss: 0.6334\n","Epoch [1/1] Batch [1320/5524] Loss: 0.7879\n","Epoch [1/1] Batch [1330/5524] Loss: 0.2994\n","Epoch [1/1] Batch [1340/5524] Loss: 0.6269\n","Epoch [1/1] Batch [1350/5524] Loss: 1.1346\n","Epoch [1/1] Batch [1360/5524] Loss: 1.1104\n","Epoch [1/1] Batch [1370/5524] Loss: 0.6249\n","Epoch [1/1] Batch [1380/5524] Loss: 0.6320\n","Epoch [1/1] Batch [1390/5524] Loss: 0.6346\n","Epoch [1/1] Batch [1400/5524] Loss: 0.6303\n","Epoch [1/1] Batch [1410/5524] Loss: 0.6397\n","Epoch [1/1] Batch [1420/5524] Loss: 0.2993\n","Epoch [1/1] Batch [1430/5524] Loss: 0.6264\n","Epoch [1/1] Batch [1440/5524] Loss: 0.7906\n","Epoch [1/1] Batch [1450/5524] Loss: 0.6228\n","Epoch [1/1] Batch [1460/5524] Loss: 0.9535\n","Epoch [1/1] Batch [1470/5524] Loss: 0.9605\n","Epoch [1/1] Batch [1480/5524] Loss: 0.7934\n","Epoch [1/1] Batch [1490/5524] Loss: 0.7924\n","Epoch [1/1] Batch [1500/5524] Loss: 1.1085\n","Epoch [1/1] Batch [1510/5524] Loss: 0.3069\n","Epoch [1/1] Batch [1520/5524] Loss: 0.7955\n","Epoch [1/1] Batch [1530/5524] Loss: 1.0992\n","Epoch [1/1] Batch [1540/5524] Loss: 0.6261\n","Epoch [1/1] Batch [1550/5524] Loss: 0.7923\n","Epoch [1/1] Batch [1560/5524] Loss: 0.4565\n","Epoch [1/1] Batch [1570/5524] Loss: 0.9654\n","Epoch [1/1] Batch [1580/5524] Loss: 0.7998\n","Epoch [1/1] Batch [1590/5524] Loss: 0.9598\n","Epoch [1/1] Batch [1600/5524] Loss: 1.1196\n","Epoch [1/1] Batch [1610/5524] Loss: 0.4747\n","Epoch [1/1] Batch [1620/5524] Loss: 0.9423\n","Epoch [1/1] Batch [1630/5524] Loss: 0.1337\n","Epoch [1/1] Batch [1640/5524] Loss: 0.6288\n","Epoch [1/1] Batch [1650/5524] Loss: 1.2725\n","Epoch [1/1] Batch [1660/5524] Loss: 0.6248\n","Epoch [1/1] Batch [1670/5524] Loss: 0.4558\n","Epoch [1/1] Batch [1680/5524] Loss: 0.7913\n","Epoch [1/1] Batch [1690/5524] Loss: 0.9655\n","Epoch [1/1] Batch [1700/5524] Loss: 0.6317\n","Epoch [1/1] Batch [1710/5524] Loss: 0.7906\n","Epoch [1/1] Batch [1720/5524] Loss: 0.6261\n","Epoch [1/1] Batch [1730/5524] Loss: 0.9536\n","Epoch [1/1] Batch [1740/5524] Loss: 1.2848\n","Epoch [1/1] Batch [1750/5524] Loss: 0.2888\n","Epoch [1/1] Batch [1760/5524] Loss: 0.7956\n","Epoch [1/1] Batch [1770/5524] Loss: 0.8010\n","Epoch [1/1] Batch [1780/5524] Loss: 0.9646\n","Epoch [1/1] Batch [1790/5524] Loss: 0.7944\n","Epoch [1/1] Batch [1800/5524] Loss: 0.4636\n","Epoch [1/1] Batch [1810/5524] Loss: 0.6264\n","Epoch [1/1] Batch [1820/5524] Loss: 0.7917\n","Epoch [1/1] Batch [1830/5524] Loss: 0.9678\n","Epoch [1/1] Batch [1840/5524] Loss: 0.4667\n","Epoch [1/1] Batch [1850/5524] Loss: 0.8000\n","Epoch [1/1] Batch [1860/5524] Loss: 0.9591\n","Epoch [1/1] Batch [1870/5524] Loss: 0.6320\n","Epoch [1/1] Batch [1880/5524] Loss: 0.4799\n","Epoch [1/1] Batch [1890/5524] Loss: 0.7911\n","Epoch [1/1] Batch [1900/5524] Loss: 1.1126\n","Epoch [1/1] Batch [1910/5524] Loss: 0.7907\n","Epoch [1/1] Batch [1920/5524] Loss: 0.9664\n","Epoch [1/1] Batch [1930/5524] Loss: 1.4344\n","Epoch [1/1] Batch [1940/5524] Loss: 0.2927\n","Epoch [1/1] Batch [1950/5524] Loss: 0.2936\n","Epoch [1/1] Batch [1960/5524] Loss: 0.7988\n","Epoch [1/1] Batch [1970/5524] Loss: 0.7996\n","Epoch [1/1] Batch [1980/5524] Loss: 0.6301\n","Epoch [1/1] Batch [1990/5524] Loss: 0.6264\n","Epoch [1/1] Batch [2000/5524] Loss: 0.6371\n","Epoch [1/1] Batch [2010/5524] Loss: 0.4681\n","Epoch [1/1] Batch [2020/5524] Loss: 0.9540\n","Epoch [1/1] Batch [2030/5524] Loss: 1.3067\n","Epoch [1/1] Batch [2040/5524] Loss: 0.6352\n","Epoch [1/1] Batch [2050/5524] Loss: 0.7908\n","Epoch [1/1] Batch [2060/5524] Loss: 0.7837\n","Epoch [1/1] Batch [2070/5524] Loss: 0.6353\n","Epoch [1/1] Batch [2080/5524] Loss: 0.3196\n","Epoch [1/1] Batch [2090/5524] Loss: 0.9628\n","Epoch [1/1] Batch [2100/5524] Loss: 0.9669\n","Epoch [1/1] Batch [2110/5524] Loss: 0.7910\n","Epoch [1/1] Batch [2120/5524] Loss: 0.7825\n","Epoch [1/1] Batch [2130/5524] Loss: 0.9589\n","Epoch [1/1] Batch [2140/5524] Loss: 0.7868\n","Epoch [1/1] Batch [2150/5524] Loss: 0.6329\n","Epoch [1/1] Batch [2160/5524] Loss: 0.6323\n","Epoch [1/1] Batch [2170/5524] Loss: 0.4592\n","Epoch [1/1] Batch [2180/5524] Loss: 0.7931\n","Epoch [1/1] Batch [2190/5524] Loss: 0.9544\n","Epoch [1/1] Batch [2200/5524] Loss: 0.6331\n","Epoch [1/1] Batch [2210/5524] Loss: 0.6299\n","Epoch [1/1] Batch [2220/5524] Loss: 0.2973\n","Epoch [1/1] Batch [2230/5524] Loss: 0.4643\n","Epoch [1/1] Batch [2240/5524] Loss: 0.4703\n","Epoch [1/1] Batch [2250/5524] Loss: 0.7971\n","Epoch [1/1] Batch [2260/5524] Loss: 0.4778\n","Epoch [1/1] Batch [2270/5524] Loss: 0.4673\n","Epoch [1/1] Batch [2280/5524] Loss: 0.4716\n","Epoch [1/1] Batch [2290/5524] Loss: 0.9523\n","Epoch [1/1] Batch [2300/5524] Loss: 0.9513\n","Epoch [1/1] Batch [2310/5524] Loss: 0.7972\n","Epoch [1/1] Batch [2320/5524] Loss: 0.6309\n","Epoch [1/1] Batch [2330/5524] Loss: 1.6149\n","Epoch [1/1] Batch [2340/5524] Loss: 0.7973\n","Epoch [1/1] Batch [2350/5524] Loss: 0.4660\n","Epoch [1/1] Batch [2360/5524] Loss: 0.6281\n","Epoch [1/1] Batch [2370/5524] Loss: 0.9628\n","Epoch [1/1] Batch [2380/5524] Loss: 0.6475\n","Epoch [1/1] Batch [2390/5524] Loss: 0.7985\n","Epoch [1/1] Batch [2400/5524] Loss: 0.3033\n","Epoch [1/1] Batch [2410/5524] Loss: 0.9617\n","Epoch [1/1] Batch [2420/5524] Loss: 1.2890\n","Epoch [1/1] Batch [2430/5524] Loss: 0.9653\n","Epoch [1/1] Batch [2440/5524] Loss: 0.4608\n","Epoch [1/1] Batch [2450/5524] Loss: 0.6248\n","Epoch [1/1] Batch [2460/5524] Loss: 0.6263\n","Epoch [1/1] Batch [2470/5524] Loss: 1.4480\n","Epoch [1/1] Batch [2480/5524] Loss: 0.2995\n","Epoch [1/1] Batch [2490/5524] Loss: 0.7859\n","Epoch [1/1] Batch [2500/5524] Loss: 0.6370\n","Epoch [1/1] Batch [2510/5524] Loss: 0.9523\n","Epoch [1/1] Batch [2520/5524] Loss: 0.6290\n","Epoch [1/1] Batch [2530/5524] Loss: 0.6232\n","Epoch [1/1] Batch [2540/5524] Loss: 0.4688\n","Epoch [1/1] Batch [2550/5524] Loss: 0.6283\n","Epoch [1/1] Batch [2560/5524] Loss: 0.4543\n","Epoch [1/1] Batch [2570/5524] Loss: 1.1186\n","Epoch [1/1] Batch [2580/5524] Loss: 1.1334\n","Epoch [1/1] Batch [2590/5524] Loss: 0.6349\n","Epoch [1/1] Batch [2600/5524] Loss: 0.6317\n","Epoch [1/1] Batch [2610/5524] Loss: 0.7878\n","Epoch [1/1] Batch [2620/5524] Loss: 1.6044\n","Epoch [1/1] Batch [2630/5524] Loss: 0.9448\n","Epoch [1/1] Batch [2640/5524] Loss: 0.6255\n","Epoch [1/1] Batch [2650/5524] Loss: 0.4646\n","Epoch [1/1] Batch [2660/5524] Loss: 0.4623\n","Epoch [1/1] Batch [2670/5524] Loss: 0.6259\n","Epoch [1/1] Batch [2680/5524] Loss: 0.7790\n","Epoch [1/1] Batch [2690/5524] Loss: 0.2989\n","Epoch [1/1] Batch [2700/5524] Loss: 1.4309\n","Epoch [1/1] Batch [2710/5524] Loss: 0.6287\n","Epoch [1/1] Batch [2720/5524] Loss: 0.1137\n","Epoch [1/1] Batch [2730/5524] Loss: 0.9598\n","Epoch [1/1] Batch [2740/5524] Loss: 0.4599\n","Epoch [1/1] Batch [2750/5524] Loss: 0.7958\n","Epoch [1/1] Batch [2760/5524] Loss: 0.6216\n","Epoch [1/1] Batch [2770/5524] Loss: 0.9442\n","Epoch [1/1] Batch [2780/5524] Loss: 1.4435\n","Epoch [1/1] Batch [2790/5524] Loss: 0.9586\n","Epoch [1/1] Batch [2800/5524] Loss: 0.7841\n","Epoch [1/1] Batch [2810/5524] Loss: 0.6365\n","Epoch [1/1] Batch [2820/5524] Loss: 0.4613\n","Epoch [1/1] Batch [2830/5524] Loss: 0.9607\n","Epoch [1/1] Batch [2840/5524] Loss: 0.6321\n","Epoch [1/1] Batch [2850/5524] Loss: 0.9729\n","Epoch [1/1] Batch [2860/5524] Loss: 0.6355\n","Epoch [1/1] Batch [2870/5524] Loss: 0.6244\n","Epoch [1/1] Batch [2880/5524] Loss: 0.6240\n","Epoch [1/1] Batch [2890/5524] Loss: 0.7866\n","Epoch [1/1] Batch [2900/5524] Loss: 0.6264\n","Epoch [1/1] Batch [2910/5524] Loss: 0.6288\n","Epoch [1/1] Batch [2920/5524] Loss: 0.6314\n","Epoch [1/1] Batch [2930/5524] Loss: 0.4633\n","Epoch [1/1] Batch [2940/5524] Loss: 0.6314\n","Epoch [1/1] Batch [2950/5524] Loss: 0.7951\n","Epoch [1/1] Batch [2960/5524] Loss: 0.9516\n","Epoch [1/1] Batch [2970/5524] Loss: 0.1430\n","Epoch [1/1] Batch [2980/5524] Loss: 1.1186\n","Epoch [1/1] Batch [2990/5524] Loss: 1.1251\n","Epoch [1/1] Batch [3000/5524] Loss: 1.1196\n","Epoch [1/1] Batch [3010/5524] Loss: 0.6247\n","Epoch [1/1] Batch [3020/5524] Loss: 1.3002\n","Epoch [1/1] Batch [3030/5524] Loss: 0.7979\n","Epoch [1/1] Batch [3040/5524] Loss: 1.4622\n","Epoch [1/1] Batch [3050/5524] Loss: 0.4708\n","Epoch [1/1] Batch [3060/5524] Loss: 0.9599\n","Epoch [1/1] Batch [3070/5524] Loss: 0.6376\n","Epoch [1/1] Batch [3080/5524] Loss: 0.6273\n","Epoch [1/1] Batch [3090/5524] Loss: 0.4793\n","Epoch [1/1] Batch [3100/5524] Loss: 0.9473\n","Epoch [1/1] Batch [3110/5524] Loss: 0.4655\n","Epoch [1/1] Batch [3120/5524] Loss: 0.8047\n","Epoch [1/1] Batch [3130/5524] Loss: 0.6275\n","Epoch [1/1] Batch [3140/5524] Loss: 0.6272\n","Epoch [1/1] Batch [3150/5524] Loss: 0.3097\n","Epoch [1/1] Batch [3160/5524] Loss: 0.2926\n","Epoch [1/1] Batch [3170/5524] Loss: 0.4560\n","Epoch [1/1] Batch [3180/5524] Loss: 0.7957\n","Epoch [1/1] Batch [3190/5524] Loss: 0.9535\n","Epoch [1/1] Batch [3200/5524] Loss: 0.1254\n","Epoch [1/1] Batch [3210/5524] Loss: 0.6330\n","Epoch [1/1] Batch [3220/5524] Loss: 0.6334\n","Epoch [1/1] Batch [3230/5524] Loss: 0.4544\n","Epoch [1/1] Batch [3240/5524] Loss: 0.8042\n","Epoch [1/1] Batch [3250/5524] Loss: 0.8013\n","Epoch [1/1] Batch [3260/5524] Loss: 0.4588\n","Epoch [1/1] Batch [3270/5524] Loss: 0.9748\n","Epoch [1/1] Batch [3280/5524] Loss: 1.4331\n","Epoch [1/1] Batch [3290/5524] Loss: 0.9558\n","Epoch [1/1] Batch [3300/5524] Loss: 0.9567\n","Epoch [1/1] Batch [3310/5524] Loss: 0.6306\n","Epoch [1/1] Batch [3320/5524] Loss: 0.7980\n","Epoch [1/1] Batch [3330/5524] Loss: 0.4666\n","Epoch [1/1] Batch [3340/5524] Loss: 0.6242\n","Epoch [1/1] Batch [3350/5524] Loss: 0.7850\n","Epoch [1/1] Batch [3360/5524] Loss: 0.6279\n","Epoch [1/1] Batch [3370/5524] Loss: 0.9485\n","Epoch [1/1] Batch [3380/5524] Loss: 0.4628\n","Epoch [1/1] Batch [3390/5524] Loss: 0.4683\n","Epoch [1/1] Batch [3400/5524] Loss: 0.6320\n","Epoch [1/1] Batch [3410/5524] Loss: 0.6303\n","Epoch [1/1] Batch [3420/5524] Loss: 0.6255\n","Epoch [1/1] Batch [3430/5524] Loss: 0.7914\n","Epoch [1/1] Batch [3440/5524] Loss: 0.6228\n","Epoch [1/1] Batch [3450/5524] Loss: 0.1316\n","Epoch [1/1] Batch [3460/5524] Loss: 0.7879\n","Epoch [1/1] Batch [3470/5524] Loss: 0.6367\n","Epoch [1/1] Batch [3480/5524] Loss: 0.4624\n","Epoch [1/1] Batch [3490/5524] Loss: 0.4622\n","Epoch [1/1] Batch [3500/5524] Loss: 0.2953\n","Epoch [1/1] Batch [3510/5524] Loss: 0.4681\n","Epoch [1/1] Batch [3520/5524] Loss: 0.9824\n","Epoch [1/1] Batch [3530/5524] Loss: 0.9632\n","Epoch [1/1] Batch [3540/5524] Loss: 1.2779\n","Epoch [1/1] Batch [3550/5524] Loss: 0.7998\n","Epoch [1/1] Batch [3560/5524] Loss: 1.2594\n","Epoch [1/1] Batch [3570/5524] Loss: 0.7899\n","Epoch [1/1] Batch [3580/5524] Loss: 0.9526\n","Epoch [1/1] Batch [3590/5524] Loss: 1.4782\n","Epoch [1/1] Batch [3600/5524] Loss: 0.9503\n","Epoch [1/1] Batch [3610/5524] Loss: 0.1414\n","Epoch [1/1] Batch [3620/5524] Loss: 0.3057\n","Epoch [1/1] Batch [3630/5524] Loss: 0.7919\n","Epoch [1/1] Batch [3640/5524] Loss: 0.6293\n","Epoch [1/1] Batch [3650/5524] Loss: 1.1211\n","Epoch [1/1] Batch [3660/5524] Loss: 0.9441\n","Epoch [1/1] Batch [3670/5524] Loss: 0.9645\n","Epoch [1/1] Batch [3680/5524] Loss: 0.9474\n","Epoch [1/1] Batch [3690/5524] Loss: 0.9624\n","Epoch [1/1] Batch [3700/5524] Loss: 0.1188\n","Epoch [1/1] Batch [3710/5524] Loss: 0.3001\n","Epoch [1/1] Batch [3720/5524] Loss: 0.7810\n","Epoch [1/1] Batch [3730/5524] Loss: 1.2823\n","Epoch [1/1] Batch [3740/5524] Loss: 0.4568\n","Epoch [1/1] Batch [3750/5524] Loss: 0.9653\n","Epoch [1/1] Batch [3760/5524] Loss: 0.9687\n","Epoch [1/1] Batch [3770/5524] Loss: 0.3038\n","Epoch [1/1] Batch [3780/5524] Loss: 0.9547\n","Epoch [1/1] Batch [3790/5524] Loss: 0.6215\n","Epoch [1/1] Batch [3800/5524] Loss: 0.7931\n","Epoch [1/1] Batch [3810/5524] Loss: 0.9599\n","Epoch [1/1] Batch [3820/5524] Loss: 0.6298\n","Epoch [1/1] Batch [3830/5524] Loss: 0.7966\n","Epoch [1/1] Batch [3840/5524] Loss: 0.6267\n","Epoch [1/1] Batch [3850/5524] Loss: 0.9580\n","Epoch [1/1] Batch [3860/5524] Loss: 0.9601\n","Epoch [1/1] Batch [3870/5524] Loss: 0.7836\n","Epoch [1/1] Batch [3880/5524] Loss: 0.6238\n","Epoch [1/1] Batch [3890/5524] Loss: 1.1179\n","Epoch [1/1] Batch [3900/5524] Loss: 0.6261\n","Epoch [1/1] Batch [3910/5524] Loss: 0.9481\n","Epoch [1/1] Batch [3920/5524] Loss: 0.8000\n","Epoch [1/1] Batch [3930/5524] Loss: 0.7950\n","Epoch [1/1] Batch [3940/5524] Loss: 0.4647\n","Epoch [1/1] Batch [3950/5524] Loss: 1.2958\n","Epoch [1/1] Batch [3960/5524] Loss: 1.4359\n","Epoch [1/1] Batch [3970/5524] Loss: 0.6344\n","Epoch [1/1] Batch [3980/5524] Loss: 1.1225\n","Epoch [1/1] Batch [3990/5524] Loss: 0.6305\n","Epoch [1/1] Batch [4000/5524] Loss: 0.4624\n","Epoch [1/1] Batch [4010/5524] Loss: 0.6242\n","Epoch [1/1] Batch [4020/5524] Loss: 0.4558\n","Epoch [1/1] Batch [4030/5524] Loss: 0.6279\n","Epoch [1/1] Batch [4040/5524] Loss: 1.1293\n","Epoch [1/1] Batch [4050/5524] Loss: 0.7925\n","Epoch [1/1] Batch [4060/5524] Loss: 1.7949\n","Epoch [1/1] Batch [4070/5524] Loss: 0.9496\n","Epoch [1/1] Batch [4080/5524] Loss: 0.9570\n","Epoch [1/1] Batch [4090/5524] Loss: 0.4574\n","Epoch [1/1] Batch [4100/5524] Loss: 0.6361\n","Epoch [1/1] Batch [4110/5524] Loss: 0.6346\n","Epoch [1/1] Batch [4120/5524] Loss: 0.1303\n","Epoch [1/1] Batch [4130/5524] Loss: 1.1367\n","Epoch [1/1] Batch [4140/5524] Loss: 0.4567\n","Epoch [1/1] Batch [4150/5524] Loss: 0.9583\n","Epoch [1/1] Batch [4160/5524] Loss: 0.7899\n","Epoch [1/1] Batch [4170/5524] Loss: 0.7974\n","Epoch [1/1] Batch [4180/5524] Loss: 0.4856\n","Epoch [1/1] Batch [4190/5524] Loss: 1.1369\n","Epoch [1/1] Batch [4200/5524] Loss: 1.1126\n","Epoch [1/1] Batch [4210/5524] Loss: 0.6299\n","Epoch [1/1] Batch [4220/5524] Loss: 0.1338\n","Epoch [1/1] Batch [4230/5524] Loss: 0.4649\n","Epoch [1/1] Batch [4240/5524] Loss: 0.9542\n","Epoch [1/1] Batch [4250/5524] Loss: 0.1152\n","Epoch [1/1] Batch [4260/5524] Loss: 0.6256\n","Epoch [1/1] Batch [4270/5524] Loss: 0.7919\n","Epoch [1/1] Batch [4280/5524] Loss: 0.9430\n","Epoch [1/1] Batch [4290/5524] Loss: 0.4731\n","Epoch [1/1] Batch [4300/5524] Loss: 0.6247\n","Epoch [1/1] Batch [4310/5524] Loss: 0.9654\n","Epoch [1/1] Batch [4320/5524] Loss: 1.4521\n","Epoch [1/1] Batch [4330/5524] Loss: 0.3090\n","Epoch [1/1] Batch [4340/5524] Loss: 0.1249\n","Epoch [1/1] Batch [4350/5524] Loss: 0.9608\n","Epoch [1/1] Batch [4360/5524] Loss: 0.7947\n","Epoch [1/1] Batch [4370/5524] Loss: 0.7905\n","Epoch [1/1] Batch [4380/5524] Loss: 0.6250\n","Epoch [1/1] Batch [4390/5524] Loss: 0.4653\n","Epoch [1/1] Batch [4400/5524] Loss: 0.9430\n","Epoch [1/1] Batch [4410/5524] Loss: 0.4625\n","Epoch [1/1] Batch [4420/5524] Loss: 0.4729\n","Epoch [1/1] Batch [4430/5524] Loss: 0.4550\n","Epoch [1/1] Batch [4440/5524] Loss: 0.9602\n","Epoch [1/1] Batch [4450/5524] Loss: 0.7844\n","Epoch [1/1] Batch [4460/5524] Loss: 0.7858\n","Epoch [1/1] Batch [4470/5524] Loss: 0.9621\n","Epoch [1/1] Batch [4480/5524] Loss: 1.1423\n","Epoch [1/1] Batch [4490/5524] Loss: 1.1257\n","Epoch [1/1] Batch [4500/5524] Loss: 0.4756\n","Epoch [1/1] Batch [4510/5524] Loss: 0.4664\n","Epoch [1/1] Batch [4520/5524] Loss: 0.4702\n","Epoch [1/1] Batch [4530/5524] Loss: 0.2996\n","Epoch [1/1] Batch [4540/5524] Loss: 0.7977\n","Epoch [1/1] Batch [4550/5524] Loss: 0.6309\n","Epoch [1/1] Batch [4560/5524] Loss: 0.7946\n","Epoch [1/1] Batch [4570/5524] Loss: 1.2767\n","Epoch [1/1] Batch [4580/5524] Loss: 0.6298\n","Epoch [1/1] Batch [4590/5524] Loss: 0.7880\n","Epoch [1/1] Batch [4600/5524] Loss: 1.2725\n","Epoch [1/1] Batch [4610/5524] Loss: 0.4706\n","Epoch [1/1] Batch [4620/5524] Loss: 0.3073\n","Epoch [1/1] Batch [4630/5524] Loss: 0.4516\n","Epoch [1/1] Batch [4640/5524] Loss: 0.7937\n","Epoch [1/1] Batch [4650/5524] Loss: 0.6254\n","Epoch [1/1] Batch [4660/5524] Loss: 0.7962\n","Epoch [1/1] Batch [4670/5524] Loss: 0.7889\n","Epoch [1/1] Batch [4680/5524] Loss: 0.9666\n","Epoch [1/1] Batch [4690/5524] Loss: 0.7860\n","Epoch [1/1] Batch [4700/5524] Loss: 1.1058\n","Epoch [1/1] Batch [4710/5524] Loss: 0.6315\n","Epoch [1/1] Batch [4720/5524] Loss: 0.4669\n","Epoch [1/1] Batch [4730/5524] Loss: 0.6200\n","Epoch [1/1] Batch [4740/5524] Loss: 0.6289\n","Epoch [1/1] Batch [4750/5524] Loss: 0.1549\n","Epoch [1/1] Batch [4760/5524] Loss: 0.9714\n","Epoch [1/1] Batch [4770/5524] Loss: 0.7872\n","Epoch [1/1] Batch [4780/5524] Loss: 0.6258\n","Epoch [1/1] Batch [4790/5524] Loss: 0.9631\n","Epoch [1/1] Batch [4800/5524] Loss: 0.4636\n","Epoch [1/1] Batch [4810/5524] Loss: 1.1115\n","Epoch [1/1] Batch [4820/5524] Loss: 0.6300\n","Epoch [1/1] Batch [4830/5524] Loss: 0.7907\n","Epoch [1/1] Batch [4840/5524] Loss: 1.4918\n","Epoch [1/1] Batch [4850/5524] Loss: 0.4620\n","Epoch [1/1] Batch [4860/5524] Loss: 1.4716\n","Epoch [1/1] Batch [4870/5524] Loss: 0.6315\n","Epoch [1/1] Batch [4880/5524] Loss: 0.3128\n","Epoch [1/1] Batch [4890/5524] Loss: 1.1063\n","Epoch [1/1] Batch [4900/5524] Loss: 0.7940\n","Epoch [1/1] Batch [4910/5524] Loss: 0.4631\n","Epoch [1/1] Batch [4920/5524] Loss: 0.6297\n","Epoch [1/1] Batch [4930/5524] Loss: 0.4633\n","Epoch [1/1] Batch [4940/5524] Loss: 1.4747\n","Epoch [1/1] Batch [4950/5524] Loss: 0.7941\n","Epoch [1/1] Batch [4960/5524] Loss: 0.9628\n","Epoch [1/1] Batch [4970/5524] Loss: 0.6315\n","Epoch [1/1] Batch [4980/5524] Loss: 0.4611\n","Epoch [1/1] Batch [4990/5524] Loss: 0.9575\n","Epoch [1/1] Batch [5000/5524] Loss: 0.6265\n","Epoch [1/1] Batch [5010/5524] Loss: 0.9409\n","Epoch [1/1] Batch [5020/5524] Loss: 1.1154\n","Epoch [1/1] Batch [5030/5524] Loss: 0.6318\n","Epoch [1/1] Batch [5040/5524] Loss: 0.7873\n","Epoch [1/1] Batch [5050/5524] Loss: 0.6308\n","Epoch [1/1] Batch [5060/5524] Loss: 0.1285\n","Epoch [1/1] Batch [5070/5524] Loss: 0.4757\n","Epoch [1/1] Batch [5080/5524] Loss: 0.7896\n","Epoch [1/1] Batch [5090/5524] Loss: 1.1337\n","Epoch [1/1] Batch [5100/5524] Loss: 0.6253\n","Epoch [1/1] Batch [5110/5524] Loss: 1.1248\n","Epoch [1/1] Batch [5120/5524] Loss: 0.6321\n","Epoch [1/1] Batch [5130/5524] Loss: 0.6266\n","Epoch [1/1] Batch [5140/5524] Loss: 0.7890\n","Epoch [1/1] Batch [5150/5524] Loss: 0.2948\n","Epoch [1/1] Batch [5160/5524] Loss: 0.7867\n","Epoch [1/1] Batch [5170/5524] Loss: 1.4794\n","Epoch [1/1] Batch [5180/5524] Loss: 0.4574\n","Epoch [1/1] Batch [5190/5524] Loss: 0.2854\n","Epoch [1/1] Batch [5200/5524] Loss: 0.7971\n","Epoch [1/1] Batch [5210/5524] Loss: 1.1340\n","Epoch [1/1] Batch [5220/5524] Loss: 1.1191\n","Epoch [1/1] Batch [5230/5524] Loss: 0.7855\n","Epoch [1/1] Batch [5240/5524] Loss: 0.7999\n","Epoch [1/1] Batch [5250/5524] Loss: 0.9495\n","Epoch [1/1] Batch [5260/5524] Loss: 0.6296\n","Epoch [1/1] Batch [5270/5524] Loss: 0.9543\n","Epoch [1/1] Batch [5280/5524] Loss: 0.9497\n","Epoch [1/1] Batch [5290/5524] Loss: 0.6247\n","Epoch [1/1] Batch [5300/5524] Loss: 0.8041\n","Epoch [1/1] Batch [5310/5524] Loss: 1.2942\n","Epoch [1/1] Batch [5320/5524] Loss: 0.3000\n","Epoch [1/1] Batch [5330/5524] Loss: 0.6312\n","Epoch [1/1] Batch [5340/5524] Loss: 0.4699\n","Epoch [1/1] Batch [5350/5524] Loss: 0.9683\n","Epoch [1/1] Batch [5360/5524] Loss: 0.9568\n","Epoch [1/1] Batch [5370/5524] Loss: 0.6262\n","Epoch [1/1] Batch [5380/5524] Loss: 0.7985\n","Epoch [1/1] Batch [5390/5524] Loss: 0.6258\n","Epoch [1/1] Batch [5400/5524] Loss: 0.7849\n","Epoch [1/1] Batch [5410/5524] Loss: 0.4716\n","Epoch [1/1] Batch [5420/5524] Loss: 0.6317\n","Epoch [1/1] Batch [5430/5524] Loss: 0.7934\n","Epoch [1/1] Batch [5440/5524] Loss: 0.7896\n","Epoch [1/1] Batch [5450/5524] Loss: 0.4605\n","Epoch [1/1] Batch [5460/5524] Loss: 0.4531\n","Epoch [1/1] Batch [5470/5524] Loss: 0.4569\n","Epoch [1/1] Batch [5480/5524] Loss: 0.7951\n","Epoch [1/1] Batch [5490/5524] Loss: 0.9591\n","Epoch [1/1] Batch [5500/5524] Loss: 0.7861\n","Epoch [1/1] Batch [5510/5524] Loss: 1.1368\n","Epoch [1/1] Batch [5520/5524] Loss: 0.9607\n"]}],"source":["from torch.optim.lr_scheduler import ExponentialLR\n","\n","# Create the ExponentialLR scheduler with the desired gamma (exponential decay factor)\n","scheduler = ExponentialLR(optimizer_adam, gamma=0.95)  # Adjust gamma as needed\n","\n","num_epochs = 1  # Adjust as needed\n","\n","for epoch in range(num_epochs):\n","    for i, data in enumerate(train_loader, 0):\n","        input1, input2, label = data\n","        input1, input2, label = input1.to(device), input2.to(device), label.to(device)\n","        optimizer_adam.zero_grad()\n","        output1, output2 = model(input1, input2)\n","        loss = criterion(output1, output2, label, model)\n","        loss.backward()\n","        optimizer_adam.step()\n","\n","        # Update the learning rate after each optimizer step\n","        scheduler.step()\n","\n","        if (i + 1) % 10 == 0:\n","            print(f\"Epoch [{epoch + 1}/{num_epochs}] Batch [{i + 1}/{len(train_loader)}] Loss: {loss.item():.4f}\")\n"]},{"cell_type":"code","execution_count":51,"metadata":{"execution":{"iopub.execute_input":"2023-10-22T13:20:05.828982Z","iopub.status.busy":"2023-10-22T13:20:05.828219Z","iopub.status.idle":"2023-10-22T13:26:19.164952Z","shell.execute_reply":"2023-10-22T13:26:19.163921Z","shell.execute_reply.started":"2023-10-22T13:20:05.828952Z"},"id":"Fv2mH6icTHPq","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Validation Accuracy: 0.4691\n","Test Accuracy: 0.3282\n"]}],"source":["validation_accuracy = evaluate(model, validation_loader)\n","test_accuracy = evaluate(model, test_loader)\n","# custom_accuracy = evaluate(model, custom_loader)\n","\n","print(f\"Validation Accuracy: {validation_accuracy:.4f}\")\n","print(f\"Test Accuracy: {test_accuracy:.4f}\")\n","# print(f\"Test Accuracy: {custom_accuracy:.4f}\")"]},{"cell_type":"code","execution_count":54,"metadata":{"execution":{"iopub.execute_input":"2023-10-22T13:28:50.811432Z","iopub.status.busy":"2023-10-22T13:28:50.811009Z","iopub.status.idle":"2023-10-22T15:04:12.669800Z","shell.execute_reply":"2023-10-22T15:04:12.668621Z","shell.execute_reply.started":"2023-10-22T13:28:50.811402Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [1/2] Batch [10/5524] Loss: 0.8633\n","Epoch [1/2] Batch [20/5524] Loss: 0.3112\n","Epoch [1/2] Batch [30/5524] Loss: 0.6310\n","Epoch [1/2] Batch [40/5524] Loss: 0.6143\n","Epoch [1/2] Batch [50/5524] Loss: 0.4683\n","Epoch [1/2] Batch [60/5524] Loss: 0.4638\n","Epoch [1/2] Batch [70/5524] Loss: 1.4421\n","Epoch [1/2] Batch [80/5524] Loss: 0.9417\n","Epoch [1/2] Batch [90/5524] Loss: 1.0885\n","Epoch [1/2] Batch [100/5524] Loss: 0.7725\n","Epoch [1/2] Batch [110/5524] Loss: 0.7632\n","Epoch [1/2] Batch [120/5524] Loss: 0.4355\n","Epoch [1/2] Batch [130/5524] Loss: 0.9319\n","Epoch [1/2] Batch [140/5524] Loss: 0.9490\n","Epoch [1/2] Batch [150/5524] Loss: 0.6099\n","Epoch [1/2] Batch [160/5524] Loss: 0.6050\n","Epoch [1/2] Batch [170/5524] Loss: 1.1089\n","Epoch [1/2] Batch [180/5524] Loss: 0.9294\n","Epoch [1/2] Batch [190/5524] Loss: 0.6046\n","Epoch [1/2] Batch [200/5524] Loss: 0.4485\n","Epoch [1/2] Batch [210/5524] Loss: 0.6124\n","Epoch [1/2] Batch [220/5524] Loss: 1.3999\n","Epoch [1/2] Batch [230/5524] Loss: 0.9567\n","Epoch [1/2] Batch [240/5524] Loss: 1.0984\n","Epoch [1/2] Batch [250/5524] Loss: 0.6127\n","Epoch [1/2] Batch [260/5524] Loss: 0.6123\n","Epoch [1/2] Batch [270/5524] Loss: 0.9431\n","Epoch [1/2] Batch [280/5524] Loss: 0.7692\n","Epoch [1/2] Batch [290/5524] Loss: 1.0677\n","Epoch [1/2] Batch [300/5524] Loss: 1.0888\n","Epoch [1/2] Batch [310/5524] Loss: 0.7798\n","Epoch [1/2] Batch [320/5524] Loss: 0.6087\n","Epoch [1/2] Batch [330/5524] Loss: 0.2763\n","Epoch [1/2] Batch [340/5524] Loss: 0.7653\n","Epoch [1/2] Batch [350/5524] Loss: 0.7800\n","Epoch [1/2] Batch [360/5524] Loss: 0.6055\n","Epoch [1/2] Batch [370/5524] Loss: 0.9376\n","Epoch [1/2] Batch [380/5524] Loss: 0.7706\n","Epoch [1/2] Batch [390/5524] Loss: 0.9410\n","Epoch [1/2] Batch [400/5524] Loss: 0.9439\n","Epoch [1/2] Batch [410/5524] Loss: 0.2962\n","Epoch [1/2] Batch [420/5524] Loss: 0.7695\n","Epoch [1/2] Batch [430/5524] Loss: 0.2797\n","Epoch [1/2] Batch [440/5524] Loss: 0.4374\n","Epoch [1/2] Batch [450/5524] Loss: 0.9511\n","Epoch [1/2] Batch [460/5524] Loss: 0.5948\n","Epoch [1/2] Batch [470/5524] Loss: 1.0929\n","Epoch [1/2] Batch [480/5524] Loss: 0.4365\n","Epoch [1/2] Batch [490/5524] Loss: 0.7751\n","Epoch [1/2] Batch [500/5524] Loss: 0.9297\n","Epoch [1/2] Batch [510/5524] Loss: 0.2985\n","Epoch [1/2] Batch [520/5524] Loss: 0.4403\n","Epoch [1/2] Batch [530/5524] Loss: 1.0844\n","Epoch [1/2] Batch [540/5524] Loss: 0.2804\n","Epoch [1/2] Batch [550/5524] Loss: 0.9520\n","Epoch [1/2] Batch [560/5524] Loss: 0.7705\n","Epoch [1/2] Batch [570/5524] Loss: 0.6041\n","Epoch [1/2] Batch [580/5524] Loss: 1.1110\n","Epoch [1/2] Batch [590/5524] Loss: 0.7683\n","Epoch [1/2] Batch [600/5524] Loss: 0.7729\n","Epoch [1/2] Batch [610/5524] Loss: 0.7734\n","Epoch [1/2] Batch [620/5524] Loss: 0.6125\n","Epoch [1/2] Batch [630/5524] Loss: 0.2700\n","Epoch [1/2] Batch [640/5524] Loss: 0.7740\n","Epoch [1/2] Batch [650/5524] Loss: 0.9359\n","Epoch [1/2] Batch [660/5524] Loss: 1.1129\n","Epoch [1/2] Batch [670/5524] Loss: 0.9341\n","Epoch [1/2] Batch [680/5524] Loss: 0.4351\n","Epoch [1/2] Batch [690/5524] Loss: 0.4483\n","Epoch [1/2] Batch [700/5524] Loss: 0.6161\n","Epoch [1/2] Batch [710/5524] Loss: 0.4334\n","Epoch [1/2] Batch [720/5524] Loss: 0.9346\n","Epoch [1/2] Batch [730/5524] Loss: 1.2987\n","Epoch [1/2] Batch [740/5524] Loss: 0.7709\n","Epoch [1/2] Batch [750/5524] Loss: 0.7663\n","Epoch [1/2] Batch [760/5524] Loss: 0.1235\n","Epoch [1/2] Batch [770/5524] Loss: 0.6055\n","Epoch [1/2] Batch [780/5524] Loss: 0.7711\n","Epoch [1/2] Batch [790/5524] Loss: 0.6043\n","Epoch [1/2] Batch [800/5524] Loss: 0.7711\n","Epoch [1/2] Batch [810/5524] Loss: 0.4332\n","Epoch [1/2] Batch [820/5524] Loss: 0.6028\n","Epoch [1/2] Batch [830/5524] Loss: 0.1242\n","Epoch [1/2] Batch [840/5524] Loss: 0.2802\n","Epoch [1/2] Batch [850/5524] Loss: 0.9415\n","Epoch [1/2] Batch [860/5524] Loss: 0.4444\n","Epoch [1/2] Batch [870/5524] Loss: 1.0963\n","Epoch [1/2] Batch [880/5524] Loss: 0.5991\n","Epoch [1/2] Batch [890/5524] Loss: 0.4316\n","Epoch [1/2] Batch [900/5524] Loss: 0.4362\n","Epoch [1/2] Batch [910/5524] Loss: 0.6040\n","Epoch [1/2] Batch [920/5524] Loss: 0.4413\n","Epoch [1/2] Batch [930/5524] Loss: 0.7668\n","Epoch [1/2] Batch [940/5524] Loss: 0.6070\n","Epoch [1/2] Batch [950/5524] Loss: 0.6146\n","Epoch [1/2] Batch [960/5524] Loss: 0.1083\n","Epoch [1/2] Batch [970/5524] Loss: 0.2722\n","Epoch [1/2] Batch [980/5524] Loss: 0.7794\n","Epoch [1/2] Batch [990/5524] Loss: 0.4359\n","Epoch [1/2] Batch [1000/5524] Loss: 0.9471\n","Epoch [1/2] Batch [1010/5524] Loss: 0.6095\n","Epoch [1/2] Batch [1020/5524] Loss: 0.1241\n","Epoch [1/2] Batch [1030/5524] Loss: 0.9440\n","Epoch [1/2] Batch [1040/5524] Loss: 0.6162\n","Epoch [1/2] Batch [1050/5524] Loss: 0.6063\n","Epoch [1/2] Batch [1060/5524] Loss: 1.0999\n","Epoch [1/2] Batch [1070/5524] Loss: 0.6029\n","Epoch [1/2] Batch [1080/5524] Loss: 0.7646\n","Epoch [1/2] Batch [1090/5524] Loss: 1.3016\n","Epoch [1/2] Batch [1100/5524] Loss: 0.7758\n","Epoch [1/2] Batch [1110/5524] Loss: 0.6051\n","Epoch [1/2] Batch [1120/5524] Loss: 0.1314\n","Epoch [1/2] Batch [1130/5524] Loss: 0.2590\n","Epoch [1/2] Batch [1140/5524] Loss: 0.7754\n","Epoch [1/2] Batch [1150/5524] Loss: 0.6035\n","Epoch [1/2] Batch [1160/5524] Loss: 0.5995\n","Epoch [1/2] Batch [1170/5524] Loss: 1.0931\n","Epoch [1/2] Batch [1180/5524] Loss: 0.4476\n","Epoch [1/2] Batch [1190/5524] Loss: 0.7780\n","Epoch [1/2] Batch [1200/5524] Loss: 0.6058\n","Epoch [1/2] Batch [1210/5524] Loss: 0.2781\n","Epoch [1/2] Batch [1220/5524] Loss: 0.4445\n","Epoch [1/2] Batch [1230/5524] Loss: 0.7691\n","Epoch [1/2] Batch [1240/5524] Loss: 0.6048\n","Epoch [1/2] Batch [1250/5524] Loss: 0.7696\n","Epoch [1/2] Batch [1260/5524] Loss: 0.7704\n","Epoch [1/2] Batch [1270/5524] Loss: 1.2813\n","Epoch [1/2] Batch [1280/5524] Loss: 1.2704\n","Epoch [1/2] Batch [1290/5524] Loss: 0.6035\n","Epoch [1/2] Batch [1300/5524] Loss: 0.4322\n","Epoch [1/2] Batch [1310/5524] Loss: 0.4368\n","Epoch [1/2] Batch [1320/5524] Loss: 0.4378\n","Epoch [1/2] Batch [1330/5524] Loss: 0.6036\n","Epoch [1/2] Batch [1340/5524] Loss: 0.9430\n","Epoch [1/2] Batch [1350/5524] Loss: 0.2585\n","Epoch [1/2] Batch [1360/5524] Loss: 0.6006\n","Epoch [1/2] Batch [1370/5524] Loss: 0.4537\n","Epoch [1/2] Batch [1380/5524] Loss: 0.6027\n","Epoch [1/2] Batch [1390/5524] Loss: 1.4617\n","Epoch [1/2] Batch [1400/5524] Loss: 0.2939\n","Epoch [1/2] Batch [1410/5524] Loss: 0.7704\n","Epoch [1/2] Batch [1420/5524] Loss: 0.4393\n","Epoch [1/2] Batch [1430/5524] Loss: 0.4425\n","Epoch [1/2] Batch [1440/5524] Loss: 0.9426\n","Epoch [1/2] Batch [1450/5524] Loss: 0.2613\n","Epoch [1/2] Batch [1460/5524] Loss: 0.9393\n","Epoch [1/2] Batch [1470/5524] Loss: 0.6098\n","Epoch [1/2] Batch [1480/5524] Loss: 1.0800\n","Epoch [1/2] Batch [1490/5524] Loss: 0.7686\n","Epoch [1/2] Batch [1500/5524] Loss: 0.9451\n","Epoch [1/2] Batch [1510/5524] Loss: 0.7840\n","Epoch [1/2] Batch [1520/5524] Loss: 0.7849\n","Epoch [1/2] Batch [1530/5524] Loss: 0.6054\n","Epoch [1/2] Batch [1540/5524] Loss: 0.2858\n","Epoch [1/2] Batch [1550/5524] Loss: 0.4438\n","Epoch [1/2] Batch [1560/5524] Loss: 0.5987\n","Epoch [1/2] Batch [1570/5524] Loss: 0.2763\n","Epoch [1/2] Batch [1580/5524] Loss: 0.9249\n","Epoch [1/2] Batch [1590/5524] Loss: 0.9416\n","Epoch [1/2] Batch [1600/5524] Loss: 0.1335\n","Epoch [1/2] Batch [1610/5524] Loss: 0.6079\n","Epoch [1/2] Batch [1620/5524] Loss: 1.0843\n","Epoch [1/2] Batch [1630/5524] Loss: 1.0907\n","Epoch [1/2] Batch [1640/5524] Loss: 0.6025\n","Epoch [1/2] Batch [1650/5524] Loss: 0.9351\n","Epoch [1/2] Batch [1660/5524] Loss: 0.9448\n","Epoch [1/2] Batch [1670/5524] Loss: 0.9326\n","Epoch [1/2] Batch [1680/5524] Loss: 1.5539\n","Epoch [1/2] Batch [1690/5524] Loss: 0.2853\n","Epoch [1/2] Batch [1700/5524] Loss: 0.4518\n","Epoch [1/2] Batch [1710/5524] Loss: 0.7693\n","Epoch [1/2] Batch [1720/5524] Loss: 0.9282\n","Epoch [1/2] Batch [1730/5524] Loss: 1.1092\n","Epoch [1/2] Batch [1740/5524] Loss: 1.0880\n","Epoch [1/2] Batch [1750/5524] Loss: 0.4373\n","Epoch [1/2] Batch [1760/5524] Loss: 0.7764\n","Epoch [1/2] Batch [1770/5524] Loss: 0.4406\n","Epoch [1/2] Batch [1780/5524] Loss: 0.7733\n","Epoch [1/2] Batch [1790/5524] Loss: 0.7691\n","Epoch [1/2] Batch [1800/5524] Loss: 0.7671\n","Epoch [1/2] Batch [1810/5524] Loss: 0.6027\n","Epoch [1/2] Batch [1820/5524] Loss: 0.9349\n","Epoch [1/2] Batch [1830/5524] Loss: 0.9400\n","Epoch [1/2] Batch [1840/5524] Loss: 1.2487\n","Epoch [1/2] Batch [1850/5524] Loss: 0.4489\n","Epoch [1/2] Batch [1860/5524] Loss: 0.7782\n","Epoch [1/2] Batch [1870/5524] Loss: 1.2628\n","Epoch [1/2] Batch [1880/5524] Loss: 0.9287\n","Epoch [1/2] Batch [1890/5524] Loss: 0.7715\n","Epoch [1/2] Batch [1900/5524] Loss: 0.6042\n","Epoch [1/2] Batch [1910/5524] Loss: 0.9325\n","Epoch [1/2] Batch [1920/5524] Loss: 0.2694\n","Epoch [1/2] Batch [1930/5524] Loss: 0.7746\n","Epoch [1/2] Batch [1940/5524] Loss: 0.4427\n","Epoch [1/2] Batch [1950/5524] Loss: 0.4408\n","Epoch [1/2] Batch [1960/5524] Loss: 1.4412\n","Epoch [1/2] Batch [1970/5524] Loss: 0.9386\n","Epoch [1/2] Batch [1980/5524] Loss: 0.7663\n","Epoch [1/2] Batch [1990/5524] Loss: 0.7661\n","Epoch [1/2] Batch [2000/5524] Loss: 1.4484\n","Epoch [1/2] Batch [2010/5524] Loss: 0.6056\n","Epoch [1/2] Batch [2020/5524] Loss: 0.9461\n","Epoch [1/2] Batch [2030/5524] Loss: 0.7728\n","Epoch [1/2] Batch [2040/5524] Loss: 0.7707\n","Epoch [1/2] Batch [2050/5524] Loss: 0.7751\n","Epoch [1/2] Batch [2060/5524] Loss: 0.4370\n","Epoch [1/2] Batch [2070/5524] Loss: 0.7663\n","Epoch [1/2] Batch [2080/5524] Loss: 0.7696\n","Epoch [1/2] Batch [2090/5524] Loss: 1.2880\n","Epoch [1/2] Batch [2100/5524] Loss: 0.1099\n","Epoch [1/2] Batch [2110/5524] Loss: 0.9414\n","Epoch [1/2] Batch [2120/5524] Loss: 1.1085\n","Epoch [1/2] Batch [2130/5524] Loss: 1.1060\n","Epoch [1/2] Batch [2140/5524] Loss: 0.6005\n","Epoch [1/2] Batch [2150/5524] Loss: 1.1056\n","Epoch [1/2] Batch [2160/5524] Loss: 0.2648\n","Epoch [1/2] Batch [2170/5524] Loss: 0.4364\n","Epoch [1/2] Batch [2180/5524] Loss: 0.9433\n","Epoch [1/2] Batch [2190/5524] Loss: 0.6062\n","Epoch [1/2] Batch [2200/5524] Loss: 0.6010\n","Epoch [1/2] Batch [2210/5524] Loss: 0.4435\n","Epoch [1/2] Batch [2220/5524] Loss: 0.4414\n","Epoch [1/2] Batch [2230/5524] Loss: 0.6010\n","Epoch [1/2] Batch [2240/5524] Loss: 0.4340\n","Epoch [1/2] Batch [2250/5524] Loss: 0.7735\n","Epoch [1/2] Batch [2260/5524] Loss: 0.2742\n","Epoch [1/2] Batch [2270/5524] Loss: 0.6057\n","Epoch [1/2] Batch [2280/5524] Loss: 0.6084\n","Epoch [1/2] Batch [2290/5524] Loss: 0.6014\n","Epoch [1/2] Batch [2300/5524] Loss: 0.6035\n","Epoch [1/2] Batch [2310/5524] Loss: 0.2871\n","Epoch [1/2] Batch [2320/5524] Loss: 0.7710\n","Epoch [1/2] Batch [2330/5524] Loss: 0.7722\n","Epoch [1/2] Batch [2340/5524] Loss: 0.6067\n","Epoch [1/2] Batch [2350/5524] Loss: 1.4138\n","Epoch [1/2] Batch [2360/5524] Loss: 0.9393\n","Epoch [1/2] Batch [2370/5524] Loss: 0.7728\n","Epoch [1/2] Batch [2380/5524] Loss: 0.7628\n","Epoch [1/2] Batch [2390/5524] Loss: 1.2896\n","Epoch [1/2] Batch [2400/5524] Loss: 0.1475\n","Epoch [1/2] Batch [2410/5524] Loss: 0.4376\n","Epoch [1/2] Batch [2420/5524] Loss: 1.0889\n","Epoch [1/2] Batch [2430/5524] Loss: 1.2634\n","Epoch [1/2] Batch [2440/5524] Loss: 1.2643\n","Epoch [1/2] Batch [2450/5524] Loss: 0.4389\n","Epoch [1/2] Batch [2460/5524] Loss: 0.7679\n","Epoch [1/2] Batch [2470/5524] Loss: 0.2658\n","Epoch [1/2] Batch [2480/5524] Loss: 0.7688\n","Epoch [1/2] Batch [2490/5524] Loss: 0.6032\n","Epoch [1/2] Batch [2500/5524] Loss: 0.7772\n","Epoch [1/2] Batch [2510/5524] Loss: 0.9437\n","Epoch [1/2] Batch [2520/5524] Loss: 0.6055\n","Epoch [1/2] Batch [2530/5524] Loss: 0.1159\n","Epoch [1/2] Batch [2540/5524] Loss: 0.7713\n","Epoch [1/2] Batch [2550/5524] Loss: 0.6037\n","Epoch [1/2] Batch [2560/5524] Loss: 0.4335\n","Epoch [1/2] Batch [2570/5524] Loss: 0.4355\n","Epoch [1/2] Batch [2580/5524] Loss: 0.6059\n","Epoch [1/2] Batch [2590/5524] Loss: 1.1071\n","Epoch [1/2] Batch [2600/5524] Loss: 0.2603\n","Epoch [1/2] Batch [2610/5524] Loss: 0.4432\n","Epoch [1/2] Batch [2620/5524] Loss: 0.7665\n","Epoch [1/2] Batch [2630/5524] Loss: 0.4459\n","Epoch [1/2] Batch [2640/5524] Loss: 0.1286\n","Epoch [1/2] Batch [2650/5524] Loss: 0.2833\n","Epoch [1/2] Batch [2660/5524] Loss: 0.4411\n","Epoch [1/2] Batch [2670/5524] Loss: 0.7785\n","Epoch [1/2] Batch [2680/5524] Loss: 0.2776\n","Epoch [1/2] Batch [2690/5524] Loss: 0.9414\n","Epoch [1/2] Batch [2700/5524] Loss: 0.9349\n","Epoch [1/2] Batch [2710/5524] Loss: 1.0932\n","Epoch [1/2] Batch [2720/5524] Loss: 0.4503\n","Epoch [1/2] Batch [2730/5524] Loss: 0.5994\n","Epoch [1/2] Batch [2740/5524] Loss: 0.7686\n","Epoch [1/2] Batch [2750/5524] Loss: 0.6111\n","Epoch [1/2] Batch [2760/5524] Loss: 0.2781\n","Epoch [1/2] Batch [2770/5524] Loss: 0.9287\n","Epoch [1/2] Batch [2780/5524] Loss: 1.2830\n","Epoch [1/2] Batch [2790/5524] Loss: 0.6001\n","Epoch [1/2] Batch [2800/5524] Loss: 0.9256\n","Epoch [1/2] Batch [2810/5524] Loss: 1.1087\n","Epoch [1/2] Batch [2820/5524] Loss: 0.4429\n","Epoch [1/2] Batch [2830/5524] Loss: 0.4396\n","Epoch [1/2] Batch [2840/5524] Loss: 0.9341\n","Epoch [1/2] Batch [2850/5524] Loss: 0.7738\n","Epoch [1/2] Batch [2860/5524] Loss: 1.2755\n","Epoch [1/2] Batch [2870/5524] Loss: 0.6099\n","Epoch [1/2] Batch [2880/5524] Loss: 0.9335\n","Epoch [1/2] Batch [2890/5524] Loss: 0.4351\n","Epoch [1/2] Batch [2900/5524] Loss: 0.9565\n","Epoch [1/2] Batch [2910/5524] Loss: 0.6061\n","Epoch [1/2] Batch [2920/5524] Loss: 0.7795\n","Epoch [1/2] Batch [2930/5524] Loss: 1.0914\n","Epoch [1/2] Batch [2940/5524] Loss: 0.2914\n","Epoch [1/2] Batch [2950/5524] Loss: 0.7677\n","Epoch [1/2] Batch [2960/5524] Loss: 0.6023\n","Epoch [1/2] Batch [2970/5524] Loss: 0.5992\n","Epoch [1/2] Batch [2980/5524] Loss: 0.9307\n","Epoch [1/2] Batch [2990/5524] Loss: 0.7789\n","Epoch [1/2] Batch [3000/5524] Loss: 0.6070\n","Epoch [1/2] Batch [3010/5524] Loss: 0.6018\n","Epoch [1/2] Batch [3020/5524] Loss: 1.0949\n","Epoch [1/2] Batch [3030/5524] Loss: 0.9234\n","Epoch [1/2] Batch [3040/5524] Loss: 0.6036\n","Epoch [1/2] Batch [3050/5524] Loss: 0.9336\n","Epoch [1/2] Batch [3060/5524] Loss: 0.2772\n","Epoch [1/2] Batch [3070/5524] Loss: 0.4514\n","Epoch [1/2] Batch [3080/5524] Loss: 0.9268\n","Epoch [1/2] Batch [3090/5524] Loss: 0.2630\n","Epoch [1/2] Batch [3100/5524] Loss: 0.7749\n","Epoch [1/2] Batch [3110/5524] Loss: 0.7804\n","Epoch [1/2] Batch [3120/5524] Loss: 0.6052\n","Epoch [1/2] Batch [3130/5524] Loss: 0.6062\n","Epoch [1/2] Batch [3140/5524] Loss: 0.4433\n","Epoch [1/2] Batch [3150/5524] Loss: 0.9275\n","Epoch [1/2] Batch [3160/5524] Loss: 0.9196\n","Epoch [1/2] Batch [3170/5524] Loss: 0.6101\n","Epoch [1/2] Batch [3180/5524] Loss: 0.6028\n","Epoch [1/2] Batch [3190/5524] Loss: 0.4516\n","Epoch [1/2] Batch [3200/5524] Loss: 1.1029\n","Epoch [1/2] Batch [3210/5524] Loss: 0.7662\n","Epoch [1/2] Batch [3220/5524] Loss: 0.7699\n","Epoch [1/2] Batch [3230/5524] Loss: 0.2727\n","Epoch [1/2] Batch [3240/5524] Loss: 0.6178\n","Epoch [1/2] Batch [3250/5524] Loss: 0.4439\n","Epoch [1/2] Batch [3260/5524] Loss: 1.0977\n","Epoch [1/2] Batch [3270/5524] Loss: 0.9390\n","Epoch [1/2] Batch [3280/5524] Loss: 0.1054\n","Epoch [1/2] Batch [3290/5524] Loss: 1.1111\n","Epoch [1/2] Batch [3300/5524] Loss: 0.2961\n","Epoch [1/2] Batch [3310/5524] Loss: 0.6067\n","Epoch [1/2] Batch [3320/5524] Loss: 0.7771\n","Epoch [1/2] Batch [3330/5524] Loss: 0.6126\n","Epoch [1/2] Batch [3340/5524] Loss: 0.2814\n","Epoch [1/2] Batch [3350/5524] Loss: 0.2633\n","Epoch [1/2] Batch [3360/5524] Loss: 0.4372\n","Epoch [1/2] Batch [3370/5524] Loss: 0.6032\n","Epoch [1/2] Batch [3380/5524] Loss: 0.6067\n","Epoch [1/2] Batch [3390/5524] Loss: 0.2625\n","Epoch [1/2] Batch [3400/5524] Loss: 0.6032\n","Epoch [1/2] Batch [3410/5524] Loss: 0.5997\n","Epoch [1/2] Batch [3420/5524] Loss: 0.5949\n","Epoch [1/2] Batch [3430/5524] Loss: 0.6033\n","Epoch [1/2] Batch [3440/5524] Loss: 1.2814\n","Epoch [1/2] Batch [3450/5524] Loss: 0.9305\n","Epoch [1/2] Batch [3460/5524] Loss: 0.6136\n","Epoch [1/2] Batch [3470/5524] Loss: 0.9397\n","Epoch [1/2] Batch [3480/5524] Loss: 0.6121\n","Epoch [1/2] Batch [3490/5524] Loss: 0.4343\n","Epoch [1/2] Batch [3500/5524] Loss: 0.2665\n","Epoch [1/2] Batch [3510/5524] Loss: 0.2767\n","Epoch [1/2] Batch [3520/5524] Loss: 0.4346\n","Epoch [1/2] Batch [3530/5524] Loss: 0.9292\n","Epoch [1/2] Batch [3540/5524] Loss: 0.3024\n","Epoch [1/2] Batch [3550/5524] Loss: 0.7758\n","Epoch [1/2] Batch [3560/5524] Loss: 1.1212\n","Epoch [1/2] Batch [3570/5524] Loss: 1.0885\n","Epoch [1/2] Batch [3580/5524] Loss: 1.2401\n","Epoch [1/2] Batch [3590/5524] Loss: 0.7624\n","Epoch [1/2] Batch [3600/5524] Loss: 0.7640\n","Epoch [1/2] Batch [3610/5524] Loss: 0.6065\n","Epoch [1/2] Batch [3620/5524] Loss: 1.1224\n","Epoch [1/2] Batch [3630/5524] Loss: 0.2705\n","Epoch [1/2] Batch [3640/5524] Loss: 0.9282\n","Epoch [1/2] Batch [3650/5524] Loss: 1.4396\n","Epoch [1/2] Batch [3660/5524] Loss: 0.6055\n","Epoch [1/2] Batch [3670/5524] Loss: 0.7780\n","Epoch [1/2] Batch [3680/5524] Loss: 0.6106\n","Epoch [1/2] Batch [3690/5524] Loss: 0.7709\n","Epoch [1/2] Batch [3700/5524] Loss: 0.7693\n","Epoch [1/2] Batch [3710/5524] Loss: 0.6093\n","Epoch [1/2] Batch [3720/5524] Loss: 0.4396\n","Epoch [1/2] Batch [3730/5524] Loss: 0.6097\n","Epoch [1/2] Batch [3740/5524] Loss: 0.6051\n","Epoch [1/2] Batch [3750/5524] Loss: 0.7716\n","Epoch [1/2] Batch [3760/5524] Loss: 0.9426\n","Epoch [1/2] Batch [3770/5524] Loss: 0.7673\n","Epoch [1/2] Batch [3780/5524] Loss: 0.9336\n","Epoch [1/2] Batch [3790/5524] Loss: 0.7754\n","Epoch [1/2] Batch [3800/5524] Loss: 1.1102\n","Epoch [1/2] Batch [3810/5524] Loss: 0.0923\n","Epoch [1/2] Batch [3820/5524] Loss: 0.4442\n","Epoch [1/2] Batch [3830/5524] Loss: 0.6024\n","Epoch [1/2] Batch [3840/5524] Loss: 0.6043\n","Epoch [1/2] Batch [3850/5524] Loss: 0.7697\n","Epoch [1/2] Batch [3860/5524] Loss: 0.9224\n","Epoch [1/2] Batch [3870/5524] Loss: 0.7687\n","Epoch [1/2] Batch [3880/5524] Loss: 0.2788\n","Epoch [1/2] Batch [3890/5524] Loss: 0.7717\n","Epoch [1/2] Batch [3900/5524] Loss: 0.4408\n","Epoch [1/2] Batch [3910/5524] Loss: 0.4413\n","Epoch [1/2] Batch [3920/5524] Loss: 0.4393\n","Epoch [1/2] Batch [3930/5524] Loss: 1.2768\n","Epoch [1/2] Batch [3940/5524] Loss: 0.7762\n","Epoch [1/2] Batch [3950/5524] Loss: 1.1148\n","Epoch [1/2] Batch [3960/5524] Loss: 0.7797\n","Epoch [1/2] Batch [3970/5524] Loss: 0.6103\n","Epoch [1/2] Batch [3980/5524] Loss: 0.9306\n","Epoch [1/2] Batch [3990/5524] Loss: 0.4398\n","Epoch [1/2] Batch [4000/5524] Loss: 0.6048\n","Epoch [1/2] Batch [4010/5524] Loss: 0.2613\n","Epoch [1/2] Batch [4020/5524] Loss: 0.6022\n","Epoch [1/2] Batch [4030/5524] Loss: 0.7654\n","Epoch [1/2] Batch [4040/5524] Loss: 0.4445\n","Epoch [1/2] Batch [4050/5524] Loss: 0.4502\n","Epoch [1/2] Batch [4060/5524] Loss: 0.4380\n","Epoch [1/2] Batch [4070/5524] Loss: 0.4317\n","Epoch [1/2] Batch [4080/5524] Loss: 0.6000\n","Epoch [1/2] Batch [4090/5524] Loss: 1.0900\n","Epoch [1/2] Batch [4100/5524] Loss: 0.6101\n","Epoch [1/2] Batch [4110/5524] Loss: 0.9344\n","Epoch [1/2] Batch [4120/5524] Loss: 0.7713\n","Epoch [1/2] Batch [4130/5524] Loss: 0.9423\n","Epoch [1/2] Batch [4140/5524] Loss: 0.9450\n","Epoch [1/2] Batch [4150/5524] Loss: 0.1260\n","Epoch [1/2] Batch [4160/5524] Loss: 0.6064\n","Epoch [1/2] Batch [4170/5524] Loss: 0.6072\n","Epoch [1/2] Batch [4180/5524] Loss: 0.9449\n","Epoch [1/2] Batch [4190/5524] Loss: 0.4470\n","Epoch [1/2] Batch [4200/5524] Loss: 0.6022\n","Epoch [1/2] Batch [4210/5524] Loss: 0.7809\n","Epoch [1/2] Batch [4220/5524] Loss: 1.5916\n","Epoch [1/2] Batch [4230/5524] Loss: 0.6052\n","Epoch [1/2] Batch [4240/5524] Loss: 0.4302\n","Epoch [1/2] Batch [4250/5524] Loss: 1.4732\n","Epoch [1/2] Batch [4260/5524] Loss: 0.9421\n","Epoch [1/2] Batch [4270/5524] Loss: 0.9273\n","Epoch [1/2] Batch [4280/5524] Loss: 1.1042\n","Epoch [1/2] Batch [4290/5524] Loss: 0.9402\n","Epoch [1/2] Batch [4300/5524] Loss: 0.4469\n","Epoch [1/2] Batch [4310/5524] Loss: 0.6207\n","Epoch [1/2] Batch [4320/5524] Loss: 0.6114\n","Epoch [1/2] Batch [4330/5524] Loss: 0.6116\n","Epoch [1/2] Batch [4340/5524] Loss: 0.4331\n","Epoch [1/2] Batch [4350/5524] Loss: 0.2825\n","Epoch [1/2] Batch [4360/5524] Loss: 0.4391\n","Epoch [1/2] Batch [4370/5524] Loss: 0.7758\n","Epoch [1/2] Batch [4380/5524] Loss: 0.6021\n","Epoch [1/2] Batch [4390/5524] Loss: 0.9460\n","Epoch [1/2] Batch [4400/5524] Loss: 1.0793\n","Epoch [1/2] Batch [4410/5524] Loss: 0.9391\n","Epoch [1/2] Batch [4420/5524] Loss: 1.4298\n","Epoch [1/2] Batch [4430/5524] Loss: 0.9299\n","Epoch [1/2] Batch [4440/5524] Loss: 0.7774\n","Epoch [1/2] Batch [4450/5524] Loss: 1.0968\n","Epoch [1/2] Batch [4460/5524] Loss: 0.6025\n","Epoch [1/2] Batch [4470/5524] Loss: 0.4373\n","Epoch [1/2] Batch [4480/5524] Loss: 0.7602\n","Epoch [1/2] Batch [4490/5524] Loss: 0.6019\n","Epoch [1/2] Batch [4500/5524] Loss: 0.4421\n","Epoch [1/2] Batch [4510/5524] Loss: 0.6069\n","Epoch [1/2] Batch [4520/5524] Loss: 0.4380\n","Epoch [1/2] Batch [4530/5524] Loss: 0.6010\n","Epoch [1/2] Batch [4540/5524] Loss: 0.9216\n","Epoch [1/2] Batch [4550/5524] Loss: 0.7727\n","Epoch [1/2] Batch [4560/5524] Loss: 0.7694\n","Epoch [1/2] Batch [4570/5524] Loss: 0.2783\n","Epoch [1/2] Batch [4580/5524] Loss: 0.7763\n","Epoch [1/2] Batch [4590/5524] Loss: 0.9348\n","Epoch [1/2] Batch [4600/5524] Loss: 0.6068\n","Epoch [1/2] Batch [4610/5524] Loss: 0.7639\n","Epoch [1/2] Batch [4620/5524] Loss: 0.6078\n","Epoch [1/2] Batch [4630/5524] Loss: 0.6021\n","Epoch [1/2] Batch [4640/5524] Loss: 0.2770\n","Epoch [1/2] Batch [4650/5524] Loss: 0.7702\n","Epoch [1/2] Batch [4660/5524] Loss: 0.7788\n","Epoch [1/2] Batch [4670/5524] Loss: 0.6079\n","Epoch [1/2] Batch [4680/5524] Loss: 1.0982\n","Epoch [1/2] Batch [4690/5524] Loss: 0.7714\n","Epoch [1/2] Batch [4700/5524] Loss: 0.9298\n","Epoch [1/2] Batch [4710/5524] Loss: 0.7693\n","Epoch [1/2] Batch [4720/5524] Loss: 0.7941\n","Epoch [1/2] Batch [4730/5524] Loss: 0.5995\n","Epoch [1/2] Batch [4740/5524] Loss: 0.7725\n","Epoch [1/2] Batch [4750/5524] Loss: 1.0956\n","Epoch [1/2] Batch [4760/5524] Loss: 0.6048\n","Epoch [1/2] Batch [4770/5524] Loss: 1.0934\n","Epoch [1/2] Batch [4780/5524] Loss: 0.4433\n","Epoch [1/2] Batch [4790/5524] Loss: 1.0963\n","Epoch [1/2] Batch [4800/5524] Loss: 0.7563\n","Epoch [1/2] Batch [4810/5524] Loss: 0.4338\n","Epoch [1/2] Batch [4820/5524] Loss: 0.6041\n","Epoch [1/2] Batch [4830/5524] Loss: 0.4355\n","Epoch [1/2] Batch [4840/5524] Loss: 0.9428\n","Epoch [1/2] Batch [4850/5524] Loss: 0.6028\n","Epoch [1/2] Batch [4860/5524] Loss: 0.1158\n","Epoch [1/2] Batch [4870/5524] Loss: 0.7760\n","Epoch [1/2] Batch [4880/5524] Loss: 0.7729\n","Epoch [1/2] Batch [4890/5524] Loss: 1.5918\n","Epoch [1/2] Batch [4900/5524] Loss: 0.4499\n","Epoch [1/2] Batch [4910/5524] Loss: 0.7696\n","Epoch [1/2] Batch [4920/5524] Loss: 0.6025\n","Epoch [1/2] Batch [4930/5524] Loss: 0.5999\n","Epoch [1/2] Batch [4940/5524] Loss: 0.2803\n","Epoch [1/2] Batch [4950/5524] Loss: 0.7675\n","Epoch [1/2] Batch [4960/5524] Loss: 0.2694\n","Epoch [1/2] Batch [4970/5524] Loss: 0.9205\n","Epoch [1/2] Batch [4980/5524] Loss: 0.4360\n","Epoch [1/2] Batch [4990/5524] Loss: 0.7755\n","Epoch [1/2] Batch [5000/5524] Loss: 0.9481\n","Epoch [1/2] Batch [5010/5524] Loss: 0.2765\n","Epoch [1/2] Batch [5020/5524] Loss: 0.6009\n","Epoch [1/2] Batch [5030/5524] Loss: 0.4402\n","Epoch [1/2] Batch [5040/5524] Loss: 0.7753\n","Epoch [1/2] Batch [5050/5524] Loss: 1.4338\n","Epoch [1/2] Batch [5060/5524] Loss: 0.7744\n","Epoch [1/2] Batch [5070/5524] Loss: 0.9365\n","Epoch [1/2] Batch [5080/5524] Loss: 0.7667\n","Epoch [1/2] Batch [5090/5524] Loss: 0.7716\n","Epoch [1/2] Batch [5100/5524] Loss: 0.6014\n","Epoch [1/2] Batch [5110/5524] Loss: 1.6242\n","Epoch [1/2] Batch [5120/5524] Loss: 1.1190\n","Epoch [1/2] Batch [5130/5524] Loss: 0.7685\n","Epoch [1/2] Batch [5140/5524] Loss: 0.4374\n","Epoch [1/2] Batch [5150/5524] Loss: 0.9405\n","Epoch [1/2] Batch [5160/5524] Loss: 0.6050\n","Epoch [1/2] Batch [5170/5524] Loss: 0.6086\n","Epoch [1/2] Batch [5180/5524] Loss: 1.2528\n","Epoch [1/2] Batch [5190/5524] Loss: 0.7757\n","Epoch [1/2] Batch [5200/5524] Loss: 0.7692\n","Epoch [1/2] Batch [5210/5524] Loss: 0.2763\n","Epoch [1/2] Batch [5220/5524] Loss: 0.7709\n","Epoch [1/2] Batch [5230/5524] Loss: 0.4544\n","Epoch [1/2] Batch [5240/5524] Loss: 0.4336\n","Epoch [1/2] Batch [5250/5524] Loss: 0.1234\n","Epoch [1/2] Batch [5260/5524] Loss: 0.9530\n","Epoch [1/2] Batch [5270/5524] Loss: 0.2826\n","Epoch [1/2] Batch [5280/5524] Loss: 0.6034\n","Epoch [1/2] Batch [5290/5524] Loss: 0.9348\n","Epoch [1/2] Batch [5300/5524] Loss: 0.4448\n","Epoch [1/2] Batch [5310/5524] Loss: 1.0792\n","Epoch [1/2] Batch [5320/5524] Loss: 0.4355\n","Epoch [1/2] Batch [5330/5524] Loss: 0.4394\n","Epoch [1/2] Batch [5340/5524] Loss: 0.2692\n","Epoch [1/2] Batch [5350/5524] Loss: 0.6122\n","Epoch [1/2] Batch [5360/5524] Loss: 0.6097\n","Epoch [1/2] Batch [5370/5524] Loss: 0.6067\n","Epoch [1/2] Batch [5380/5524] Loss: 0.4353\n","Epoch [1/2] Batch [5390/5524] Loss: 0.2872\n","Epoch [1/2] Batch [5400/5524] Loss: 0.7710\n","Epoch [1/2] Batch [5410/5524] Loss: 1.4463\n","Epoch [1/2] Batch [5420/5524] Loss: 1.1011\n","Epoch [1/2] Batch [5430/5524] Loss: 0.2797\n","Epoch [1/2] Batch [5440/5524] Loss: 1.4325\n","Epoch [1/2] Batch [5450/5524] Loss: 0.9258\n","Epoch [1/2] Batch [5460/5524] Loss: 0.6042\n","Epoch [1/2] Batch [5470/5524] Loss: 0.7721\n","Epoch [1/2] Batch [5480/5524] Loss: 0.7785\n","Epoch [1/2] Batch [5490/5524] Loss: 0.5995\n","Epoch [1/2] Batch [5500/5524] Loss: 0.7685\n","Epoch [1/2] Batch [5510/5524] Loss: 0.6088\n","Epoch [1/2] Batch [5520/5524] Loss: 0.6053\n","Epoch [2/2] Batch [10/5524] Loss: 0.6107\n","Epoch [2/2] Batch [20/5524] Loss: 0.4341\n","Epoch [2/2] Batch [30/5524] Loss: 0.2697\n","Epoch [2/2] Batch [40/5524] Loss: 0.9230\n","Epoch [2/2] Batch [50/5524] Loss: 0.9376\n","Epoch [2/2] Batch [60/5524] Loss: 0.7693\n","Epoch [2/2] Batch [70/5524] Loss: 0.4490\n","Epoch [2/2] Batch [80/5524] Loss: 0.9343\n","Epoch [2/2] Batch [90/5524] Loss: 0.2889\n","Epoch [2/2] Batch [100/5524] Loss: 0.6073\n","Epoch [2/2] Batch [110/5524] Loss: 0.7793\n","Epoch [2/2] Batch [120/5524] Loss: 0.9360\n","Epoch [2/2] Batch [130/5524] Loss: 0.5984\n","Epoch [2/2] Batch [140/5524] Loss: 0.9391\n","Epoch [2/2] Batch [150/5524] Loss: 0.6027\n","Epoch [2/2] Batch [160/5524] Loss: 0.7689\n","Epoch [2/2] Batch [170/5524] Loss: 0.6094\n","Epoch [2/2] Batch [180/5524] Loss: 0.6051\n","Epoch [2/2] Batch [190/5524] Loss: 0.2701\n","Epoch [2/2] Batch [200/5524] Loss: 0.7715\n","Epoch [2/2] Batch [210/5524] Loss: 0.4295\n","Epoch [2/2] Batch [220/5524] Loss: 0.9385\n","Epoch [2/2] Batch [230/5524] Loss: 0.6005\n","Epoch [2/2] Batch [240/5524] Loss: 0.6051\n","Epoch [2/2] Batch [250/5524] Loss: 0.6088\n","Epoch [2/2] Batch [260/5524] Loss: 0.2812\n","Epoch [2/2] Batch [270/5524] Loss: 0.7737\n","Epoch [2/2] Batch [280/5524] Loss: 0.7694\n","Epoch [2/2] Batch [290/5524] Loss: 0.2845\n","Epoch [2/2] Batch [300/5524] Loss: 0.6082\n","Epoch [2/2] Batch [310/5524] Loss: 0.4309\n","Epoch [2/2] Batch [320/5524] Loss: 0.7694\n","Epoch [2/2] Batch [330/5524] Loss: 0.6080\n","Epoch [2/2] Batch [340/5524] Loss: 0.9306\n","Epoch [2/2] Batch [350/5524] Loss: 0.2851\n","Epoch [2/2] Batch [360/5524] Loss: 0.7759\n","Epoch [2/2] Batch [370/5524] Loss: 0.6040\n","Epoch [2/2] Batch [380/5524] Loss: 0.9398\n","Epoch [2/2] Batch [390/5524] Loss: 0.7736\n","Epoch [2/2] Batch [400/5524] Loss: 0.7779\n","Epoch [2/2] Batch [410/5524] Loss: 0.9430\n","Epoch [2/2] Batch [420/5524] Loss: 0.7670\n","Epoch [2/2] Batch [430/5524] Loss: 1.0980\n","Epoch [2/2] Batch [440/5524] Loss: 0.7728\n","Epoch [2/2] Batch [450/5524] Loss: 0.9414\n","Epoch [2/2] Batch [460/5524] Loss: 0.7634\n","Epoch [2/2] Batch [470/5524] Loss: 0.6025\n","Epoch [2/2] Batch [480/5524] Loss: 0.6119\n","Epoch [2/2] Batch [490/5524] Loss: 0.6006\n","Epoch [2/2] Batch [500/5524] Loss: 0.7696\n","Epoch [2/2] Batch [510/5524] Loss: 0.4441\n","Epoch [2/2] Batch [520/5524] Loss: 0.9411\n","Epoch [2/2] Batch [530/5524] Loss: 1.3969\n","Epoch [2/2] Batch [540/5524] Loss: 0.4365\n","Epoch [2/2] Batch [550/5524] Loss: 1.2549\n","Epoch [2/2] Batch [560/5524] Loss: 0.4312\n","Epoch [2/2] Batch [570/5524] Loss: 1.4803\n","Epoch [2/2] Batch [580/5524] Loss: 0.2800\n","Epoch [2/2] Batch [590/5524] Loss: 1.1023\n","Epoch [2/2] Batch [600/5524] Loss: 0.9303\n","Epoch [2/2] Batch [610/5524] Loss: 1.2678\n","Epoch [2/2] Batch [620/5524] Loss: 0.6052\n","Epoch [2/2] Batch [630/5524] Loss: 0.9339\n","Epoch [2/2] Batch [640/5524] Loss: 0.2757\n","Epoch [2/2] Batch [650/5524] Loss: 0.6071\n","Epoch [2/2] Batch [660/5524] Loss: 0.7667\n","Epoch [2/2] Batch [670/5524] Loss: 0.6066\n","Epoch [2/2] Batch [680/5524] Loss: 0.4349\n","Epoch [2/2] Batch [690/5524] Loss: 0.6057\n","Epoch [2/2] Batch [700/5524] Loss: 1.1038\n","Epoch [2/2] Batch [710/5524] Loss: 0.4471\n","Epoch [2/2] Batch [720/5524] Loss: 1.0872\n","Epoch [2/2] Batch [730/5524] Loss: 0.6071\n","Epoch [2/2] Batch [740/5524] Loss: 0.9297\n","Epoch [2/2] Batch [750/5524] Loss: 0.7621\n","Epoch [2/2] Batch [760/5524] Loss: 0.7714\n","Epoch [2/2] Batch [770/5524] Loss: 0.7610\n","Epoch [2/2] Batch [780/5524] Loss: 0.4398\n","Epoch [2/2] Batch [790/5524] Loss: 0.4471\n","Epoch [2/2] Batch [800/5524] Loss: 1.6048\n","Epoch [2/2] Batch [810/5524] Loss: 0.4374\n","Epoch [2/2] Batch [820/5524] Loss: 1.0853\n","Epoch [2/2] Batch [830/5524] Loss: 1.0982\n","Epoch [2/2] Batch [840/5524] Loss: 0.7775\n","Epoch [2/2] Batch [850/5524] Loss: 0.2574\n","Epoch [2/2] Batch [860/5524] Loss: 0.6021\n","Epoch [2/2] Batch [870/5524] Loss: 0.6045\n","Epoch [2/2] Batch [880/5524] Loss: 0.6102\n","Epoch [2/2] Batch [890/5524] Loss: 0.9339\n","Epoch [2/2] Batch [900/5524] Loss: 0.4418\n","Epoch [2/2] Batch [910/5524] Loss: 0.6064\n","Epoch [2/2] Batch [920/5524] Loss: 1.1157\n","Epoch [2/2] Batch [930/5524] Loss: 0.4374\n","Epoch [2/2] Batch [940/5524] Loss: 0.5996\n","Epoch [2/2] Batch [950/5524] Loss: 0.4406\n","Epoch [2/2] Batch [960/5524] Loss: 0.4510\n","Epoch [2/2] Batch [970/5524] Loss: 0.9373\n","Epoch [2/2] Batch [980/5524] Loss: 0.7708\n","Epoch [2/2] Batch [990/5524] Loss: 0.4352\n","Epoch [2/2] Batch [1000/5524] Loss: 0.6115\n","Epoch [2/2] Batch [1010/5524] Loss: 0.6140\n","Epoch [2/2] Batch [1020/5524] Loss: 0.4488\n","Epoch [2/2] Batch [1030/5524] Loss: 0.9322\n","Epoch [2/2] Batch [1040/5524] Loss: 1.4519\n","Epoch [2/2] Batch [1050/5524] Loss: 0.7776\n","Epoch [2/2] Batch [1060/5524] Loss: 0.5992\n","Epoch [2/2] Batch [1070/5524] Loss: 1.2931\n","Epoch [2/2] Batch [1080/5524] Loss: 0.6077\n","Epoch [2/2] Batch [1090/5524] Loss: 0.4423\n","Epoch [2/2] Batch [1100/5524] Loss: 0.2689\n","Epoch [2/2] Batch [1110/5524] Loss: 0.9334\n","Epoch [2/2] Batch [1120/5524] Loss: 0.6020\n","Epoch [2/2] Batch [1130/5524] Loss: 0.6003\n","Epoch [2/2] Batch [1140/5524] Loss: 0.9344\n","Epoch [2/2] Batch [1150/5524] Loss: 0.4334\n","Epoch [2/2] Batch [1160/5524] Loss: 0.4692\n","Epoch [2/2] Batch [1170/5524] Loss: 0.7726\n","Epoch [2/2] Batch [1180/5524] Loss: 0.7722\n","Epoch [2/2] Batch [1190/5524] Loss: 0.6096\n","Epoch [2/2] Batch [1200/5524] Loss: 1.2768\n","Epoch [2/2] Batch [1210/5524] Loss: 0.6123\n","Epoch [2/2] Batch [1220/5524] Loss: 0.7724\n","Epoch [2/2] Batch [1230/5524] Loss: 0.9317\n","Epoch [2/2] Batch [1240/5524] Loss: 0.9378\n","Epoch [2/2] Batch [1250/5524] Loss: 0.7674\n","Epoch [2/2] Batch [1260/5524] Loss: 0.7651\n","Epoch [2/2] Batch [1270/5524] Loss: 0.9193\n","Epoch [2/2] Batch [1280/5524] Loss: 0.9341\n","Epoch [2/2] Batch [1290/5524] Loss: 0.7754\n","Epoch [2/2] Batch [1300/5524] Loss: 0.7762\n","Epoch [2/2] Batch [1310/5524] Loss: 0.2788\n","Epoch [2/2] Batch [1320/5524] Loss: 0.9347\n","Epoch [2/2] Batch [1330/5524] Loss: 0.4330\n","Epoch [2/2] Batch [1340/5524] Loss: 0.9377\n","Epoch [2/2] Batch [1350/5524] Loss: 0.9378\n","Epoch [2/2] Batch [1360/5524] Loss: 1.2499\n","Epoch [2/2] Batch [1370/5524] Loss: 0.7685\n","Epoch [2/2] Batch [1380/5524] Loss: 0.7661\n","Epoch [2/2] Batch [1390/5524] Loss: 1.0704\n","Epoch [2/2] Batch [1400/5524] Loss: 1.2645\n","Epoch [2/2] Batch [1410/5524] Loss: 0.5991\n","Epoch [2/2] Batch [1420/5524] Loss: 0.4423\n","Epoch [2/2] Batch [1430/5524] Loss: 0.6109\n","Epoch [2/2] Batch [1440/5524] Loss: 0.4612\n","Epoch [2/2] Batch [1450/5524] Loss: 0.6018\n","Epoch [2/2] Batch [1460/5524] Loss: 0.4374\n","Epoch [2/2] Batch [1470/5524] Loss: 0.2610\n","Epoch [2/2] Batch [1480/5524] Loss: 0.7739\n","Epoch [2/2] Batch [1490/5524] Loss: 0.4412\n","Epoch [2/2] Batch [1500/5524] Loss: 0.4373\n","Epoch [2/2] Batch [1510/5524] Loss: 1.2442\n","Epoch [2/2] Batch [1520/5524] Loss: 0.9294\n","Epoch [2/2] Batch [1530/5524] Loss: 0.4405\n","Epoch [2/2] Batch [1540/5524] Loss: 1.1112\n","Epoch [2/2] Batch [1550/5524] Loss: 0.9367\n","Epoch [2/2] Batch [1560/5524] Loss: 0.7767\n","Epoch [2/2] Batch [1570/5524] Loss: 1.2542\n","Epoch [2/2] Batch [1580/5524] Loss: 0.2737\n","Epoch [2/2] Batch [1590/5524] Loss: 0.9430\n","Epoch [2/2] Batch [1600/5524] Loss: 0.7710\n","Epoch [2/2] Batch [1610/5524] Loss: 0.4386\n","Epoch [2/2] Batch [1620/5524] Loss: 0.6116\n","Epoch [2/2] Batch [1630/5524] Loss: 0.6022\n","Epoch [2/2] Batch [1640/5524] Loss: 0.2992\n","Epoch [2/2] Batch [1650/5524] Loss: 0.6036\n","Epoch [2/2] Batch [1660/5524] Loss: 0.7741\n","Epoch [2/2] Batch [1670/5524] Loss: 0.6068\n","Epoch [2/2] Batch [1680/5524] Loss: 0.9260\n","Epoch [2/2] Batch [1690/5524] Loss: 0.7837\n","Epoch [2/2] Batch [1700/5524] Loss: 0.4395\n","Epoch [2/2] Batch [1710/5524] Loss: 0.7678\n","Epoch [2/2] Batch [1720/5524] Loss: 0.4395\n","Epoch [2/2] Batch [1730/5524] Loss: 0.7580\n","Epoch [2/2] Batch [1740/5524] Loss: 0.6063\n","Epoch [2/2] Batch [1750/5524] Loss: 1.0973\n","Epoch [2/2] Batch [1760/5524] Loss: 0.4427\n","Epoch [2/2] Batch [1770/5524] Loss: 0.5992\n","Epoch [2/2] Batch [1780/5524] Loss: 0.2657\n","Epoch [2/2] Batch [1790/5524] Loss: 0.4383\n","Epoch [2/2] Batch [1800/5524] Loss: 0.7685\n","Epoch [2/2] Batch [1810/5524] Loss: 1.5930\n","Epoch [2/2] Batch [1820/5524] Loss: 0.7644\n","Epoch [2/2] Batch [1830/5524] Loss: 1.5746\n","Epoch [2/2] Batch [1840/5524] Loss: 1.2702\n","Epoch [2/2] Batch [1850/5524] Loss: 0.2795\n","Epoch [2/2] Batch [1860/5524] Loss: 0.6031\n","Epoch [2/2] Batch [1870/5524] Loss: 0.7675\n","Epoch [2/2] Batch [1880/5524] Loss: 0.4441\n","Epoch [2/2] Batch [1890/5524] Loss: 0.7651\n","Epoch [2/2] Batch [1900/5524] Loss: 0.4421\n","Epoch [2/2] Batch [1910/5524] Loss: 1.1157\n","Epoch [2/2] Batch [1920/5524] Loss: 0.7750\n","Epoch [2/2] Batch [1930/5524] Loss: 0.6020\n","Epoch [2/2] Batch [1940/5524] Loss: 0.1433\n","Epoch [2/2] Batch [1950/5524] Loss: 0.1014\n","Epoch [2/2] Batch [1960/5524] Loss: 1.2701\n","Epoch [2/2] Batch [1970/5524] Loss: 0.1029\n","Epoch [2/2] Batch [1980/5524] Loss: 0.9274\n","Epoch [2/2] Batch [1990/5524] Loss: 1.2492\n","Epoch [2/2] Batch [2000/5524] Loss: 0.6029\n","Epoch [2/2] Batch [2010/5524] Loss: 0.9188\n","Epoch [2/2] Batch [2020/5524] Loss: 0.7638\n","Epoch [2/2] Batch [2030/5524] Loss: 0.9434\n","Epoch [2/2] Batch [2040/5524] Loss: 0.6083\n","Epoch [2/2] Batch [2050/5524] Loss: 0.4536\n","Epoch [2/2] Batch [2060/5524] Loss: 0.7676\n","Epoch [2/2] Batch [2070/5524] Loss: 1.7630\n","Epoch [2/2] Batch [2080/5524] Loss: 0.4449\n","Epoch [2/2] Batch [2090/5524] Loss: 0.6065\n","Epoch [2/2] Batch [2100/5524] Loss: 0.7638\n","Epoch [2/2] Batch [2110/5524] Loss: 0.7791\n","Epoch [2/2] Batch [2120/5524] Loss: 0.7760\n","Epoch [2/2] Batch [2130/5524] Loss: 0.9475\n","Epoch [2/2] Batch [2140/5524] Loss: 0.4377\n","Epoch [2/2] Batch [2150/5524] Loss: 0.7707\n","Epoch [2/2] Batch [2160/5524] Loss: 0.9318\n","Epoch [2/2] Batch [2170/5524] Loss: 0.4417\n","Epoch [2/2] Batch [2180/5524] Loss: 0.4385\n","Epoch [2/2] Batch [2190/5524] Loss: 0.2782\n","Epoch [2/2] Batch [2200/5524] Loss: 0.2705\n","Epoch [2/2] Batch [2210/5524] Loss: 0.6106\n","Epoch [2/2] Batch [2220/5524] Loss: 1.1007\n","Epoch [2/2] Batch [2230/5524] Loss: 0.7543\n","Epoch [2/2] Batch [2240/5524] Loss: 0.6078\n","Epoch [2/2] Batch [2250/5524] Loss: 0.7706\n","Epoch [2/2] Batch [2260/5524] Loss: 0.2716\n","Epoch [2/2] Batch [2270/5524] Loss: 0.2688\n","Epoch [2/2] Batch [2280/5524] Loss: 1.2806\n","Epoch [2/2] Batch [2290/5524] Loss: 0.7673\n","Epoch [2/2] Batch [2300/5524] Loss: 0.4523\n","Epoch [2/2] Batch [2310/5524] Loss: 0.7709\n","Epoch [2/2] Batch [2320/5524] Loss: 0.4365\n","Epoch [2/2] Batch [2330/5524] Loss: 0.7722\n","Epoch [2/2] Batch [2340/5524] Loss: 0.6076\n","Epoch [2/2] Batch [2350/5524] Loss: 0.4315\n","Epoch [2/2] Batch [2360/5524] Loss: 0.9411\n","Epoch [2/2] Batch [2370/5524] Loss: 1.1073\n","Epoch [2/2] Batch [2380/5524] Loss: 0.4408\n","Epoch [2/2] Batch [2390/5524] Loss: 1.5592\n","Epoch [2/2] Batch [2400/5524] Loss: 0.6044\n","Epoch [2/2] Batch [2410/5524] Loss: 0.4412\n","Epoch [2/2] Batch [2420/5524] Loss: 0.6065\n","Epoch [2/2] Batch [2430/5524] Loss: 0.7569\n","Epoch [2/2] Batch [2440/5524] Loss: 0.9327\n","Epoch [2/2] Batch [2450/5524] Loss: 0.4310\n","Epoch [2/2] Batch [2460/5524] Loss: 0.2852\n","Epoch [2/2] Batch [2470/5524] Loss: 0.6083\n","Epoch [2/2] Batch [2480/5524] Loss: 0.9429\n","Epoch [2/2] Batch [2490/5524] Loss: 1.0852\n","Epoch [2/2] Batch [2500/5524] Loss: 1.1069\n","Epoch [2/2] Batch [2510/5524] Loss: 0.7660\n","Epoch [2/2] Batch [2520/5524] Loss: 0.7675\n","Epoch [2/2] Batch [2530/5524] Loss: 1.0946\n","Epoch [2/2] Batch [2540/5524] Loss: 0.6072\n","Epoch [2/2] Batch [2550/5524] Loss: 0.9237\n","Epoch [2/2] Batch [2560/5524] Loss: 0.6069\n","Epoch [2/2] Batch [2570/5524] Loss: 1.1223\n","Epoch [2/2] Batch [2580/5524] Loss: 0.5989\n","Epoch [2/2] Batch [2590/5524] Loss: 1.1068\n","Epoch [2/2] Batch [2600/5524] Loss: 0.9443\n","Epoch [2/2] Batch [2610/5524] Loss: 0.4379\n","Epoch [2/2] Batch [2620/5524] Loss: 0.4397\n","Epoch [2/2] Batch [2630/5524] Loss: 0.6008\n","Epoch [2/2] Batch [2640/5524] Loss: 0.9330\n","Epoch [2/2] Batch [2650/5524] Loss: 0.7741\n","Epoch [2/2] Batch [2660/5524] Loss: 0.6079\n","Epoch [2/2] Batch [2670/5524] Loss: 0.7649\n","Epoch [2/2] Batch [2680/5524] Loss: 0.7689\n","Epoch [2/2] Batch [2690/5524] Loss: 0.4364\n","Epoch [2/2] Batch [2700/5524] Loss: 0.6062\n","Epoch [2/2] Batch [2710/5524] Loss: 0.9380\n","Epoch [2/2] Batch [2720/5524] Loss: 0.4413\n","Epoch [2/2] Batch [2730/5524] Loss: 0.1101\n","Epoch [2/2] Batch [2740/5524] Loss: 0.9463\n","Epoch [2/2] Batch [2750/5524] Loss: 0.7739\n","Epoch [2/2] Batch [2760/5524] Loss: 0.6086\n","Epoch [2/2] Batch [2770/5524] Loss: 0.6032\n","Epoch [2/2] Batch [2780/5524] Loss: 1.1023\n","Epoch [2/2] Batch [2790/5524] Loss: 0.7729\n","Epoch [2/2] Batch [2800/5524] Loss: 0.9382\n","Epoch [2/2] Batch [2810/5524] Loss: 0.3099\n","Epoch [2/2] Batch [2820/5524] Loss: 1.1008\n","Epoch [2/2] Batch [2830/5524] Loss: 0.4470\n","Epoch [2/2] Batch [2840/5524] Loss: 0.6058\n","Epoch [2/2] Batch [2850/5524] Loss: 0.4447\n","Epoch [2/2] Batch [2860/5524] Loss: 0.6041\n","Epoch [2/2] Batch [2870/5524] Loss: 0.2614\n","Epoch [2/2] Batch [2880/5524] Loss: 0.6049\n","Epoch [2/2] Batch [2890/5524] Loss: 0.7795\n","Epoch [2/2] Batch [2900/5524] Loss: 0.4398\n","Epoch [2/2] Batch [2910/5524] Loss: 0.9227\n","Epoch [2/2] Batch [2920/5524] Loss: 0.7703\n","Epoch [2/2] Batch [2930/5524] Loss: 0.9321\n","Epoch [2/2] Batch [2940/5524] Loss: 0.7647\n","Epoch [2/2] Batch [2950/5524] Loss: 0.9297\n","Epoch [2/2] Batch [2960/5524] Loss: 0.7803\n","Epoch [2/2] Batch [2970/5524] Loss: 0.9184\n","Epoch [2/2] Batch [2980/5524] Loss: 0.6063\n","Epoch [2/2] Batch [2990/5524] Loss: 0.7665\n","Epoch [2/2] Batch [3000/5524] Loss: 0.7775\n","Epoch [2/2] Batch [3010/5524] Loss: 1.4345\n","Epoch [2/2] Batch [3020/5524] Loss: 1.1033\n","Epoch [2/2] Batch [3030/5524] Loss: 0.9484\n","Epoch [2/2] Batch [3040/5524] Loss: 1.0816\n","Epoch [2/2] Batch [3050/5524] Loss: 0.6101\n","Epoch [2/2] Batch [3060/5524] Loss: 0.6091\n","Epoch [2/2] Batch [3070/5524] Loss: 0.6014\n","Epoch [2/2] Batch [3080/5524] Loss: 0.6078\n","Epoch [2/2] Batch [3090/5524] Loss: 1.1114\n","Epoch [2/2] Batch [3100/5524] Loss: 0.7615\n","Epoch [2/2] Batch [3110/5524] Loss: 0.7682\n","Epoch [2/2] Batch [3120/5524] Loss: 0.9382\n","Epoch [2/2] Batch [3130/5524] Loss: 0.2763\n","Epoch [2/2] Batch [3140/5524] Loss: 0.9308\n","Epoch [2/2] Batch [3150/5524] Loss: 0.9406\n","Epoch [2/2] Batch [3160/5524] Loss: 1.2513\n","Epoch [2/2] Batch [3170/5524] Loss: 0.6052\n","Epoch [2/2] Batch [3180/5524] Loss: 0.2706\n","Epoch [2/2] Batch [3190/5524] Loss: 0.2699\n","Epoch [2/2] Batch [3200/5524] Loss: 1.4357\n","Epoch [2/2] Batch [3210/5524] Loss: 0.6156\n","Epoch [2/2] Batch [3220/5524] Loss: 0.7681\n","Epoch [2/2] Batch [3230/5524] Loss: 1.0972\n","Epoch [2/2] Batch [3240/5524] Loss: 1.2798\n","Epoch [2/2] Batch [3250/5524] Loss: 0.2708\n","Epoch [2/2] Batch [3260/5524] Loss: 0.4456\n","Epoch [2/2] Batch [3270/5524] Loss: 0.7673\n","Epoch [2/2] Batch [3280/5524] Loss: 0.7721\n","Epoch [2/2] Batch [3290/5524] Loss: 0.6137\n","Epoch [2/2] Batch [3300/5524] Loss: 1.4346\n","Epoch [2/2] Batch [3310/5524] Loss: 0.4365\n","Epoch [2/2] Batch [3320/5524] Loss: 1.1030\n","Epoch [2/2] Batch [3330/5524] Loss: 1.1015\n","Epoch [2/2] Batch [3340/5524] Loss: 1.0932\n","Epoch [2/2] Batch [3350/5524] Loss: 0.2746\n","Epoch [2/2] Batch [3360/5524] Loss: 0.2775\n","Epoch [2/2] Batch [3370/5524] Loss: 0.7671\n","Epoch [2/2] Batch [3380/5524] Loss: 0.7785\n","Epoch [2/2] Batch [3390/5524] Loss: 0.7661\n","Epoch [2/2] Batch [3400/5524] Loss: 0.7757\n","Epoch [2/2] Batch [3410/5524] Loss: 0.7668\n","Epoch [2/2] Batch [3420/5524] Loss: 0.7716\n","Epoch [2/2] Batch [3430/5524] Loss: 0.9463\n","Epoch [2/2] Batch [3440/5524] Loss: 0.4450\n","Epoch [2/2] Batch [3450/5524] Loss: 1.0877\n","Epoch [2/2] Batch [3460/5524] Loss: 1.1144\n","Epoch [2/2] Batch [3470/5524] Loss: 0.7721\n","Epoch [2/2] Batch [3480/5524] Loss: 1.0712\n","Epoch [2/2] Batch [3490/5524] Loss: 0.7681\n","Epoch [2/2] Batch [3500/5524] Loss: 0.6038\n","Epoch [2/2] Batch [3510/5524] Loss: 0.1088\n","Epoch [2/2] Batch [3520/5524] Loss: 0.7730\n","Epoch [2/2] Batch [3530/5524] Loss: 0.4369\n","Epoch [2/2] Batch [3540/5524] Loss: 0.4371\n","Epoch [2/2] Batch [3550/5524] Loss: 0.0995\n","Epoch [2/2] Batch [3560/5524] Loss: 0.2964\n","Epoch [2/2] Batch [3570/5524] Loss: 0.6036\n","Epoch [2/2] Batch [3580/5524] Loss: 1.4536\n","Epoch [2/2] Batch [3590/5524] Loss: 0.7816\n","Epoch [2/2] Batch [3600/5524] Loss: 1.1066\n","Epoch [2/2] Batch [3610/5524] Loss: 0.6033\n","Epoch [2/2] Batch [3620/5524] Loss: 0.7696\n","Epoch [2/2] Batch [3630/5524] Loss: 0.6007\n","Epoch [2/2] Batch [3640/5524] Loss: 0.6114\n","Epoch [2/2] Batch [3650/5524] Loss: 1.4363\n","Epoch [2/2] Batch [3660/5524] Loss: 0.6089\n","Epoch [2/2] Batch [3670/5524] Loss: 0.4434\n","Epoch [2/2] Batch [3680/5524] Loss: 0.9293\n","Epoch [2/2] Batch [3690/5524] Loss: 0.6097\n","Epoch [2/2] Batch [3700/5524] Loss: 0.6184\n","Epoch [2/2] Batch [3710/5524] Loss: 1.4214\n","Epoch [2/2] Batch [3720/5524] Loss: 0.0980\n","Epoch [2/2] Batch [3730/5524] Loss: 0.9336\n","Epoch [2/2] Batch [3740/5524] Loss: 0.7679\n","Epoch [2/2] Batch [3750/5524] Loss: 0.4426\n","Epoch [2/2] Batch [3760/5524] Loss: 1.2688\n","Epoch [2/2] Batch [3770/5524] Loss: 0.4419\n","Epoch [2/2] Batch [3780/5524] Loss: 0.6006\n","Epoch [2/2] Batch [3790/5524] Loss: 0.9268\n","Epoch [2/2] Batch [3800/5524] Loss: 1.1049\n","Epoch [2/2] Batch [3810/5524] Loss: 0.2643\n","Epoch [2/2] Batch [3820/5524] Loss: 0.6042\n","Epoch [2/2] Batch [3830/5524] Loss: 0.4354\n","Epoch [2/2] Batch [3840/5524] Loss: 0.9244\n","Epoch [2/2] Batch [3850/5524] Loss: 1.2692\n","Epoch [2/2] Batch [3860/5524] Loss: 0.9413\n","Epoch [2/2] Batch [3870/5524] Loss: 0.7690\n","Epoch [2/2] Batch [3880/5524] Loss: 0.7820\n","Epoch [2/2] Batch [3890/5524] Loss: 0.7718\n","Epoch [2/2] Batch [3900/5524] Loss: 1.1102\n","Epoch [2/2] Batch [3910/5524] Loss: 0.4432\n","Epoch [2/2] Batch [3920/5524] Loss: 0.7692\n","Epoch [2/2] Batch [3930/5524] Loss: 1.0799\n","Epoch [2/2] Batch [3940/5524] Loss: 0.9420\n","Epoch [2/2] Batch [3950/5524] Loss: 0.7640\n","Epoch [2/2] Batch [3960/5524] Loss: 0.2815\n","Epoch [2/2] Batch [3970/5524] Loss: 1.1240\n","Epoch [2/2] Batch [3980/5524] Loss: 0.4328\n","Epoch [2/2] Batch [3990/5524] Loss: 1.2850\n","Epoch [2/2] Batch [4000/5524] Loss: 0.6126\n","Epoch [2/2] Batch [4010/5524] Loss: 0.9336\n","Epoch [2/2] Batch [4020/5524] Loss: 0.7733\n","Epoch [2/2] Batch [4030/5524] Loss: 0.4382\n","Epoch [2/2] Batch [4040/5524] Loss: 0.6090\n","Epoch [2/2] Batch [4050/5524] Loss: 0.7692\n","Epoch [2/2] Batch [4060/5524] Loss: 1.5641\n","Epoch [2/2] Batch [4070/5524] Loss: 0.9329\n","Epoch [2/2] Batch [4080/5524] Loss: 0.9393\n","Epoch [2/2] Batch [4090/5524] Loss: 0.6100\n","Epoch [2/2] Batch [4100/5524] Loss: 0.7769\n","Epoch [2/2] Batch [4110/5524] Loss: 0.6055\n","Epoch [2/2] Batch [4120/5524] Loss: 0.2816\n","Epoch [2/2] Batch [4130/5524] Loss: 0.7655\n","Epoch [2/2] Batch [4140/5524] Loss: 0.9395\n","Epoch [2/2] Batch [4150/5524] Loss: 0.4344\n","Epoch [2/2] Batch [4160/5524] Loss: 0.9348\n","Epoch [2/2] Batch [4170/5524] Loss: 0.6120\n","Epoch [2/2] Batch [4180/5524] Loss: 1.1263\n","Epoch [2/2] Batch [4190/5524] Loss: 0.6143\n","Epoch [2/2] Batch [4200/5524] Loss: 0.7777\n","Epoch [2/2] Batch [4210/5524] Loss: 0.7717\n","Epoch [2/2] Batch [4220/5524] Loss: 0.7739\n","Epoch [2/2] Batch [4230/5524] Loss: 0.9391\n","Epoch [2/2] Batch [4240/5524] Loss: 0.4365\n","Epoch [2/2] Batch [4250/5524] Loss: 0.7666\n","Epoch [2/2] Batch [4260/5524] Loss: 0.7704\n","Epoch [2/2] Batch [4270/5524] Loss: 0.1043\n","Epoch [2/2] Batch [4280/5524] Loss: 0.9393\n","Epoch [2/2] Batch [4290/5524] Loss: 0.2627\n","Epoch [2/2] Batch [4300/5524] Loss: 0.6055\n","Epoch [2/2] Batch [4310/5524] Loss: 0.2670\n","Epoch [2/2] Batch [4320/5524] Loss: 0.6005\n","Epoch [2/2] Batch [4330/5524] Loss: 0.4370\n","Epoch [2/2] Batch [4340/5524] Loss: 0.6042\n","Epoch [2/2] Batch [4350/5524] Loss: 0.4330\n","Epoch [2/2] Batch [4360/5524] Loss: 0.4506\n","Epoch [2/2] Batch [4370/5524] Loss: 0.4601\n","Epoch [2/2] Batch [4380/5524] Loss: 0.6094\n","Epoch [2/2] Batch [4390/5524] Loss: 1.0961\n","Epoch [2/2] Batch [4400/5524] Loss: 0.6026\n","Epoch [2/2] Batch [4410/5524] Loss: 0.6005\n","Epoch [2/2] Batch [4420/5524] Loss: 0.6084\n","Epoch [2/2] Batch [4430/5524] Loss: 0.4615\n","Epoch [2/2] Batch [4440/5524] Loss: 0.6082\n","Epoch [2/2] Batch [4450/5524] Loss: 0.9366\n","Epoch [2/2] Batch [4460/5524] Loss: 0.2723\n","Epoch [2/2] Batch [4470/5524] Loss: 0.7706\n","Epoch [2/2] Batch [4480/5524] Loss: 0.2772\n","Epoch [2/2] Batch [4490/5524] Loss: 1.4325\n","Epoch [2/2] Batch [4500/5524] Loss: 1.0948\n","Epoch [2/2] Batch [4510/5524] Loss: 0.6039\n","Epoch [2/2] Batch [4520/5524] Loss: 0.6115\n","Epoch [2/2] Batch [4530/5524] Loss: 0.4369\n","Epoch [2/2] Batch [4540/5524] Loss: 0.2824\n","Epoch [2/2] Batch [4550/5524] Loss: 0.4373\n","Epoch [2/2] Batch [4560/5524] Loss: 1.0965\n","Epoch [2/2] Batch [4570/5524] Loss: 0.6027\n","Epoch [2/2] Batch [4580/5524] Loss: 0.7734\n","Epoch [2/2] Batch [4590/5524] Loss: 0.2724\n","Epoch [2/2] Batch [4600/5524] Loss: 0.7741\n","Epoch [2/2] Batch [4610/5524] Loss: 1.0968\n","Epoch [2/2] Batch [4620/5524] Loss: 0.9299\n","Epoch [2/2] Batch [4630/5524] Loss: 1.0994\n","Epoch [2/2] Batch [4640/5524] Loss: 0.4395\n","Epoch [2/2] Batch [4650/5524] Loss: 0.4430\n","Epoch [2/2] Batch [4660/5524] Loss: 1.4467\n","Epoch [2/2] Batch [4670/5524] Loss: 1.2792\n","Epoch [2/2] Batch [4680/5524] Loss: 0.4409\n","Epoch [2/2] Batch [4690/5524] Loss: 0.7742\n","Epoch [2/2] Batch [4700/5524] Loss: 0.9284\n","Epoch [2/2] Batch [4710/5524] Loss: 0.4382\n","Epoch [2/2] Batch [4720/5524] Loss: 0.6000\n","Epoch [2/2] Batch [4730/5524] Loss: 0.6044\n","Epoch [2/2] Batch [4740/5524] Loss: 0.7766\n","Epoch [2/2] Batch [4750/5524] Loss: 0.4451\n","Epoch [2/2] Batch [4760/5524] Loss: 0.4346\n","Epoch [2/2] Batch [4770/5524] Loss: 1.1092\n","Epoch [2/2] Batch [4780/5524] Loss: 0.6049\n","Epoch [2/2] Batch [4790/5524] Loss: 0.9381\n","Epoch [2/2] Batch [4800/5524] Loss: 1.3001\n","Epoch [2/2] Batch [4810/5524] Loss: 1.8065\n","Epoch [2/2] Batch [4820/5524] Loss: 0.4382\n","Epoch [2/2] Batch [4830/5524] Loss: 1.7753\n","Epoch [2/2] Batch [4840/5524] Loss: 0.5985\n","Epoch [2/2] Batch [4850/5524] Loss: 0.9395\n","Epoch [2/2] Batch [4860/5524] Loss: 1.1066\n","Epoch [2/2] Batch [4870/5524] Loss: 0.6048\n","Epoch [2/2] Batch [4880/5524] Loss: 0.4353\n","Epoch [2/2] Batch [4890/5524] Loss: 0.6044\n","Epoch [2/2] Batch [4900/5524] Loss: 0.7660\n","Epoch [2/2] Batch [4910/5524] Loss: 0.6053\n","Epoch [2/2] Batch [4920/5524] Loss: 1.0930\n","Epoch [2/2] Batch [4930/5524] Loss: 0.6045\n","Epoch [2/2] Batch [4940/5524] Loss: 1.1267\n","Epoch [2/2] Batch [4950/5524] Loss: 0.6038\n","Epoch [2/2] Batch [4960/5524] Loss: 0.4403\n","Epoch [2/2] Batch [4970/5524] Loss: 0.6147\n","Epoch [2/2] Batch [4980/5524] Loss: 0.7818\n","Epoch [2/2] Batch [4990/5524] Loss: 0.9340\n","Epoch [2/2] Batch [5000/5524] Loss: 0.7595\n","Epoch [2/2] Batch [5010/5524] Loss: 1.2699\n","Epoch [2/2] Batch [5020/5524] Loss: 0.6075\n","Epoch [2/2] Batch [5030/5524] Loss: 0.4428\n","Epoch [2/2] Batch [5040/5524] Loss: 0.7758\n","Epoch [2/2] Batch [5050/5524] Loss: 0.5992\n","Epoch [2/2] Batch [5060/5524] Loss: 0.4366\n","Epoch [2/2] Batch [5070/5524] Loss: 0.4315\n","Epoch [2/2] Batch [5080/5524] Loss: 0.6061\n","Epoch [2/2] Batch [5090/5524] Loss: 0.2867\n","Epoch [2/2] Batch [5100/5524] Loss: 0.4419\n","Epoch [2/2] Batch [5110/5524] Loss: 1.1111\n","Epoch [2/2] Batch [5120/5524] Loss: 0.7706\n","Epoch [2/2] Batch [5130/5524] Loss: 0.9453\n","Epoch [2/2] Batch [5140/5524] Loss: 0.9399\n","Epoch [2/2] Batch [5150/5524] Loss: 0.2672\n","Epoch [2/2] Batch [5160/5524] Loss: 0.9297\n","Epoch [2/2] Batch [5170/5524] Loss: 0.4440\n","Epoch [2/2] Batch [5180/5524] Loss: 0.7662\n","Epoch [2/2] Batch [5190/5524] Loss: 0.4344\n","Epoch [2/2] Batch [5200/5524] Loss: 0.9404\n","Epoch [2/2] Batch [5210/5524] Loss: 0.4498\n","Epoch [2/2] Batch [5220/5524] Loss: 0.9362\n","Epoch [2/2] Batch [5230/5524] Loss: 0.7692\n","Epoch [2/2] Batch [5240/5524] Loss: 0.9206\n","Epoch [2/2] Batch [5250/5524] Loss: 0.6104\n","Epoch [2/2] Batch [5260/5524] Loss: 0.2728\n","Epoch [2/2] Batch [5270/5524] Loss: 0.6078\n","Epoch [2/2] Batch [5280/5524] Loss: 0.9485\n","Epoch [2/2] Batch [5290/5524] Loss: 0.9365\n","Epoch [2/2] Batch [5300/5524] Loss: 0.9392\n","Epoch [2/2] Batch [5310/5524] Loss: 0.4511\n","Epoch [2/2] Batch [5320/5524] Loss: 0.6022\n","Epoch [2/2] Batch [5330/5524] Loss: 1.1004\n","Epoch [2/2] Batch [5340/5524] Loss: 0.7703\n","Epoch [2/2] Batch [5350/5524] Loss: 1.1027\n","Epoch [2/2] Batch [5360/5524] Loss: 1.2636\n","Epoch [2/2] Batch [5370/5524] Loss: 0.4419\n","Epoch [2/2] Batch [5380/5524] Loss: 0.2928\n","Epoch [2/2] Batch [5390/5524] Loss: 0.6106\n","Epoch [2/2] Batch [5400/5524] Loss: 0.4467\n","Epoch [2/2] Batch [5410/5524] Loss: 0.7620\n","Epoch [2/2] Batch [5420/5524] Loss: 0.4459\n","Epoch [2/2] Batch [5430/5524] Loss: 0.7728\n","Epoch [2/2] Batch [5440/5524] Loss: 0.6077\n","Epoch [2/2] Batch [5450/5524] Loss: 0.2777\n","Epoch [2/2] Batch [5460/5524] Loss: 0.6122\n","Epoch [2/2] Batch [5470/5524] Loss: 0.2752\n","Epoch [2/2] Batch [5480/5524] Loss: 0.7745\n","Epoch [2/2] Batch [5490/5524] Loss: 1.1159\n","Epoch [2/2] Batch [5500/5524] Loss: 0.7649\n","Epoch [2/2] Batch [5510/5524] Loss: 0.4382\n","Epoch [2/2] Batch [5520/5524] Loss: 0.7652\n"]}],"source":["from torch.optim.lr_scheduler import ExponentialLR\n","\n","optimizer_nadam = optim.NAdam(model.parameters(), lr=0.001)\n","\n","# Create the ExponentialLR scheduler with the desired gamma (exponential decay factor)\n","scheduler = ExponentialLR(optimizer_nadam, gamma=0.95)  # Adjust gamma as needed\n","\n","num_epochs = 2  # Adjust as needed\n","\n","for epoch in range(num_epochs):\n","    for i, data in enumerate(train_loader, 0):\n","        input1, input2, label = data\n","        input1, input2, label = input1.to(device), input2.to(device), label.to(device)\n","        optimizer_nadam.zero_grad()\n","        output1, output2 = model(input1, input2)\n","        loss = criterion(output1, output2, label, model)\n","        loss.backward()\n","        optimizer_nadam.step()\n","\n","        # Update the learning rate after each optimizer step\n","        scheduler.step()\n","\n","        if (i + 1) % 10 == 0:\n","            print(f\"Epoch [{epoch + 1}/{num_epochs}] Batch [{i + 1}/{len(train_loader)}] Loss: {loss.item():.4f}\")\n"]},{"cell_type":"code","execution_count":55,"metadata":{"execution":{"iopub.execute_input":"2023-10-22T15:04:12.672073Z","iopub.status.busy":"2023-10-22T15:04:12.671740Z","iopub.status.idle":"2023-10-22T15:10:27.268636Z","shell.execute_reply":"2023-10-22T15:10:27.267586Z","shell.execute_reply.started":"2023-10-22T15:04:12.672045Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Validation Accuracy: 0.4691\n","Test Accuracy: 0.3282\n"]}],"source":["validation_accuracy = evaluate(model, validation_loader)\n","test_accuracy = evaluate(model, test_loader)\n","\n","print(f\"Validation Accuracy: {validation_accuracy:.4f}\")\n","print(f\"Test Accuracy: {test_accuracy:.4f}\")"]},{"cell_type":"markdown","metadata":{},"source":["Tested for Validation and test set"]},{"cell_type":"markdown","metadata":{},"source":["Testing for custom image input"]},{"cell_type":"code","execution_count":57,"metadata":{"execution":{"iopub.execute_input":"2023-10-22T15:44:33.603008Z","iopub.status.busy":"2023-10-22T15:44:33.601853Z","iopub.status.idle":"2023-10-22T15:44:33.606693Z","shell.execute_reply":"2023-10-22T15:44:33.605871Z","shell.execute_reply.started":"2023-10-22T15:44:33.602968Z"},"trusted":true},"outputs":[],"source":["custom_accuracy = evaluate(model, custom_loader)\n","print(f\"Test Accuracy: {custom_accuracy:.4f}\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
